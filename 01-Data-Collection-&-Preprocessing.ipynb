{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection & Preprocessing \n",
    "\n",
    "The code in this notebook is for the use of scaping the latest comment titles and body from a specific subreddit, converting to a dataframe, and exporting to a .csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw\n",
    "import regex as re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit API Scrape\n",
    "\n",
    "To use the below code and function you must create a reddit account [here](https://www.reddit.com/) and register for use of the API. The username and password will be from your general Reddit account while your client id, cleint secret, and user agent will be from the API token your create attached to the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your personal account info in accordingly\n",
    "reddit = praw.Reddit(\n",
    "    client_id='YOUR CLIENT ID',\n",
    "    client_secret='YOUR ACCOUNT CLIENT SECRET',\n",
    "    password='YOUR ACCOUNT PASSWORD', \n",
    "    user_agent='YOUR ACCOUNT USER AGENT',\n",
    "    username='YOUR ACCOUNT USERNAME'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function takes in the above-created praw instance, the subreddit you would like to scape as a string, and the number of latest posts you would like to collect as an integer. \n",
    "\n",
    "The output will print the number of comments successfully scaped and a dataframe with each row including the comments category, body, and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_scapper(praw_object, sub_reddit, num_posts):\n",
    "    # create submissions object to iterate over\n",
    "    submissions = praw_object.subreddit(sub_reddit).new(limit = num_posts)\n",
    "    \n",
    "    # create list of dictionaries for easy conversion to df\n",
    "    dictionary = []\n",
    "    for post in submissions:\n",
    "        dictionary.append({\n",
    "            'categroy': post.subreddit,\n",
    "            'title': post.title,\n",
    "            'body': post.selftext\n",
    "        })\n",
    "    \n",
    "    # show number of articles collected and out df\n",
    "    print('You collected {} reddit comments about {}'.format(len(dictionary), sub_reddit))\n",
    "    return pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You collected 885 reddit comments about history\n",
      "You collected 989 reddit comments about conspiracy\n"
     ]
    }
   ],
   "source": [
    "# scape 1,000 articles on history and consipracy to model\n",
    "history_df = reddit_scapper(reddit, 'history', 1_000)\n",
    "conspiracy_df = reddit_scapper(reddit, 'conspiracy', 1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine into one dataframe for further preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([history_df, conspiracy_df], axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "The below cells are for the purpose of exploring the data and preparing for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1891 entries, 0 to 1890\n",
      "Data columns (total 3 columns):\n",
      "category    1891 non-null object\n",
      "body        1292 non-null object\n",
      "title       1891 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 59.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a binary target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making category column binary\n",
    "df['category'] = df['category'].map({'conspiracy': 0,\n",
    "                                     'history': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Engineering\n",
    "\n",
    "A large number of consuparcy posts did not contain a body. To be able to still use the text, I used the text from both body and title in this feature.\n",
    "\n",
    "\n",
    "Below lambda function replaces the null values with empty strings, combines the two text cells, and overwrites the entry into body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lamba function to treat nulls as empty text cells for new feature\n",
    "df['body'] = df['title'] + df['body'].apply(lambda x: x if type(x)== str else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function takes in text, removes puctuation, emojis, applies a porter stemmer to each word, and outputs the newly processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stemmer(raw_text_rows):\n",
    "    # Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text_rows)\n",
    "\n",
    "    # tokenize the script\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(letters_only)\n",
    "\n",
    "    #stem the words\n",
    "    p_stemmer = PorterStemmer()\n",
    "    clean_stems = [p_stemmer.stem(w) for w in tokens]\n",
    "\n",
    "    # Join the words back into one string separated by space\n",
    "    return(\" \".join(clean_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses the above function to create features that contain stemmed text for both title and body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column with transformed title\n",
    "df['stemmed_title'] = [text_stemmer(text) for text in df['title']]\n",
    "\n",
    "# create new column with transformed body data\n",
    "df['stemmed_body'] = [text_stemmer(text) for text in df['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>body</th>\n",
       "      <th>title</th>\n",
       "      <th>stemmed_title</th>\n",
       "      <th>stemmed_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>George Popadopoulos Judiciary Committee Transc...</td>\n",
       "      <td>George Popadopoulos Judiciary Committee Transc...</td>\n",
       "      <td>georg popadopoulo judiciari committe transcrip...</td>\n",
       "      <td>georg popadopoulo judiciari committe transcrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Scientists Will Spray Particles Into the Sky t...</td>\n",
       "      <td>Scientists Will Spray Particles Into the Sky t...</td>\n",
       "      <td>scientist will spray particl into the ski to b...</td>\n",
       "      <td>scientist will spray particl into the ski to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>We Are Change confronted Joe Biden in 2007 abo...</td>\n",
       "      <td>We Are Change confronted Joe Biden in 2007 abo...</td>\n",
       "      <td>We are chang confront joe biden in about hi me...</td>\n",
       "      <td>We are chang confront joe biden in about hi me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Jeff Sessions swats creepy Uncle Joe's hands away</td>\n",
       "      <td>Jeff Sessions swats creepy Uncle Joe's hands away</td>\n",
       "      <td>jeff session swat creepi uncl joe s hand away</td>\n",
       "      <td>jeff session swat creepi uncl joe s hand away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NXIVM cultist (üçï gate) admits to enslaving wom...</td>\n",
       "      <td>NXIVM cultist (üçï gate) admits to enslaving wom...</td>\n",
       "      <td>nxivm cultist gate admit to enslav woman for y...</td>\n",
       "      <td>nxivm cultist gate admit to enslav woman for y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               body  \\\n",
       "0         0  George Popadopoulos Judiciary Committee Transc...   \n",
       "1         0  Scientists Will Spray Particles Into the Sky t...   \n",
       "2         0  We Are Change confronted Joe Biden in 2007 abo...   \n",
       "3         0  Jeff Sessions swats creepy Uncle Joe's hands away   \n",
       "4         0  NXIVM cultist (üçï gate) admits to enslaving wom...   \n",
       "\n",
       "                                               title  \\\n",
       "0  George Popadopoulos Judiciary Committee Transc...   \n",
       "1  Scientists Will Spray Particles Into the Sky t...   \n",
       "2  We Are Change confronted Joe Biden in 2007 abo...   \n",
       "3  Jeff Sessions swats creepy Uncle Joe's hands away   \n",
       "4  NXIVM cultist (üçï gate) admits to enslaving wom...   \n",
       "\n",
       "                                       stemmed_title  \\\n",
       "0  georg popadopoulo judiciari committe transcrip...   \n",
       "1  scientist will spray particl into the ski to b...   \n",
       "2  We are chang confront joe biden in about hi me...   \n",
       "3      jeff session swat creepi uncl joe s hand away   \n",
       "4  nxivm cultist gate admit to enslav woman for y...   \n",
       "\n",
       "                                        stemmed_body  \n",
       "0  georg popadopoulo judiciari committe transcrip...  \n",
       "1  scientist will spray particl into the ski to b...  \n",
       "2  We are chang confront joe biden in about hi me...  \n",
       "3      jeff session swat creepi uncl joe s hand away  \n",
       "4  nxivm cultist gate admit to enslav woman for y...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data\n",
    "Export for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/preprocessed_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
