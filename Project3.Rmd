  
  
---
title: "Project 3"
authors: "Vincent Miceli, Justin Hsi, Kevin Potter, Bruno de Melo"
date: "3/17/2020"
output: html_document
---

```{r}
pacman::p_load(RPostgreSQL, dplyr, dbplyr, tidyr, magrittr, stringr, udpipe, tm, lattice, tidytext)
library(ggplot2)
```

```{r}
# <ourgroupname> is no capitals and no spaces
host="soundsgood.crg53husyk2z.us-east-2.rds.amazonaws.com"
port="5432"
database="soundsgood"
username="soundsgood"
password="soundsgood"
my_db <-  src_postgres(database, host=host, port=port, user=username, password=password)
```

```{r}
con <- DBI::dbConnect(RPostgreSQL::PostgreSQL(),
  host = host,
  user = username,
  password = password
)
comms <- data.frame(tbl(con, 'comms'))
posts <- data.frame(tbl(con, 'posts'))
```

```{r}
comms %<>%
  arrange(index)
head(comms, 10)
```

Commms is a dataframe containing comments in response to different reddit posts.  We can Identify which comments belong to which posts with the index and sub_post_id columns.

```{r}
head(posts, 10)
```

Posts is a dataframe of the original reddit posts, containing the title of the post, the body of the post, and the number of comments on the post.

```{r}
p1  = posts %>%
  filter(n_comments == 1) %>%
  arrange(index)
p2_plus = posts %>%
  filter(n_comments > 1) %>%
  arrange(index)
```

In order to find which comments are connected to which posts, we can merge the two dataframes.  In order to do this, they need to be of the same length.  Using the n_comments column, we can multiply each post by the number of comments on them so that the dataframes will have the same length.  The following for loop does this:

```{r}
for (row in 1:length(p2_plus$n_comments)){
  for (n in 1:p2_plus[row, 7]){
    p1 = rbind(p1, p2_plus[row,])
  }
}
```

```{r}
p1 %<>%
  arrange(index)
head(p1, 10)
```

Before we can merge, we need the index columns to match.  The indices are not matching anymore because we multiplied the rows.

```{r}
p1 %<>%
  mutate(index = comms$index)
head(p1,10)
```

The posts are now multiplied by the number of comments on them, so now we can merge the dataframes by index:

```{r}
reddit <- merge(p1, comms, by = 'index')
head(reddit, 10)
```

```{r}
reddit %<>%
  select(-c(index, n_comments, sub_post_id.y, n_comments, likes, category)) %>%
  rename(comments = text, post_id = sub_post_id.x) %>%
  mutate(post_id = post_id + 1)
head(reddit, 10)
```
We now removed uneccessary columns.  The merged data set contains the title of the post, the body, the sub-reddit it belongs to, the post id, and all of the corresponding comments.

```{r}
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
reddit$comments = stringr::str_replace_all(reddit$comments, stopwords_regex, '')
```
Remove stopwords to help identify the true keywords in our dataset.

```{r}
model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(model)

s <- udpipe_annotate(udmodel_english, reddit$comments)
x <- data.frame(s)
```

Created a POS tagger using udpipe, so that we can identify different parts of speech and analyze the most frequent words.

```{r}
stats <- subset(x, upos %in% c("NOUN")) 
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring nouns", xlab = "Freq")
```

```{r}
stats <- keywords_rake(x = x, term = "lemma", group = "doc_id", 
                       relevant = x$upos %in% c("NOUN", "ADJ"))
stats %<>%
  filter(!grepl('Ã‚', keyword)) 
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ rake, data = head(subset(stats, freq > 3), 20), col = "red", 
         main = "Keywords identified by RAKE", 
         xlab = "Rake")
```

```{r}
stats <- subset(stats, ngram > 1 & freq > 4)
stats$key <- factor(stats$keyword, levels = rev(stats$keyword))
barchart(key ~ freq, data = head(stats, 20), col = "magenta", 
         main = "Keywords - simple noun phrases", xlab = "Frequency")
```

##XML Data

```{r}
if (!require('XML')) install.packages('XML')
library(XML)
```

```{r}
# Reading XML file from the web
con <-'https://stackoverflow.com/jobs/feed?dr=DataScientist&j=permanent%2ccontract'
job_raw<-readLines(con, warn = FALSE)
#function that scraps node category which contains job skills
require(XML)
Fun1 <- function(xdata){
  dum <- xmlParse(xdata)
  xDf<- xmlToDataFrame(nodes = getNodeSet(dum, "//*/category"), stringsAsFactors = FALSE)
  xDf
}
# calling function and converting data frame
skills<-Fun1(job_raw)
skills_tbl<-sort(table(skills), decreasing=T)
skills_tbl<-data.frame(skills_tbl)
# writing table to database for comparison
# writing function is commented out as table has already been created, results are showed
# <ourgroupname> is no capitals and no spaces
host="soundsgood.crg53husyk2z.us-east-2.rds.amazonaws.com"
port="5432"
database="soundsgood"
username="soundsgood"
password="soundsgood"
drv <- dbDriver("PostgreSQL")
conc <- DBI::dbConnect(drv, 
  host = host,
  user = username,
  password = password)
# write function is commented out
#######    dbWriteTable(conc,"skills_xml",skills_tbl)
# results
(skills_db <- tbl(conc, "skills_xml"))
# converting to a data frame
skills_db<-data.frame(skills_db)
# summarizing 20 most frequent skills
skills_db<-head(skills_db,20)
#Bar Chart
# barchart(skills_db$skills ~ skills_db$Freq, xlab = "Frequency", ylab = "Skills", main=" 15 Most Frequent Skills Required")
#skills_db$
ggplot(skills_db, aes(x = reorder(skills, Freq), y = Freq))+ geom_bar(stat = 'identity', color="blue", fill = 'white') + coord_flip() + labs(y="Frequency Mentioned", x="Skill")

dbDisconnect(conc)
```
