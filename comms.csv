"","index","sub_post_id","text","likes"
"1",0,0,"I'm currently interviewing for an entry-level business analyst job in Institutional Research at a college. I'd be transitioning careers from educational admin. to IR. I'm self taught in stats, R, and assessment, so this would be a dream job for me. I had to complete a skills assessment activity for them, in which I was to analyze the data in any way I wanted and then write an executive summary. The dataset was for 30,000 students, and included demographic info, information on financial aid, and then ""retained"" and ""graduated"" columns, coded 0=not retained/not graduated, 1=retained/graduated.

I did the following, all using R:

-Cleaned data and transformed it, coding certain categorical variables to numeric.

-Calculated average federal loan/Pell amounts for the student body and compared these amounts to institutions of similar size/demographic makeup.

-Calculated retention rates for full- and part-time students (a surprising 98% for both), and compared these to the same benchmark institutions.

-Created visualizations to show the distribution of financial aid amount across races using boxplots.

-Conducted a logistic regression analysis to study the predictive effect of financial aid amount on retention (again coded as 0,1). I provided the summary() output in a ""figures"" section of the summary.

I submitted to them my raw R code, visualizations, benchmark data, image of my R environment, and executive summary. Now that I've submitted everything, I'm overthinking and panicking that maybe I didn't do enough. What do you guys think? Are these analyses sufficient for something like this? I'm pretty sure using R in itself is overkill (they said they mostly use Excel, but that if I'm comfortable with R I should use that). Would appreciate any insight.",NA
"2",1,0,"I am going to be starting a Computer Science Masters with a focus in Data Science/Machine Learning soon and I am interested in applying the skills I learn to cyber security. My question is, should I dedicate a few courses in my masters towards cyber security or should I get a certification or two in cyber security to begin to develop domain knowledge?",NA
"3",2,1,"If you don’t call everything data science you will find much more studies. „Data Science“ covers a huge variety of academic fields. For instance business analytics is also a part of data science, and you will find a lot of studies in this area too. 
(Despite that DS is much more than a statistician)
For my perspective it depends on what you are interested in.",NA
"4",3,1,"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-europe.html check this site that could guide you but you will not find a lot of good programs that cost less than 5000 per year. I suggest that you could try to communicate with the universities and try to apply for scholarships or check something like erasmus mundus scholarships. However, due to the circumstances in the world right now, I strongly advise that you start connecting with companies in Egypt right now and be on the lookout for jobs if possible just in case travel becomes difficult later this year. So try to work on both at the same time. You can also do this career shift more easily because of your degree and so you could work by yourself for awhile to practice data science & analytics techniques on platforms such as udemy and coursera. I personally recommend DataCamp as I still use till this day to increase my technical proficiency, but keep in mind that it is paid. Hope this helps!",NA
"5",4,1,"MSc Information Studies: Data Science track. It is 1 year, in Amsterdam (UvA), you got merely 1 compulsory Big Data course (6 ECTS). Currently working on my master thesis.

Edit: tuition is €2000.",NA
"6",5,1,"There are good unis in Ireland and it's a nice place to be for a data scientist at this time. However, €5000 remains the challenge. You could check out Trinity College Dublin and National University Ireland Galway. 

Apart from that, Simon Fraser University in Canada (I know it's outside Europe) has a verg good course in data science.",NA
"7",6,1,"The Netherlands has excellent AI Masters degrees at 6 universities (from memory). For EU-passport holders the fees should be around EUR2500 yearly, whereas non-EU people should expect to pay between EUR10-25k (school dependent). If you're from outside of the EU, you should [check for scholarships](https://www.studyinholland.nl/finances/scholarships/find-a-scholarship?facet.query=modified%3A%5BNOW-7DAY%2FDAY+TO+NOW%2FDAY%2B1DAY%5D&facet.query=modified%3A%5BNOW-1MONTH%2FDAY+TO+NOW%2FDAY%2B1DAY%5D&facet.query=modified%3A%5BNOW-1YEAR%2FDAY+TO+NOW%2FDAY%2B1DAY%5D&facet.query=modified%3A%5B%2A+TO+NOW-1YEAR%2FDAY%5D&fq=field_of_study_label%3A%22Mathematics+and+computer+sciences%22&facet.field=audiences_label&facet.field=purpose_label&facet.field=countries_label).",NA
"8",7,1,"If you want a degree, check r/OMSCS, the online master's in computer science (with the Machine Learning specialization) from Georgia Tech",NA
"9",8,1,"French universities are almost free from tuition fee and give you a descent degree. There are plenty international master in data science/machine learning/data mining.",NA
"10",9,1,"are they any online ones less Georgiatech?",NA
"11",10,1,"University of Milano-Bicocca, MSc in Data Science. Tuition is around €2000~3000, but I don’t really know for foreigners. Give it a look if your interested.",NA
"12",11,1,"Very good DS program is at Czech Technical University in Prague: [https://oi.fel.cvut.cz/en/master-specialization-data-science](https://oi.fel.cvut.cz/en/master-specialization-data-science)

But it's kinda hard to get accepted there.",NA
"13",12,1,"In Rome at La Sapienza there is the Data Science Master (I am a student of it), is one of the best universities in Europe, is public and have a lot of learning financial aids, in Italy I consider it the best for this course and I am enjoying so much my Master.",NA
"14",13,1,"Germany: Mannheim, LMU Munich, TU Munich seem to be the most prominent.",NA
"15",14,1,"[deleted]",NA
"16",15,1,"Sure",NA
"17",16,1,"None. Do stats or comp sci",NA
"18",17,1,"I'm a DS in biological sciences doing bioinformatics.  I don't know about specific masters, but many of my colleagues are from CS backgrounds and thrive in bioinformatics.",NA
"19",18,1,"Yea I was looking through some programs and DS is a very wild field. However, I think I have a small idea on atleast what I don't want, I am not the biggest fan of things like Spark, mapreduce and Hadoop and stuff like that. So I am thinking the whole Big Data path isn't for me, maybe just a few elective courses and thats it, but its true like you mentionnee bioinformatics and I have no idea what that is, any suggestions on how to expand my knowledge about the fields in DS",NA
"20",19,1,"Just as a heads up. I'm an ex-pat living in France who was looking for Master programs about a couple of months ago. I went to check the programs listed and they failed to mention the two best programs that I know. They also listed some weird programs (Data Science for Business @ École Polytechnique/HEC is pretty good if you want to go for Data Science consulting, but it is not by a long shot the best DS program at École Polytechnique).

Edit: the best programs, in my opinion, are the MVA (Mathématiques, Vision et Apprentissage) from ENS Cachan, the Masters from the LIP6 laboratory from the Sorbonne University and the ones from the IP Paris, notably the one from École Polytechnique. These are all more applied mathematics than computer science, but I believe that mathematics is the strong point of French universities.",NA
"21",20,1,"I def wouldn't rely on this list. It's a starting point. Pretty sure they're missing stuff compared to what I was looking into about a year ago. One I can see now though is they mention a Business Analytics degree at Imperial College London, when they have a statistics degree with a Data Science stream.

Also DataCamp is ok for total beginners who are just starting to learn how to code but I don't think I'd go much further than that.",NA
"22",21,1,"I do contact hiring companies indeed, infact if things go well which I hope they do I might get accepted at one of Google's summer of code organizations and work on a data science project, I did finish Andrew ng's deep learning specialization on coursera, plus his machine learning course. 

And as for post- google summer of code, I have some contacts in a company I had an intern in ""Teradata"" if you had ever heard of it, and that should get me into a more full-time job if things go well of course. But the thing is in Egypt there is a mandatory military service for a year that would start Jan 2021 for me which in turn would mess up my chances in Teradata abit, unless of course I manage to get a masters aboard and yea finish my masters there. I hope I wasn't too clumsy with my reply :D and thanks alot

Edit: some typos",NA
"23",22,1,"> Edit: tuition is €2000.

This is probably not true for non-EU passport holders.",NA
"24",23,1,"Can I dm?",NA
"25",24,1,"for nuig the fees for international students is 20750€",NA
"26",25,1,"How well are these online masters accepted?",NA
"27",26,1,"€2000 euro for EU citizens right? Not international fees I guess?",NA
"28",27,1,"That’s what I meant, much more if you don’t call it DS ;-) Biostatistics is also very nice",NA
"29",28,1,"I came here to say this. I feel like DS masters is a little goofy. We had a major in my college called ""Undecided Engineering"". DS feels similar. Btw, my title is data scientist and my background is CS. I'm currently finishing a MS in CS where there are specializations in machine learning, interactive intelligence, and other data sciencey topics. I think it's a better choice, but I'm obviously very biased.",NA
"30",29,1,"You can check out some courses like DataCamp, edX or check out multiple syllabi and check out what comes again and again. 
Or check out websites like medium.com, towardsdatascience.com etc.",NA
"31",30,1,"If money isn't an issue for OP then it's a very good college. Otherwise getting masters within €5000 abroad is quite tough unless you go to Germany. If I'm not wrong, there are some cheap courses in France as well but I'm not too sure about how good they are",NA
"32",31,1,"You could try to search by typing gatech admission tableau. You will see the acceptance rate is not really low, though.",NA
"33",32,1,"ikr I'm going for Msc AI for sept 2020.",NA
"34",33,1,"NUIG? That's great and all the best! I got admits from DCU and Maynooth for September 2020 but I got a job offer I couldn't turn down so I'll try again after two years!",NA
"35",34,2,"Fellow econ major here. Graduated last year. I'm currently a research assistant, so do minor coding work. To be clear, I'm not a Data scientist, but I think I understand where you are coming from.

If you're interested in DS you should know that it is somewhat different than what econometrics might have taught you. To quote a statistician, statistics is data driven, econometrics is theory driven. That is to say, econometrics is mostly composed of trying to prove something. A Data scientist is mostly interested in what works best.

I mostly learned coding through Udemy too. Mostly in R, now learning Python. I'd also suggest learning SQL, not only will it look good on your resume, but it's the more appropriate for larger data sets. The ITSL book is a good one, and good job figuring out that linear algebra is important too. There's some courses on udemy that teach Linear algebra, and use Matlab and python to help visualize and understand the concepts, bonus learning a bit of both. STATA and SAS might be useful to know if you were to work in academia (or older supervisors).

It goes without saying, but try not to forget the basic stats and econometrics. And most importantly, dont mistake your online courses as equivalent to complete and formal training. It's a good launch pad, but don't tell an interviewer that I know ML (without mentioning that it was an online course). 
Best of luck to you! It's a fair amount of work, and I hope you stay motivated to see it through.",NA
"36",35,2,"Spend 1-2h a day working on a personal programming project that interests you. If you aren't already, get yourself familliar with Git. Employers will often take into account any projects you have on github when considering applicants. Knowing all the common techniques is great, but if you can't show that you also know how to put them into practice or deal with messy data then it won't matter.

If youre already fairly comfortable with a programming language, I'd also recommend picking up some cloud computing skills using AWS or GCP. Their free tier services can help you learn the engineering side of data science at a very low cost, and it will help you stand out to potential employers.",NA
"37",36,2,"I basically went through the same route. Here is my two cents.

The problem I found out is that the online courses or videos you watch will fade over time and it does not reserve any value if your learned skill is not realised. It is old but gold to say that **project** is the most ideal way to consolidate/realise the knowledge you have learned. Use [handson machine learning with scikit learn and tensorflow](https://www.bookdepository.com/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow-Geron-Aurelien/9781492032649?redirected=true&utm_medium=Google&utm_campaign=Base1&utm_source=HK&utm_content=Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow&selectCurrency=HKD&w=AFF1AU96Q22YV5A8VCCF&pdg=pla-317697301101:cmp-8783422496:adg-92224525801:crv-410452267580:pos-:dev-c&gclid=EAIaIQobChMI-LaHjuub6AIVx6qWCh0XuAfCEAYYASABEgLPB_D_BwE) by [Geron Aurelien](https://www.bookdepository.com/author/Geron-Aurelien) and [Kaggle](https://www.kaggle.com/) as a starting point.

I am not sure if you are interested in furthering your learning to formal university level, if you do not, you should not focus too much on the theory but the application for employment purpose. As u/WokFu mentioned, GBP/AWS is one way to go as soon as you get comfortable to code. You come from DataQuest so I believe you are using Python, so make sure you check out some basic usage of `Flask/Django` such that you could deploy your machine learning model online for showcases.

Having said that, if you are interested in learning more theory to understand what is under the hood when you hit `model.fit()` check out [Machine learning by Andrew Ng in Coursera](https://www.coursera.org/learn/machine-learning). Econometrics obviously helps but it is not the main in Data Science. Rather I would suggest you be very good at linear algebra, statistical inference and probability (especially Bayesian analysis, bootstrap/Monte Carlo simulation). Some basic of computer science such as algorithm and data structure is a must for you to go to some university-level training for rigorous DS postgraduate course. MIT OpenCourseWare is your honest friend.",NA
"38",37,2,"Really get comfortable with linear algebra. ""Coding the Matrix"" (by Philip Klein) would be a good one to work through after your first pass through other linear algebra & programming resources.",NA
"39",38,2,"I studied Econ, worked as a Data Scientist for a year and now go back to Uni for a PhD in Econometrics+ML.

You should build on your current strengths and work from there. Always keep in mind that prediction =\= causality when learning standard ML algorithms and solving problems. Econometrics enables your to do inference while ML is mostly prediction. Use your intuition from Econometrics to understand which variables are important when building models. Don't just throw your data into a NN and expect great things to happen. 

I would start learning econometrics in matrix notation. That teaches you something you already know in linear algebra. Afterwards use Introduction to Statistical Learning and Elements of Statistical Learning to learn the standard algorithms. After learning a new algorithm, apply them to some data (many examples exist online). 

Before applying for jobs do some projects you are interested in. You should come up with something that is interesting and shows you know how to build datapipelines and models or just know how to code. For example, I programmed Pong and built two Q-Learning models that play the game against each other. Don't just use Kaggle because that is not how Data Scientists work. Data is messy and companies want to see that you can work on a project end to end. 

Good luck!",NA
"40",39,2,"End of summer, so that's 5-6 months. From my experience, textbooks give you a better knowledge/time ratio than online courses. I'd say:

1. **Brush up on your mathematics**: I'd replace the Youtube videos with a *Calculus* textbook
2. **Learn algorithms and data structures**: Most people skip this step, but it is a foundation for *everything* in programming. Pick something lighter than Cormen, due to your limited time.
3. **Learn data science**: with a programming language of your choice, either R or Python",NA
"41",40,2,"I would recommend reading The Art of Data Science by Roger D Peng, Elizabeth Matsui.

This book gives an overview of the process involved in doing Data Analysis, right from asking the right questions to developing a statistical model. A lot of people like to focus on the technical aspect, but Data Science is so much more than building Machine Learning models.

It is a small book and shouldn't take more than 2 hours to finish but a must read.",NA
"42",41,2,"You might also want to learn Git/version control. GitHub is probably the most popular site, but there are others. This is useful for collaboration in DS, plus Github gives you a place to show other people your code.

Relatedly, like others have said, I’d strongly advise working on projects that are interesting to you. Kaggle is a good place to start, but any dataset you have access to can work. The most important thing is that you’re interested in what you’re doing so you’ll put in the effort. Then post your project & code to Github so others can see it. Even if you don’t have a plan to broadly distribute your work, the act of posting it encourages good practices.

Best of luck in maximizing your quarantine time!",NA
"43",42,2,"For youtube video supplements to your linear algebra learning, I'd highly recommend checking out 3blue1brown's ""essence of linear algebra"" series, it's really good for developing your geometric intuitions of core linear algebra concepts!",NA
"44",43,2,"Two words: Kaggle competitions!",NA
"45",44,2,"Econ major here, Piggybacking off of an earlier comment.  Econometrics wants to learn why things happen, data science wants to learn what will probably happen in the future.",NA
"46",45,2,"Statistics/ Econ major here. I recommend looking into Time series analysis. Will go well if you are Econ heavy focus",NA
"47",46,2,"My boss came from economics and is now chief data scientist. If you’re going to be in development, please take courses or get familiar with fundamental computer science concepts (memory management, code optimization), get familiar with bash. If you know this, great!",NA
"48",47,2,"I have an economics degree. I went into finance initially through a graduate scheme, but became fairly disillusioned fairly quickly so wormed by way into the BI analytics team - which is fairly advanced by BI standards. From there, taught myself SQL and Python to enable increasingly complex projects. Et voilla.

That said, my job is massively varied. I do pure data science (predicitve modelling/ML), but also will essentiall business partner with decision makers to help them marry business need with data availability on short timeframes. 

Honestly I love it, but not for everyone.",NA
"49",48,2,"Try to find alumni who have followed your same path.

There are lots of different data scientists. Eg Amazon has 'economists' positions doing econometric analyses with big data, investigating price elasticity etc. The whole causal inference is big

On the other hand imo, most data science positions are doing short term predictions ( eg for recommender systems in purchasing)

Here you are really just copying the past ( with dummy variables for eg stock codes ( or other high cardinality categories)..  no one is claiming to have a model of why people buy X instead of Y, as long as it works for eg a week at a time, and you can retrain.  Note that this approach is not likely to be useful for long term buying decisions, but I guess this is  where people are still making the decision.... ( Which style is going to be popular next year... Is probably something you get from reading fashion magazines, rather than looking at this season's sales)

Lastly, you should really learn software engineering..  I would suggest eg hitchhiker guide to python.... imo Most data scientists hiring managers come from a cs background, and still value engineering skills above statistical understanding...( But depends on area you work in etc)",NA
"50",49,2,"I'm not sure if you've done so already, but I would add multivariate calculus, and numerical analysis afterwards, as a very advanced topic in the field. Basically, everything you do with a computer is an approximation of reality, and so that's why I'd say numerical analysis is important in the field, yet I never hear anyone talking about it.",NA
"51",50,2,"I think it sounds like you have a strong learning curriculum. I am aware that dataquest also has projects as part of its curriculum and those projects and whichever other ones you do are going to be important when it comes time for you to demonstrate competence. 

My biggest advice would be as you are learning try to come out with a few well polished and well done projects than you can eventually create into some kind of portfolio for companies to see the work you’ve done. As much as you can, try to make these projects problem solving or opportunity finding, basically take on projects that have clear real world applicability.",NA
"52",51,2,"I think with SQL you want to find online exercises. I found those to be far more useful than any introductory course. Though it has been a very long time since i have been to those websites.",NA
"53",52,2,"Actually I do know SQL and practice it regularly.  I just excluded from this post. I also learned it through Udemy and now I'm using hackerrank around 30-40 minutes per day.

Thanks for clarifying main difference between econometrics and ds",NA
"54",53,2,"Speaking of econ and python, my stock brokerage recently partnered with Blueshift. Do you happen to know anything about it?",NA
"55",54,2,"where can I start with AWS? do they have some tutorials themselves?",NA
"56",55,2,"thanks for the reply.  I'm looking at some graduate programs in CS/DS but I really want to start working as soon as possible, with something like data analyst business or analyst positions. Even if it's an internship",NA
"57",56,2,"thanks for feedback

&#x200B;

>I would start learning econometrics in matrix notation.

any good source for this?",NA
"58",57,2,"For learning mathematics I use a combination of textbook supplemented with youtube videos explaining the individual concepts I approach in my book. It works well for me.",NA
"59",58,2,"already got one! thanks",NA
"60",59,2,"I'd suggest Select Star SQL for learning the basics of SQL.",NA
"61",60,2,"Which program are you using, R or Python? Or both?",NA
"62",61,2,"I'm sorry,  but I do not",NA
"63",62,2,"I like seeing side projects on resumes because IMO it shows that they have the chops to learn on their own. 

For aws, you can likely deploy your project using ec2. It would be good to have a rough understanding of s3/Athena, but that is not critical especially considering you are entry level. 

Best thing IMO is to triple down on SQL/Python to help yourself get into an entry level Data Analyst job and try and level yourself up as fast as possible. You will pick up a lot of best practices on the job that are nearly impossible to pick up learning. 

For SQL, I like HackerRank to prep. I also like these docs on window functions. https://mode.com/sql-tutorial/sql-window-functions/
I would expect every company will ask a question about every join type, aggregations(group by) as well as a rolling sum, moving average, Row_number() window function question.",NA
"64",63,2,"AWS offers a bunch of certifications and online tutorials for the basics of their products. You can start [here](https://aws.amazon.com/training/) to find their 'learning library'

I'm not recommending you take a certification, but the videos are a good place to familiarize yourself with the available tools and how to get started.

I agree with everyone else saying SQL is a key part of data science. It's probably one of my most used tools in day-to-day work. Additionally, when interviewing candidates at my current company, SQL questions are asked at each stage to gauge technical ability. 

My background is also in economics, and I'd also say that algorithms and data structures is another important skill you may be missing currently. Its not something you'd often think about in economics, but comes up frequently when implementing models. MIT OpenCourseWare offers some great, free lectures on the topic that can be an excellent starting point.",NA
"65",64,2,"You can start with Bruce Hansen Econometrics courses but it's a graduate level.",NA
"66",65,2,"As already mentioned: 
Bruce Hansen's book
Hayashi's book

Both are graduate level, but you don't do matrix notation in undergrad.",NA
"67",66,2,"mainly Python but I'm familiar with R as well as one of my econometric courses was covered in R",NA
"68",67,2,"I also started using Hackerrank like 2 weeks ago and will continue doing that by steadily increasing difficulty of problems",NA
"69",68,2,"Then there probably isn't much else I can tell you :D. 

You might find this book useful,  introduction to data science by Rafael a. Irizarry. It uses R. It can be bought for 0 dollars, in case you're short on cash.",NA
"70",69,3,"Pretty cool videos",NA
"71",70,5,"Roughly 1% of the total water supply goes to domestic use in the US. Considering the influence of agriculture, mining, industry, and energy - there's almost zero chance that hand washing behavior causes an increase in overall water consumption. Given the economic slowdown that appears necessary to contain the virus, I'm almost certain you'll see the opposite - a sharp decrease in overall water consumption.",NA
"72",71,5,"This has always been the guidelines on hand washing. Nothing is new here. My guess is that people are not going to change their current habits all that much. 

Also, with all the water bottles being purchased I’m we will see less tap water usage overall.",NA
"73",72,5,"From the perspective of water management in the western united states -

When you look at municipal water use, hands washing is going to be only a small fraction of indoor water use. Just as a rough idea, indoor water use works out to somewhere in the ballpark of 200 gpd/equivalent residential unit (more or less per single family home). Washing your hands, even several times a day, can add a small amount to that, but it's a small component compared to water use from toilets, showers, kitchen, appliances, and especially outdoor water use in the summer.

Now add on that a growing number of businesses closing their doors as covid concerns ramp up. That decreased commercial and industrial demand is going to have a much larger impact on water use than hands washing is. Also consider reductions in energy demand for the same reason - energy has an associated water demand too, unless you are getting your electricity from solar and wind power.

Ultimately, in the scope of overall water supply - agricultural, industrial, municipal - it's not even a blip on the radar. If anything, look for how utility demand might decreases in response to the shutdown happening in response to the virus.",NA
"74",73,5,"I run large scale field experiments getting households to reduce water consumption. This is definitely going to mess with the data. Good thing I have a control group to adjust the difference from",NA
"75",74,5,"Interesting. Thanks.",NA
"76",75,5,"Why are people buying bottled water over tap water?",NA
"77",76,5,"Great perspective. Thanks!",NA
"78",77,5,"Also a significant amount is used by industries and businesses. With their water needs drastically down due to less demand and people working from home I bet you have an overall net reduction.",NA
"79",78,5,"Because they are morons. From what I can tell there is no reason to assume our basic infrastructure will be affected at all.",NA
"80",79,5,"Yes, exactly. But also because people are sheep. They are, in general, incapable of thinking for themselves, much less thinking critically. They see their neighbor buying truckloads of water and toilet paper, they figure they should do the same. This is part of the very definition of “economic panic” - human behavior is defined by herd-like mentality.  It’s really funny to me that human beings remind me so much of the horses I take care of - one gets a glimpse of something shiny and pretty soon the entire herd is galloping madly to who-knows-where.",NA
"81",80,5,"Well, FWIW, for the vast majority of our evolution that was a pretty good strategy. It's just that most people never understand those primal instincts in themselves and still blindly follow them.",NA
"82",81,6,"Impossible to answer unless you tell us what you’ll be working on",NA
"83",82,7,"I would call it a data analyst tool, not a data science tool and IMO it’s designed for non programmers. 
In that respect it’s pretty decent, there’s nodes for almost everything, it’s free, there’s a good user base and it can help share work across teams. 

It’s better than Excel/Access but less than R/Python. 

All tools have their weaknesses.",NA
"84",83,7,"I have never used it. So can't say much about it, but in this day of R and Python, not sure why people would these softwares unless you have some legacy code.",NA
"85",84,9,"I'm also curious about how does Worldometer scrape the data, besides getting some of it from WHO",NA
"86",85,9,"Might also be interesting to know how many tests are beeing used in each country.
But I don't know where to get that data from.",NA
"87",86,9,"There's a good list of datasets on this r/datasets sticky. I'm using the [Italian dataset](https://github.com/pcm-dpc/COVID-19) to track what's happening locally in [🇮🇹 The Corona Virus in Turin, Italy](https://nextjournal.com/schmudde/corona-in-italy).",NA
"88",87,9,"I am founder of [CoronaTracker.com](https://CoronaTracker.com), you may also consider using our API directly, we source our data from multiple sources, some of it are from JHU, and also manually provided by our volunteers, hope that helps you.",NA
"89",88,9,"I was using their data for a map at work but it seems like they stopped reporting at the city level and only report state level cases... Does anyone know of an arcgis dataset with city level data ?",NA
"90",89,9,"Does anyone know where there are data sets on hospital capacity by city: number of beds, average capacity, staff ect. Also can anyone post any datasets on population density by city?",NA
"91",90,9,"If you end up needing to share large COVID data that won't fit in GitHub, I can help. Created https://open.quiltdata.com/ to fix this problem. DM me and we'll get you a bucket.",NA
"92",91,9,"A colab notebook that makes it easy to work with this dataset would accelerate adoption by the datascience community. Have you thought about including a jupyter notebook in your repo?

Same thing for the other relevant datasets, like relevant COVID genome/transcriptome data, relevant medical imaging data, etc.

A lot of people would start working on analyzing data and doing research if it were as simple as clicking a notebook link to get started.",NA
"93",92,9,"Can somebody help me out, I’m new to data analysis,
Can someone make a step by step process of how to use the datasets provided?",NA
"94",93,9,"This is great. What would be even better is to show cases by city, especially for big countries like US or China.",NA
"95",94,9,"I am using this dataset. Looks pretty good with minor inconsistencies (some which I could find were -1 in few places, count is 2 higher for India).

Github: [Coronavirus Dataset](https://github.com/RamiKrispin/coronavirus-csv/blob/master/coronavirus_dataset.csv)",NA
"96",95,9,"Interesting. Pretty frustrated with the Johns Hopkins data set since keep changing labels and such, so annoying.",NA
"97",96,9,"Is anyone doing cluster analysis for age demographics?",NA
"98",97,9,"do you update this dataset?",NA
"99",98,9,"This is funny... I was just wondering about creating a prediction model on corona virus, what algorithms would be best fit, and went here to see this right away.",NA
"100",99,9,"Most of western countries only report the cases in the critical conditions, not including the mild cases. It somehow is not meanful to performance analysis, might be a good exereices of processing the data.",NA
"101",100,9,"This dayily updated time series data (confirmed cases, deaths) by country  from the  European Centre for Disease Prevention and Control (not for all countries daily updates seem to be available) 

&#x200B;

[https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide)",NA
"102",101,9,"Thank you very much for sharing your work.

Please post to r/COVID19, it's the sub that is sharing research related to c-19. They don't allow cross-posting, so it's ok to just copy/paste this post there.

Please also [see this post](https://www.reddit.com/r/Coronavirus/comments/ffvqe2/i_work_at_ebsco_information_services_if_you_are/) about EBSCO making their research page about coronavirus free to read.

u/mod, can we please sticky this information?",NA
"103",102,9,"UK data appears to have gone missing in the latest update.",NA
"104",103,9,"This data is also flawn. For example Spain and Italy have the same number of cases for 12 and 13 March. Same for the other countries. The issue is detailed here:

https://github.com/CSSEGISandData/COVID-19/issues/619",NA
"105",104,9,"Hey, I've been working on this: [https://github.com/covid19-data/covid19-data](https://github.com/covid19-data/covid19-data) The goal is creating interoperable and transparent data pipeline for COVID-19 related data.

For instance, here is WHO daily case data (processed by Our World in Data team): [https://github.com/covid19-data/covid19-data/blob/master/output/cases/cases\_WHO.csv](https://github.com/covid19-data/covid19-data/blob/master/output/cases/cases_WHO.csv) with ISO 3166 country code. This can be merged with country metadata I prepared from the worldbank datasets: [https://github.com/covid19-data/covid19-data/tree/master/output/metadata/country](https://github.com/covid19-data/covid19-data/tree/master/output/metadata/country)

I hope more people join this effort for open COVID-19 datasets and workflows!",NA
"106",105,9,"Would it not be more sensible to model the curve with a probability distribution? Say a gaussian? In this manner, we can gain an estimate of the peak infections and if we bootstrap we can get an idea of the variability on this estimate which I expect would be very large.",NA
"107",106,9,"If anyone is using these datasets for forecasting, please post your forecasts here: [https://www.unitarity.com/app/challenges/us-coronavirus-outbreak/events/mar-20](https://www.unitarity.com/app/challenges/us-coronavirus-outbreak/events/mar-20)

The public is completely in the dark about what the possible toll of this pandemic will be.",NA
"108",107,9,"this would be the most useful. in the usa at least, there is no authoritative number, some states are not reporting and it's become such a political failure that there's an active interest to not share this information. [https://www.theatlantic.com/health/archive/2020/03/why-coronavirus-testing-us-so-delayed/607954/](https://www.theatlantic.com/health/archive/2020/03/why-coronavirus-testing-us-so-delayed/607954/)",NA
"109",108,9,"Here’s an ok source - https://www.worldometers.info/coronavirus/covid-19-testing/",NA
"110",109,9,"The [official Danish site](https://www.ssi.dk/aktuelt/sygdomsudbrud/coronavirus) for overview of current number of tested and positive cases, sorry it is in Danish. But the top tabel indicate tested, positive and deaths in the three columns for Denmark and the Faeroe Islands respectively in the rows.",NA
"111",110,9,"Thanks! That's a lot of good info, I should have checked there first!",NA
"112",111,9,"Thanks for this.",NA
"113",112,9,"Wow that's an **awesome** page! And it works very fast too under what I'm guessing is very heavy traffic. How are you hosting it?

I couldn't find any docs about your API, can you give me a hint? I can take a guess of how it works based on the network requests, but I want to be mindful of rate limits, etc.

Also, where are you getting your **recovered** data from? I see it being reported in many places, but rarely in the official reports of local authorities.",NA
"114",113,9,"Hi. I clicked through all the links at the top, and I don't see anything about how to use your API or how to download snapshots of any tabular data. Is this documented anywhere on your site?",NA
"115",114,9,"Not every country reports data down to that level. E.g. Finland only reports data once per 24h (Some hospital districts have broken this and published data at differing times) and only per hospital region.",NA
"116",115,9,"I’m looking for this too now. I was using it at the county level to plot and project when our local hospitals will reach max capacity. The state level data is useless in a state as large as CA",NA
"117",116,9,"I know in Canada they stopped data at city level.   That sucks.",NA
"118",117,9,"A \*\*versioned\*\* data portal, that's so cool! I'll definitely let you know if we run into storage issues with Github, and I'll bookmark your page for the future.",NA
"119",118,9,"I love that idea! I created a folder for sample notebooks. So far there's only one, I will create more soon: [https://github.com/open-covid-19/data/tree/master/analysis](https://github.com/open-covid-19/data/tree/master/analysis)

You can load the notebooks directly in Colab: [https://colab.research.google.com/github/open-covid-19/data/](https://colab.research.google.com/github/open-covid-19/data/)",NA
"120",119,9,"Hey, I am new too. 

Just go to GitHub>Click on raw (or download)>copy the link and use it in your pd.read\_csv. Thats the way I am doing it.

Example link:  [https://raw.githubusercontent.com/RamiKrispin/coronavirus-csv/master/coronavirus\_dataset.csv](https://raw.githubusercontent.com/RamiKrispin/coronavirus-csv/master/coronavirus_dataset.csv) 

More experienced guys please let me know if there are other, better ways to do it.",NA
"121",120,9,"In excel if you copy the raw (comma delimited files) you can paste the data into one column into excel and then split that column. Thats how I did it.  Using R for analysis.",NA
"122",121,9,"I created an example Notebook, you can open it directly on Google Colab without having to install anything on your computer: https://colab.research.google.com/github/open-covid-19/data/",NA
"123",122,9,"There is no city-level reporting that I'm aware of. You can see the WHO reports for Chinese provinces; I might add another dataset to scrape those and put them into their own table.

I don't know if US has official, centralized, state-level reporting of cases anywhere. If you know a good source for that, please let me know.",NA
"124",123,9,"Thank you! Feel free to report any issues with the dataset. I am taking the data directly from the ECDC portal.",NA
"125",124,9,"I have renamed the dataset from ""aggregate.csv"" / ""aggregate.json"" to ""world.csv"" / ""world.json"". Sorry for the breaking change, I will try not to make any other breaking changes moving forward.",NA
"126",125,9,"I have renamed the dataset from ""aggregate.csv"" / ""aggregate.json"" to ""world.csv"" / ""world.json"". Sorry for the breaking change, I will try not to make any other breaking changes moving forward.",NA
"127",126,9,"I might add labels to this one in the future, but I am going to try my hardest to keep it backwards-compatible",NA
"128",127,9,"Yea I would be willing to write a web scraper with an API to update in real time if someone hasn't done this yet",NA
"129",128,9,"I plan on keeping this up to date with the daily data dumps from ECDC",NA
"130",129,9,"/u/BeggarInSpain take a look at the Notebooks available here: [https://colab.research.google.com/github/open-covid-19/data/](https://colab.research.google.com/github/open-covid-19/data/)

If you replace `IT` with `ES` in the country code, it looks like the exponential model overestimates the disease growth in Spain a bit, I will add more Notebooks with more sophisticated models soon.",NA
"131",130,9,"It is important if what you care about is capacity of health care system, which only critical cases will have access to. I would be interested in seeing some models that estimate actual cases from reported cases -- I would presume it varies by country.",NA
"132",131,9,"This is a great resource, I changed my dataset to use this as the source rather than building upon the flawed Johns Hopkins data or scraping the WHO PDF reports.",NA
"133",132,9,"Thanks for the kind words. I tried posting there but it could only post a link to this post; hopefully that is good enough.

Thanks for the EBSCO link, I'll dig into that!",NA
"134",133,9,"Yikes! Good catch. It's now fixed: https://github.com/open-covid-19/data/commit/c96cdf1f40430040df9e9a1bbece7fa04561c852",NA
"135",134,9,"Are you using the latest version?  I'm seeing different number of cases for 12 and 13 of Match, e.g. [https://github.com/open-covid-19/data/blob/master/output/aggregated.csv#L4624](https://github.com/open-covid-19/data/blob/master/output/aggregated.csv#L4624)",NA
"136",135,9,"Awesome work! Have you considered automating some of the report parsing? Feel free to poke around the repo I linked, all the data available at my repo is parsed automatically from the daily reports from WHO and ECDC.",NA
"137",136,9,"Yeah, unfortunately the data is only as good as the authorities collecting it want it to be and the whole issue has been so politicized that we must be very mindful of where the data comes from; which is why I am far more keen on using the data from ECDC, which is reported by the local authorities (same as WHO) as opposed to Johns Hopkins', which has unknown data sources and they are being weirdly non-transparent about it.",NA
"138",137,9,"That looks like a great source, but they don't appear to have an API to access the data. I only see a (very expensive) option to embed a table.",NA
"139",138,9,"I wish the ECDC had at least optional reporting of number of tested cases",NA
"140",139,9,"Look like 1000 positive out of 4455 tests? Are these tests random or only to people negative flu virus  but have flu symptoms ?",NA
"141",140,9,"https://api.coronatracker.com

We gather most of the stats by crawling news and stats site, and many of our volunteers will help to enter manually as well.",NA
"142",141,9,"You may read the docs at https://api.coronatracker.com",NA
"143",142,9,"Agreed.  Why did they change it? Too much to keep track of?",NA
"144",143,9,"Nope, that’s precisely it!  It’s a very nice way to avoid needing to re-download the csv each time it updates.

The only qualifier I’d add is that if your network requires going through a proxy (eg you’re doing this at work/on a workplace device) you’ll need to use requests and io, like:

    response = requests.get(csv_url, proxies=...)
    df = pd.read_csv(io.StringIO(response.content))",NA
"145",144,9,"[deleted]",NA
"146",145,9,"RemindMe! 7 days",NA
"147",146,9,"Thank you sir",NA
"148",147,9,"Ah I didn't mean anything so simple as that :) just was annoying how Johns Hopkins switched from county-level reporting to state-wide, for example. And kept all the numbers so it doubled everything up on that date. And changed Iran to ""Iran (Islamic Republic of)"", also doubling numbers. And same for South Korea (3 different ways), etc. Just weird having to check their stuff every time lol. So careless given that so many people are relying on their data set.",NA
"149",148,9,"Good idea. I'm just going to the ECDC website and copying a link, which is trivial to automate. I still wouldn't want to make it fully automated because I don't want to break anyone using the data if something goes wrong. I would propose a Github bot that opens a PR daily, so it can be reviewed and approved / rejected.",NA
"150",149,9,"Great! Thank you. Keep us posted.",NA
"151",150,9,"Yeah, that's what I have been expecting. Thanks!

What I had in mind, was some world pupulation density map predicting further spread of the disease. It would be very sophisticated though, because you can't just assume move of people proportional to density areas and there would be a lot of variables to play with.",NA
"152",151,9,"Thanks, I made these charts out of that data.

https://i.imgur.com/k7mPUeT.png

https://i.imgur.com/6JaHaVy.png",NA
"153",152,9,"Sorry that link does not work for me. :-/",NA
"154",153,9,"So the criteria has recently changed for who is tested.

Before Friday the criteria was that people had to show some kind of symptoms and had to have been in one of the red areas defined by the foreign ministry, Ichlg in Austria, Italy's red areas as they developed etc. or been in contact with a known case.

Now they say they only test people with severe symptoms who have to go to the hospital, because it is spreading inside Denmark. So since they have placed most of the country in willing home quarantine, they do not find it useful to test whether people have the virus, we are recommended to stay at home if we get sick and contact our general practitioner doctor if the symptoms get bad.",NA
"155",154,9,"Yes in their notes, they said they realized some double counting was occurring so they decided to focus on state level data",NA
"156",155,9,"Thanks. This might come in handy.

Can anyone tell me how to import data in kaggle? I mean, the kaggle datasets",NA
"157",156,9,"I will be messaging you in 7 days on [**2020-03-21 19:42:26 UTC**](http://www.wolframalpha.com/input/?i=2020-03-21%2019:42:26%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/datascience/comments/fieuqo/open_covid19_dataset/fki9r4m/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdatascience%2Fcomments%2Ffieuqo%2Fopen_covid19_dataset%2Ffki9r4m%2F%5D%0A%0ARemindMe%21%202020-03-21%2019%3A42%3A26%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20fieuqo)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",NA
"158",157,9,"Yes, the data being stale was not the only reason why I decided to create my own dataset... I promise you that the dataset from this repo won't have those issues, mainly because I don't have the time to introduce so many inconsistencies and breaking changes.",NA
"159",158,9,"Is there a link to the github repo? Sorry if I missed it, just want to make sure I habe the correct one...",NA
"160",159,9,"I think that what you have in mind has less to do with population density and more with transportation infrastructure, e.g. roads, trains, airports. Probably a good fit for an ML model, but I'm not an expert in this particular domain.",NA
"161",160,9,"Very interesting to see the countries relative to each other after scaling, thanks for sharing!",NA
"162",161,10,"Kaggle",NA
"163",162,10,"they have database questions on there (SQL) - relevant to DS",NA
"164",163,10,"some say 'kaggle'. i dont agree - it's competitive thing. 

i wish there was website with micro tasks like ""combine 2 datasets"". 

pivot dataframe by this.

&#x200B;

so far there is none. i can think of dataquest io. pretty close but still not exact what i wanted",NA
"165",164,10,"ya wished some website like tht existed",NA
"166",165,10,"Hey, I am down to making one and sell it to ~ leetcode:p",NA
"167",166,10,"Hmm leetcode has built in correct answers and submissions either meet the criteria or don’t. And so it’s binary, your code works or it doesn’t. The only variations are on memory usage and speed; and these topics frame the LC discussions.

Kaggle (in my experience) is a whole lot more subjective, less directed, and takes more human review (in the form of competition decisions.) 

It’s certainly the closest thing we’ve got in answering OP’s question - but I think it’s only a matter of time before the DS community fills the vacuum that OP (and others) have noticed.",NA
"168",167,10,"Do you find the discussion on kaggle to be informative like leetcode?",NA
"169",168,10,"[datacamp.com](https://datacamp.com) \- projects",NA
"170",169,10,"Same there is no website for data science professionals where we could talk about how are interview at Fang companies and stuff",NA
"171",170,10,"Some, the only thing is people sometimes are trying to do very different things with a given dataset",NA
"172",171,11,"Scope: unsure... it is probably dependent on the company more than anything

Pay: try using google ""data science salaries by industry""",NA
"173",172,11,"Looking at your profile you're pre uni. So I'd say pursue Cs and stats and then find an area you are interested in (finance and Green energy  for me) and then apply for jobs in those industries. Doing something you are actually interested in is worth so much more than the extra like 10%",NA
"174",173,11,"Data science itself is broad, like accounting.",NA
"175",174,11,"I'm going to assume you're trying to find what area you should specifically go into to make the most bank.  Frankly, this is the wrong way to go.  Find the area that interests you the most and focus on developing that!",NA
"176",175,11,"I'd suggest a double major in math and stats coupled with a minor in CS. The theory side of stats, to really understand it properly, requires a good chunk of nontrivial math. It's much easier to self study the things you may need from CS than from pure mathematics.",NA
"177",176,12,"I would probably first look at a Fixed Effects econometric (Panel Model - Time Series Cross Sectional) model.  There is probably a lot involved with 200 different companies, such that you might want to group them by some category if you want to do a VAR/ARIMA.  But you want to maintain their independence as individual observations and account for things like unobserved heterogeneity, which a fixed effects panel data model would do.",NA
"178",177,12,"You could try a joint Monte Carlo simulation with ARIMA on returns adjusting for a correlation matrix across the companies. From the sims, you’d get a range of scenarios for your forecast.
21 data points is a low count though so the panel approach above may work better.",NA
"179",178,12,"Does it have to be ARIMA? Facebook open sourced a forecasting tool called 'Prophet' which is really cool and easy to use.",NA
"180",179,12,"Thanks for responding. I will surely try a fixed effects panel data model!",NA
"181",180,13,"Go look at Kaggle and DrivenData for projects. Something local is always good, I had a mentee do one on local spray parks. Our city has a policy of building spray parks in neighbourhoods, however if a community chooses to build their own, the city will maintain/pay for it from there on out. I was curious if this policy was causing more pools to be built in the richer areas or if they were still primarily in areas where kids are residing. 

How to prioritize the areas based on kids and location of other parks was a predictive part of the project. 

That is a local issue and she found a job with a school board doing school level analytics which is a great fit for her since she was teaching before. Point being, play to your strengths. What’s something you’re interested in and data could answer the question? Sports? Gambling? Food? OpenTable has released their restaurant data to show the decrease in reservations.",NA
"182",181,13,"Try gapminder. Got my first dataset there",NA
"183",182,13,"Ya all good advice, but havent found anything interesting or meaningful.  Food service is pointless cause you cant capture the most important variables, for example.  Browsed a lot of Kaggle but i have to just keep going",NA
"184",183,14,"Kaggle",NA
"185",184,14,"Honestly i dont think data science problems are similar to the algorithmic problems that come in SWE interviews. Kaggle would be what you're looking for but it isn't really the same thing.",NA
"186",185,14,"I recently discovered https://projectlovelace.net, which is like project Euler but for scientific programming problems. I haven't used it yet, so cannot say how good it is.",NA
"187",186,14,"I've thought about how feasible it is to build such a site.

Essentially you'll have to come up with a lot of problems. For each one, you need a toy data set, and some specific insight that is crucial to solve it

For example, one is with time series data, and you'll learn the difference on cross validation on time series. And another problem comes with many dirty CSV files and teaches cleaning and joining data.

It could be a fun project to set up such a thing.",NA
"188",187,14,"I'm working on something kinda like that. Maybe more like leetcode but for implementing machine learning algorithms and being tested for runtime and correctness. 

I'm only working in my free time but I hope to have the website and some of the initial challenges up in a couple months.",NA
"189",188,14,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [Is there something like Project Euler but for machine learning specifically? (r\/DataScience)](https://www.reddit.com/r/datascienceproject/comments/fiadma/is_there_something_like_project_euler_but_for/)

- [/r/datascienceproject] [Is there something like Project Euler but for machine learning specifically? (r\/DataScience)](https://www.reddit.com/r/datascienceproject/comments/fit3lt/is_there_something_like_project_euler_but_for/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"190",189,14,"u/nboro94:  
I had a similar thought a few years ago  
give this reddit post a quick skim and see if its similar to what you were thinking?  
 [https://www.reddit.com/r/datascience/comments/838tlf/there\_are\_way\_too\_many\_getting\_started\_with\_data/](https://www.reddit.com/r/datascience/comments/838tlf/there_are_way_too_many_getting_started_with_data/) 

I started collecting ideas for it, but never got around to breaking ground on the project.  Thinking more about this, I think I'd like to resurrect this project.

 [https://github.com/ezeeetm/30problems](https://github.com/ezeeetm/30problems)",NA
"191",190,14,"This sounds like a fun idea, and I have an idea for a question or two. Not at a computer right now but will bake a sample up in the next few days. ^_^",NA
"192",191,14,"This is the correct choice",NA
"193",192,14,"I find it to be too difficult, anything easier for beginners?",NA
"194",193,14,"Thank you! There's also http://rosalind.info/ for bioinformatics and https://cryptopals.com/ for cryptography",NA
"195",194,14,"!RemindMe 10 days",NA
"196",195,14,"read a book (good starter may be Deep Learning with Python by Francois Chollet) or get a course (coursera, udacity) and then try it once again. it’s a fairly scientific field and i doubt you’ll learn meaningful things by hopping right into some code without any background knowledge.",NA
"197",196,14,"Titanic on Kaggle?",NA
"198",197,14,"Project Euler will be difficult too if you don't know math. Learn first with books and moocs.",NA
"199",198,14,"If you're able to build and submit a model but are disappointed with your score, don't give up! 

A lot of the scores on kaggle are just minor tweaks of publicly shared solutions. A middle of the pack result with an original model is a great result for a beginner! Also try averaging your results with the top shared models, which could bump you ahead of the tweakers for a top 10% score.",NA
"200",199,14,"I will be messaging you in 9 days on [**2020-03-24 07:37:03 UTC**](http://www.wolframalpha.com/input/?i=2020-03-24%2007:37:03%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/datascience/comments/fia7n5/is_there_something_like_project_euler_but_for/fkgrydf/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdatascience%2Fcomments%2Ffia7n5%2Fis_there_something_like_project_euler_but_for%2Ffkgrydf%2F%5D%0A%0ARemindMe%21%202020-03-24%2007%3A37%3A03%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20fia7n5)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",NA
"201",200,14,">I doubt you’ll learn meaningful things by hopping right into some code without any background knowledge 

/u/jeremyhoward, and fast.ai:
> hold my beer",NA
"202",201,14,"Some of the difficult problems on Project Euler are very challening, but a lot of the easier ones are doable with only limited programming experience and no formal math education. Project Euler does have a nice difficult curve that starts out fairly easy so it is still very noob friendly. After solving 10-15 problems you start to get an idea of what you're in for with PE before the difficulty seriously starts to ramp up. I looked into Kaggle a bit and I do agree with the above poster that Kaggle appears to be fairly difficult for beginners without doing a lot of research on ML first.",NA
"203",202,14,"fair enough 😄",NA
"204",203,15,"Yes",NA
"205",204,15,"Yes, but you're a bit all over the place:

Are you taking about the whole market, or one stock price?

What independent variables go you think can be used to predict future prices? 

What's your hypothesis exactly? That past movements predict future movements? That you (uniquely) can identify other inputs required? This is pure fantasy. 

""Uniquely"", because if everyone could do it, they'd buy or sell the stock and therefore change the price. This is a highly liquid market, after all. 

To repeat; yes., bad idea. There are already people who try to do this, some based on simple past movements, some on broader factors. Some have short term success driven by good luck and not their models. None of them are rich and famous and none of them have won Nobel prizes. There is a very good reason for this.",NA
"206",205,15,"You should read about chaotic systems. I would challenge you to try to predict something even as deceptively simple as a double pendulum where all the exact laws of natural interaction are know.

Even if you knew all the forces and rules governing the stock market you would still fail to predict it. Financial markets tend to exhibit multifractal chaotic behavior.",NA
"207",206,15,"Economist and former investment banking analyst here. Your question can be described as lacking in some understanding of this topic. Let me clear a few things up.

The stock market is what is known as an R-complexity problem. It can be described in a similar way to the problem of ""solving English"", meaning you come up with a machine that makes it redundant to ever communicate or describe anything regardless of the complexity of that communication. The solution to this can be described by the common trope of infinite monkeys on infinite typewriters, or more simply put literally being god.

The R part of R-complexity stems from a problem being recursive, for example if I told you to write ""1+1+1... and so forth"". You can take all the individual steps in linear time, but let me know when you'll stop and hand me a paper with the exact numeric value of infinity.

The reason the stock market is this way is that if you have any strategy for making money you'll start capturing more and more of the total value of the world's capital. At some point it will make sense for the world to stop everything it is doing, and counteract what you are doing because turning your solution on itself is literally more valuable than everything else combined. The problem inherently defines itself at that point by it's own solution leading to automatic recursion, and we have R-complexity.

Now in the real world there are two ways I'm aware of to remove the R-complexity of markets. You can either *inherently* control the majority of the market's resources in perpetuity, or you can make a law that just says you win. The former solution is how Bitcoin works because its algorithm is inherently designed to increase the difficulty of solving Bitcoin based on the amount of work that is being put into solving Bitcoin. The later solution is how governments work. These of course require effectively **being** the market, which is a tall order for a solution.

This process works on a sliding scale for the record. You don't need to capture the majority of value in the global market to be recursively counteracted, it will happen proportionally to the capital you gain from your solution. If you have an easy solution that captures a bit of capital, you'll be counteracted easily just so someone else can gain that little bit of capital too.

## So NO.

You are not solving any R-complexity problem with **linear anything**, and it's a bad idea. In fact it's exactly as bad of an idea as trying to solve English by writing a sentence to make the act of writing anything forever obsolete. To even begin solving the stock market and R-complexity problems you need to start with a Turning Machine equivalent on some level, and nothing less. Something that can adaptively solve any well defined problem and keep adapting and solving as the parameters and functions of the problem change.

Hope that provides some insight.",NA
"208",207,15,"What are the assumptions behind linear regression and does the stock market scenario meet those assumptions?",NA
"209",208,15,"Chances of you getting it right is 50/50.",NA
"210",209,15,"Arimas works ok for it as well if you want to play with it",NA
"211",210,15,"Maybe a recurrent neural network or something might work better",NA
"212",211,15,"its not a bad idea, but it cannot be the 100% source of truth. its an indicator of a larger trend",NA
"213",212,15,"Try a log periodic power law. Have a read into the JLS model",NA
"214",213,15,"I just wanted to confirm this was the top answer. 
I kind of predicted it with a linear regression ;)",NA
"215",214,15,"Yes. Maybe one stock or one industry with multi variate. Maybe the whole market as it relates to major global events. You won’t be able to “predict” or forecast much",NA
"216",215,15,"Just the thing i wanted to add.

Same as weather forecast if your prediction is wrong for even 0,5% it will add up by time.

So  within a week your forecast is 20% off depending on your 'forecast' on the first day.",NA
"217",216,15,"Great post!",NA
"218",217,15,"Very interesting read!",NA
"219",218,15,"Oh, it’s way below 50/50.",NA
"220",219,15,"Id put it at 33% because it can go up down or sideways and the regression will basically be guessing",NA
"221",220,15,"No, it won't.",NA
"222",221,15,"50 % it goes up 
50 % it goes down

Take a look at S&P 500 under quantmods package for R-CRAN.  It’s hard to predict it.",NA
"223",222,15,"r/badmathematics",NA
"224",223,17,"Here is a project that I recently put together. It uses [the Reddit Comments data set](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/), and finds likely bot comments using Benford's Law.",NA
"225",224,17,"qq: why would you fit the first digit of gap time, instead of the gap time itself (e.g. by exponential distribution)?",NA
"226",225,17,"Well, plainly, the goal of this post was to explore an application of Benford's Law, and this is how Benford's Law works. 

There are certainly other ways to accomplish the same goal of identifying likely bots. The next one I would (and actually will) do is a frequency analysis to see which users has spikes in the frequency domain of their post intervals. In some ways, I would argue that this application of Benford's Law is a quick-and-dirty frequency analysis. Other ways to find bots would be to look for time of day patterns (though I suspect that would have more false positives), behavioral distributions, and more. I do think one advantage that Benford's Law has is that it is very simple to implement and takes minimal computational power. I analyzed the full 2006-2017 Reddit comments data set in under 5 minutes on a 48 core cluster. The approach certainly has its limitations. I speak to one in the post. But, as I said, my goal was to demonstrate Benford's Law application, especially in a field other than financial fraud, where it is normally used.",NA
"227",226,18,"No.  Well, maybe other data scientists.  The only thing standing in my way are the laws of physics and money.  

&#x200B;

and maybe HIPAA",NA
"228",227,18,"I like the idea...    
For my work (at a distributor/supplier with a grossly underdeveloped appreciation of data integrity/management),  I've thought about implementing the 4 commandments of clean data:

1- Before creating a new entry for a product, search data warehouse if it already exists. 

2- If it doesn't exist, search again 

3- Oh found it did ya?  Good boy. Stop making duplicate entries! 

4-  search by UPC, not text strings.  There's a reason they exist.",NA
"229",228,18,"4 commandments of data-warehousing.
1. Information more than two steps away from its source is information.
2. Data is created from information using definitions.
3. Data is owned, if it is not it is information. 
4. Data = shareable.",NA
"230",229,19,"The ThinkPad has really outdated cpu FYI. While X1 has the 10th Gen cpu but it's an U version which is more of power saving battery, but it's new cpu. Might check the GPU but usually its for deep learning just read, other Than that focus on your liking then CPU. Happy hunting! :D",NA
"231",230,19,"6 cores > 4 cores",NA
"232",231,19,"I went with the P1 based on hardware requirements for my masters program in data science...didn’t want to spend the money on a P52 (this was last year).  

Ended up with the i7 8850H / 32GB / P2000 / 1 TB SSD

I was advised to focus on the GPU which is why I went with the P2000 (now T2000). 

No complaints so far and still a smaller form factor.",NA
"233",232,19,"If you want to take an other approach just create a data science VM on Azure.

You get free credit for a year and after that it's not that much per month depending on the machine.",NA
"234",233,19,"Thank you.
I do want to make sure that if I am using the laptop it will not overheat and die especially the Carbon-7.
And I do like how small and sturdy (from what I have read) the X1 carbon is and very capable with the 6 cores. 
Will need to research this more :)",NA
"235",234,19,"That makes sense but would the x1 Carbon last and be a capable machine in case I want to prototype etc.. in case I don’t have access to the cloud.  

Thank you",NA
"236",235,19,"That I can't really help, been on desktop. [the laptop review ](https://www.ultrabookreview.com/31104-lenovo-thinkpad-x1-carbon-7th-gen-review/) if you're right you should be on the better cpu 2 generation newer.",NA
"237",236,20,"I’d check out [GitHub Pages](https://pages.github.com/). They have a good introduction and explanation of how to use Jekyll and markdown for blogging.",NA
"238",237,20,"Blogdown is a great tool you can use in R. Quite straightforward and with fantastic documentation:  [https://bookdown.org/yihui/blogdown/](https://bookdown.org/yihui/blogdown/)",NA
"239",238,20,"Netlify",NA
"240",239,21,"I work for a 30 person startup with 1 private investor... Maybe I should start looking.",NA
"241",240,21,"I feel these are the steps that will happen:

\- First, OPEX is reduced by laying off contractors and external vendors. So if you're a contractor, you're definitely going to be impacted.

\- Then R&D will be trimmed a lot, but 'few' projects will be maintained. All this depends on CEO's strategy. Unless they pull off a Yahoo (before Marissa Mayer took over), and layoff the whole R&D team which was one of the factors of Yahoo's death. If you feel your boss isn't a great advocate of your research or project, keep an eye out for new gigs or backup plan.

\- Finally, shrinking core personnel to functional level. If the job can be done by 2 Data Scientists instead of 10, then they'll shrink it. If there is no job needed (in essence, let's just stick to our dashboard and we can do analytics ourselves), then you're all gone. Depends on the culture of the firm you're in.

If we're sticking to the vocabulary that Scientist does more research versus application, then I think Data Scientists will get hit more than Data Analysts because of the cost involved to maintain a scientist and low interest in research during recession.

At the same time, businesses with a lot of cash at hand may use this moment to capture the creme de la creme of the industry into their company before businesses bounce bank. There's a great Harvard Business Review article on the supply and demand for job hunting.

""Employers raised education and experience requirements within occupations, and even within firm and job titles, during the Great Recession when workers were plentiful. But perhaps what was more surprising was that employers subsequently [lowered](https://www.sciencedirect.com/science/article/abs/pii/S0927537116300240) these skill requirements during the recovery, as workers became more scarce."" ([Harvard Business Review Link](https://hbr.org/2018/08/research-when-the-economy-is-good-employers-demand-fewer-credentials))",NA
"242",241,21,"What does everyone think about the insurance industry?",NA
"243",242,21,"Are data scientists at FAANG categorized as data analysts?",NA
"244",243,21,"Work in an energy saving company as a data scientist. One of two trained in a main piece of value adding software and run most of their data pipelines. Hoping that a)
As we are in cost saving we are safe
And b) I am fairly indespencable",NA
"245",244,21,"A recession would certainly slow down and reduce number of open positions, but I doubt companies would layoff their current analysts and data scientists. Companies need to be even leaner and rely more heavily on their data to maximize their operations. Plus, it takes a long time to understand the business and become familiar with the underlying data.. it doesn't make sense to get rid of people that know both.",NA
"246",245,21,"For what it's worth machine learning has been used to create an AI driven Etf ticker= aieq

Myself and some coworkers are anticipating it jumping darastically once it accumelates enough market data. No telling when that will be",NA
"247",246,21,"Elon musk?",NA
"248",247,21,"> Employers raised education and experience requirements within occupations, and even within firm and job titles, during the Great Recession when workers were plentiful. But perhaps what was more surprising was that employers subsequently lowered these skill requirements during the recovery, as workers became more scarce.

Well this is no duh... But that's just like saying soon, instead of having one PhD in either CS, stats or math, you'd need two PhD degrees.

Instead of getting by with just knowing Python or R, you'll be expected to know Python and R and Java and C and C++",NA
"249",248,21,"To answer:

>What would you need in terms of skills, knowledge, ability, etc so that your value is higher than your costs (so that your organization will keep you)?

Better demonstrate why your skills are necessary for business by connecting your work to bottomline. Literally right now is all about optics of showing your work is profitable.",NA
"250",249,21,"more and more actuaries are taking up ""data science"" jobs in the insurance industry",NA
"251",250,21,"Facebook in particular is notorious for deliberately mislabeling data and product analysts as data scientists",NA
"252",251,21,"On the other hand in a recession few companies will have the capital to impliment your finding as they don't know going in how much that will be or how much disruption the renovations will cause. In contrast any savings will likely take years to pay for itself so there isn't much of a hurry to get it done. For obvious changes like swapping out CFL for LED lights they might just go through the company doing the work to see what needs to be done

Thus they may consider the cost of the audit, esp for larger facilites, to be a need less oppurtunity cost that can be pushed until there is no sign of recession ahead. Really I would be pretty concerned of I worked at an energy saving company",NA
"253",252,21,"You realize algorithms have been trading since the 1980s? This is no different that every other hedge fund or quant shop out there. And it's under-performing the damn S&P so far. Goes to show, slap ""AI"" on something, suckers will come like flies.",NA
"254",253,21,"Yes, that's correct. 

I don't think two Ph.D.s will be the expectation, but if there's a pool of Ph.D. candidates to pick from, and among them include those with two Ph.D.s, or a Ph.D. and multiple years of experience, then of course they'll be given a greater chance.

In a recession, companies can raise the stake if there's a lot of layoffs and people competing for the same job. But I wouldn't think that job hunting is limited to someone knowing many scripting languages or having Ph.D.s",NA
"255",254,21,"Lessons that I learned from past 08-09 crisis when I first graduated with BS civil engineering and nobody was building **anything** because credit dried up:

1. Know the company you're working for, especially the financials. If the company is private and is not leveraged, it's likely they will keep you. Especially if they're anticipating only a mild recession. They may cut your hours or furlough, but at least you'll have a job. If they're publicly traded, be on the safe side b/c it's likely they're going to want to cut costs to make shareholders happy. 

2. Not all companies within the same sector behave similarly. For example, in 08-09 crisis, credit market froze up and people weren't buying cars. However, this shifted the demand to auto parts sellers and makers. To extrapolate, perhaps tea_anyone can find a sector in which the demand shifts to and starts keeping an eye on them.",NA
"256",255,21,"Good points! It may stop the number of new customers that's true. I don't think we will lose existing companies unless they go bust due to our business model: after installation we split the savings we make. We are also introducing a way ij which customers pay for the audit out of the future savings which I think will be prioritised if we enter full nasty recession.",NA
"257",256,21,"The difference is that this is deep learning. Previous algorithmic trader needed to be programed by hand. This one is underperforming because its new which is typical of deep learning ai",NA
"258",257,21,"The thing is data science jobs are already very competitive to begin with. If you go to LinkedIn openings, you'll see that the number of applicants per job posting is much higher than let's say aerospace engineers or electrical engineers, and most of these engineering positions only require BS minimum vs masters minimum for DS positions. This tells me that when the recession comes within the next year, a lot of people in masters program vying for DS positions will be disappointed.",NA
"259",258,21,"Base tech manufacturers protect themselves somewhat from recession by making contracts for 5 years or more",NA
"260",259,21,"I agree. In layman's term, Data Science is like Analytics on steroid. As long those analytics pipeline and basic business dashboards are running, the need for Data Science expertise is smaller and smaller unless there's consistent R&D going on to explore new frontiers based on the data ingested.

During the recession, R&D shrinks a lot because it's too risky to place money in new ventures, especially in an unstable economy. So when businesses hire during recession, they expect the best of the best because it'll be cheaper (hire one person that can do everything versus multiple people). There are some who are fully aware that that's a ridiculous idea in terms of human resourcing, but in recession, the #1 goal of any business is to try and stay afloat.",NA
"261",260,22,"Great idea!! will you be using R or Python?",NA
"262",261,22,"Thank you for doing this. It will finally force me to use discord.",NA
"263",262,22,"Looking forward to it.",NA
"264",263,22,"From beginner level or are there prerequisites?",NA
"265",264,22,"Amazing. Thanks.",NA
"266",265,22,"Would love to be a part!",NA
"267",266,22,"I joined. Let's learn together!",NA
"268",267,22,"Fantastic!",NA
"269",268,22,"Amazing! I am data analytics student  and would love to join.",NA
"270",269,22,"good idea！willing to join",NA
"271",270,22,"Great idea! Will try to participate.",NA
"272",271,22,"I would love to join",NA
"273",272,22,"Great iniciative, thank you.",NA
"274",273,22,"This is awesome!! I love this community. This is the type of the thing that wouldn’t ever come to fruition in my previous line of work",NA
"275",274,22,"Umm. IM IN ! great idea",NA
"276",275,22,"I'm really pleased how welcoming the data science community is :)",NA
"277",276,22,"You are a nice human /u/kiwiboy94 .",NA
"278",277,22,"Hi, 

I graduated with a b.s. in computer science and minor in math. Unfortunately I was unable to land a job in the software engineering field so I am currently working in account payables, receivables and logistics. Would love to join to learn more as I am hoping to transfer into a new line of work that better utilizes my educational background.",NA
"279",278,22,"Ooooo awesome! I’m doing some R learning in datacamp but can switch to Python. Also doing continuing education. This seems very helpful!",NA
"280",279,22,"This sounds like a great idea, I would like to advise everyone to make a post on medium or another website with a storytelling project so we can all add it to our resume. I can't wait to get started with this data.",NA
"281",280,22,"This sounds like a wonderful idea !",NA
"282",281,22,"Yes please, sounds amazing!",NA
"283",282,22,"Looking forward to this!!",NA
"284",283,22,"Looking forward to see",NA
"285",284,22,"Amazing! Can’t wait to join! I want to switch from a bioinformatician to a data scientist in the near future so I would like to see if my skillset fits those tasks well.",NA
"286",285,22,"Amazing, would love to join it. looking forward to it.",NA
"287",286,22,"interested!",NA
"288",287,22,"I'm not able to join",NA
"289",288,22,"I'm accepting the invite but it does not join",NA
"290",289,22,"Would be very interested in joining this - currently learning python to get into data analytics",NA
"291",290,22,"I like the sound of your idea. Would you be covering intermediate level ideas too?",NA
"292",291,22,"very good idea!!",NA
"293",292,22,"Great initiative!",NA
"294",293,22,"This is great. I'm looking forward to this. Thanks",NA
"295",294,22,"I can’t open it with my account.... I can’t “claim” it because it’s already in use, but can’t open the link with my current account (bad omen for my  CS skills 🥶)",NA
"296",295,22,"Hell yes",NA
"297",296,22,"I really like this idea! Thank you for pushing open-source education. Would prerequisite knowledge of Python programming be necessary?",NA
"298",297,22,"Anyone knows if there is a similar discord for data scientists?",NA
"299",298,22,"what are the scheduled times when you begin these projects?",NA
"300",299,22,"Hey man, thanks this is awesome, just the perfect timing!",NA
"301",300,22,"Amazing! Would love to join!",NA
"302",301,22,"How do I join?",NA
"303",302,22,"I can’t join when I click on the link",NA
"304",303,22,"This is an awesome initiative :) thank you for doing this!",NA
"305",304,22,"Please keep me in this loop.",NA
"306",305,22,"Count me in! Thanks",NA
"307",306,22,"Would love to join where’s the link?",NA
"308",307,22,"I have attached it on this post. Click on the picture",NA
"309",308,22,"I'd love to join in! What time will it be online?",NA
"310",309,22,"Python mainly. Will move towards R if anyone is interested",NA
"311",310,22,"Hope you are consistent and let me know if you need any help",NA
"312",311,22,"As someone who plays games and uses other platforms for work, honestly discord is superior. If they removed the gamer-y language, it would probably gain a lot of professional traction. Discord is an incredible platform.",NA
"313",312,22,"Beginner level. I will be focusing mainly on python",NA
"314",313,22,"Please do. You can join by clicking the link",NA
"315",314,22,"Well, my idea is to gather people (those that can offer to help and those who need help) together.",NA
"316",315,22,"Great to see you onboard",NA
"317",316,22,"Haha thanks",NA
"318",317,22,"Sure click on the link",NA
"319",318,22,"Guess what. I am planning to get into data analytics to get into bioinformatics lol",NA
"320",319,22,"I am sorry to hear that. Can you kindly tell me what error did you see?",NA
"321",320,22,"I have adjusted accordingly. Please try again now",NA
"322",321,22,"Me too. Hop in and let's get our hands dirty! The best way to learn is to apply!",NA
"323",322,22,"For me intermediate level will mean in depth statistical analysis. Is that in line with what you think or are you talking about machine learning 🤔",NA
"324",323,22,">Can you try clicking the link on incognito?",NA
"325",324,22,"Yes. I encourage u to take the effort to learn it. Take your time and reach out to me if you need help",NA
"326",325,22,"One data vis project a week. A notebook walk through with solution for beginners",NA
"327",326,22,"You can click on the link and accept invite.",NA
"328",327,22,"Hi, what error message do you see",NA
"329",328,22,"It's on this post",NA
"330",329,22,"It's up now.",NA
"331",330,22,"Am interested",NA
"332",331,22,"R makes sense, it really is superior to python when used with RStudio.  
Edit: here come the python fan boys downvoting me to oblivion.... so sour.",NA
"333",332,22,"My desire is same as your hope. Would love to learn to use discord effectively for the time being.",NA
"334",333,22,"That's what I've always felt. I've used Slack before after using Discord for probably the past 4 years or so, and it immediately just felt like a worse Discord in every way. It would be so immensely better if more places moved to it.",NA
"335",334,22,"This sounds really cool!",NA
"336",335,22,"If ever you're using R, please post an update. New to R and want to learn from seniors.",NA
"337",336,22,"Gotta love reddit.",NA
"338",337,22,"Can you send me another link",NA
"339",338,22,"I am essentially hoping for something more than just a brief introduction so that I can be exposed to new ideas.. Happy to chat about stats, ML, python ect as long as there will be some new stuff :)",NA
"340",339,22,"The solution was to send an invitation link to myself",NA
"341",340,22,"When I click on accept invite it just opens discord app, I don’t see a link to your server !",NA
"342",341,22,"Awesome. I got on, thanks a lot man!",NA
"343",342,22,"It really depends on what you're trying to do. Though I have always thought Python's plot functions were more complicated.",NA
"344",343,22,"R was my first language. I won't compare R and python, its like comparing apples and oranges. It only matters when your job requires it. Let's say if I work for this company but they insist that I use R or Python then there is nothing I can do. What I can do is adapt.",NA
"345",344,22,"Joined. Thank you",NA
"346",345,22,"Did you try on pc or mobile?",NA
"347",346,22,"agreed. As a data scientist, I tend to pull up R for any analysis, and python for anything that would need to be put in production. Plot() is really nice for quick stuff, but ggplot is worth learning even though the curve is a little steep.",NA
"348",347,22,"Sweet",NA
"349",348,22,"Mobile",NA
"350",349,22,"ggplot in python is a pain in the ass",NA
"351",350,22,"Add me as friend on discord

Kian #7903",NA
"352",351,22,"Done ✅ new to discord so had to google for it, thanks 😊",NA
"353",352,23,"Just use the default. There's nothing special about data science that impacts how to partition.",NA
"354",353,23,"I mean maybe I need more storage to store the data.",NA
"355",354,23,"Always store you data on a separate partition. If your OS gets messed up you can easily format the OS partition and your data is unaffected.",NA
"356",355,23,"Can’t you change the size later on? Pretty sure you can and I even have shared drives between them to it’s not an issue. I’ve never found a and to a Linux partion thought, periodically use Windows one because of software requirements.",NA
"357",356,23,"You run out of RAM much earlier than you do hard drive space.",NA
"358",357,24,"I'm looking for frontend web developers to help out with a project where Oxford University researchers are trying to communicate the results of pandemic modelling software to event-organisers and decision-makers. (Note: not looking for helping in producing data visualisations, we're already working with data scientists, rather looking for frontend web dev work). PM me!",NA
"359",358,24,"All epidemiologists.  But they are good at it already.",NA
"360",359,24,"There’s been lots of posts like this and while I think they mean well, I don’t think there’s much you can do that will be helpful. This thing is more in the realm of doctors/epidemiologists/other health professionals. Unless you fall into one of those, the best thing you can probably do is practice good hygiene and stay away from people if you’re sick.",NA
"361",360,24,"If you got some nice hardware, you can donate spare computing power to [Folding@home](https://foldingathome.org/). [They're running simulations of using drugs on the virus](https://foldingathome.org/2020/03/10/covid19-update/) to search for a drug that will work against it and they need all the GPU compute they can get.",NA
"362",361,24,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [Data science and the Corona virus (r\/DataScience)](https://www.reddit.com/r/datascienceproject/comments/fiadmj/data_science_and_the_corona_virus_rdatascience/)

- [/r/datascienceproject] [Data science and the Corona virus (r\/DataScience)](https://www.reddit.com/r/datascienceproject/comments/fit3m5/data_science_and_the_corona_virus_rdatascience/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"363",362,24,"What is your experience? Sorry but by calling it ""Corona virus"" fired alerts on several levels.",NA
"364",363,24,"Cool idea. How close are you guys to publishing something?",NA
"365",364,24,"This. If you insist or really want, check out the Kaggle competitions.",NA
"366",365,24,"With the students gone, I might consider going in over the weekend and firing up every single lab computer to help compute?",NA
"367",366,24,"Yes you could set it up as a daemon. Although you might want to get administrative approval first.",NA
"368",367,24,"I'm an ask forgiveness not permission kind of person.",NA
"369",368,24,"I'm a big fan of your mentality. Well, since you're ostensibly my CS professor I assume you have an appreciation for the details, so I'll give you the low down:

Folding@home runs jobs that are GPU-parallelizable. The coronavirus jobs in particular are GPU-only. I assume your class's computers are on the older side and only have CPU - they would be put to much better use on CPU-only projects like [Rosetta@home](https://boinc.bakerlab.org/). The Rosetta software is the one that does the initial screening for COVID-19 vaccine candidates, while Folding@home runs more in-depth simulations of those candidates against the actual virus. Both projects are highly regarded with many highly cited papers and the underlying software seeing a lot of industry usage as well.

Rosetta runs on the [BOINC](https://boinc.berkeley.edu/) platform which has a bunch of other scientific computing projects, while Folding distributes its own software. The BOINC software is advantageous for your case in that:

- you can control the CPU/RAM/network usage and hard disk storage of your entire fleet of computers to a very fine-grained level from a master node
- the program can be configured to run as a daemon in the background
- BOINC's network permissions are very tightly controlled and the software runs in a mini virtual machine so it would take a very malicious and skilled hacker to use BOINC to leak a virus onto your computers

Hope that helps. Let's save lives together :)",NA
"370",369,24,"Luckily I was able to upgrade my lab to halfway decent gpus for my data science class!  I can't speak on other labs, but depending how many number crunching machines I want to get going I'll check the ones out of my lab.",NA
"371",370,24,"Wow, that’s amazing! Is this a high school or college class? I paid so much tuition and never got to use a computer lab ugh. Anyway, Folding is definitely your best bet for the GPUs then - maybe you can install both Folding and BOINC and give BOINC the CPU jobs but that seems like a good deal more effort lol",NA
"372",371,24,"I built a GPU cluster in my basement with 6 NVIDIA Titan RTX GPUs in it, and killed the projects I had running and am running Folding@Home on it now.",NA
"373",372,24,"Its a small liberal arts college.  I fought tooth and nail to make my lab what it is.  Without students working hard, I want my lab working hard!",NA
"374",373,25,"There's no simple, immediately useful way to quantify that, and certainly not one that is generalizeable. The book ""A Theory of Fun for Game Design"" by Raph Koster is pretty good about it. If i were you, I would just ask myself which one is more fun than the others in the same column in the same grid instead of trying to assign hard numbers.",NA
"375",374,25,"I work in the industry, and the simple answer is: We don't. Do qualitative research for that. What we quantify is *engagement*. Which features do users engage with, how often, for how long, and at which point during their time with the game.",NA
"376",375,25,"Is it possible to get an estimate of how long it takes to do one run through of each game? If so, you could measure how much more time people spend playing the game above and beyond how much they need to run through it (this assumes they spend more time on it because it was fun). I know this a coarse measure but Steam might have a data set like this.",NA
"377",376,25,"Is this fun as perceived by the subject or that perceived by observers ? Or is it the objective fun ? Or it is the anticipated fun before playing the game ?   How would you measure any of these ?",NA
"378",377,25,"Ranking does makes sense, thanks a lot!",NA
"379",378,25,"Fun as perceived by the person who's playing the game? I am trying to play the games myself to judge that. I understand it would be really biased, but I guess I could roll out a survey for players to fill, but I will still need buckets to fill so that I get standard answers",NA
"380",379,25,"Perception is always subjective.

How would you ask people to grade fun ?  Scale of 0 to 9 ?  

How will profitability to be measured ?  Total revenue from game, total net profit from all sales ?  Net profit from each game sold ?  

Anyway, you have two numeric variables.  Before you decide on definition of each bucket, why not analyze these numeric variables, including their covariance, to see whether there is any bucket formed naturally, rather than subjectively by you.",NA
"381",380,26,"wasn't NeurIPS the biggest AI-DL conference?",NA
"382",381,26,"Hell yes, so excited",NA
"383",382,27,"I would try to get into a graduate software engineering position and take up a part-time masters. If you're in WA I'd check out mining companies who are increasingly getting into data analytics
http://www.omscs.gatech.edu",NA
"384",383,27,"I thought about that, how viable is it? Having a full time job and studying 2 units a semester. I was thinking of maybe studying part time once I’m a bit more experienced, maybe after 2 years or something. Is that online data science masters available to Australians? And not gonna lie I am really skeptical about it, anything online for that matter even though technically my bachelors degree is 80% online except the tests.",NA
"385",384,27,"I guess it depends on where you are in life, it's hard but you can work full-time and do 1 or 2 subs per semester. I'm doing 1 per semester, and have friends doing 2.

It's true that although GaTech is a top 10 CS school in the US, most Australian employers won't know that. Consider the costs and time to finish. You can finish GaTech in 2 years if you take maximum course load (2 in spring, 1 in summer, 2 in autumn).

|Feature|GaTech|Aus|
|:-|:-|:-|
|Cost|$10k|$50k+|
|Time|2 years part-time|2 years full time|

Project 2 years into the future:

Australian option:You study full-time, work part-time. You come out with maybe an industry-relevant summer internship but otherwise no relevant work experience and add $50k+ to your HECS debt.

GaTech option:

You work full-time, study part-time. You come out with 2 years industry experience and earn \~$70k/year for 2 years. You pay $10k for your masters, or maybe less if you claim on tax or get your employer to help pay for it.

There's a pretty active subreddit for GaTech: [https://reddit.com/r/omscs](https://reddit.com/r/omscs)

If you have a really bad GPA you might not get in. You could consider the UT Austin program which uses the GRE for acceptance

[https://www.cs.utexas.edu/graduate-program/masters-program/online-option](https://www.cs.utexas.edu/graduate-program/masters-program/online-option)",NA
"386",385,28,"Your title is misleading. It's as if you were trying to prove that soft skills are more important than hard skills in your hypothesis. The correct way would be to say ""An example of why communication skills matter more than technical skills in certain cases"". Even then, I'd rather say they are complementary to each other.",NA
"387",386,28,"Redacting skills and at least minor sense of aesthetics is where its at.",NA
"388",387,28,"I don't think this is an either or thing.

This is like debating which is more important - the gun or the bullet.",NA
"389",388,28,"Good article and good point for discussion. I agree with the previous comments though, that it isn't appropriate to say communication skills matter more than technical ones, but I do agree they are deeply complementary. Data analysis without communication is pointless.",NA
"390",389,28,"One more thing which is important is critical thinking, for example not comparing very different things.

You could also say that this guy helped more than NASA, for example.

&#x200B;

(the article is great by the way)",NA
"391",390,28,"It's funny because I thought that same exact thing as I read it Wed. Great point.",NA
"392",391,28,"Fact is communication skills are a lot easier to hone than technical skills. Technical skills are always more time intensive so that's what everyone focuses on. Not to say that communication doesn't play a huge role. It does. It's just easily overlooked like most soft skills for that reason.",NA
"393",392,28,"Anyone else on their phone who also thought the graph was a hair on their screen when scrolling down?",NA
"394",393,28,"It seems to me like you lack an understanding of tech",NA
"395",394,28,"Agree, I'd go as far as to call them synergistic in the literal sense. The sum of their products is far greater than either on their own. I've always maintained that the best data scientists I've worked with were greatly enabled by their people skills. In my recent role, I worked with two guys: a data analyst that had incredibly impressive technical chops but no people skills, and a data scientist that didn't have as great technical skills (relatively speaking), but felt comfortable getting out there, building relationships, understanding the business, and building domain specific knowledge.

The analyst just couldn't understand why he was having so much trouble getting the data scientist role. He put so much effort into the technical aspect, and was brilliant, but you could not take this guy anywhere. He provided no value if not in front of his computer. He was a bit of an extreme example so don't take it as me inferring that everyone here is socially inept. But you get the point.

I find a lot of us here focus so heavily on learning stats, programming, machine learning, etc. yet we overlook doing some of the things that are actually uncomfortable and not as natural to us. Toastmasters, leadership development, presentations--you don't have to become the best at these things, but man just getting better at them will have a tremendous impact. My opinion anyways.",NA
"396",395,28,"That's true. You can't edit titles though. I did not think too much about it and wanted to illustrate the fact that you can go a long way with simple tools. 

Obviously you won't cure cancer with beautiful charts.",NA
"397",396,28,"As far as I know, the NASA has not released any PR about them helping the fight against Coronavirus.",NA
"398",397,28,"If you stay in the weeds, you'll stay in the weeds.  Communication skills allow you to progress in a career far beyond being the best technically.  Not equating this to happiness but more towards a higher monetary value position.  Unless of course you work for Google and you're part of the million dollar engineer club.",NA
"399",398,28,"On the flip side, we had “DS”with few tech skills, but could sell ice cream to a person of Inuit decent. He was very capable of explaining quantitiative concepts, and oversold many clients bc he lacked the skills necessary to properly process and analyze the data to know its limitations. He ended up costing the company boatloads of money in not just legal fees (after we got sued by 2 separate clients), but also in lost revenue and the costs incurred by a damaged reputation. Too many experts in pivot tables convince themselves that tech skills are overrated, and that they can be a DS. If you want to succeed, my opinion is that you have to have a healthy tech background coupled with above average presentation abilities, and of course advanced research and statistical acumen. There are no short cuts- at least in the long run.",NA
"400",399,28,"Indeed",NA
"401",400,28,"Yes. Because neither NASA, nor Deepmind are about this",NA
"402",401,28,"They have tried to tackle the the virus problem with Deepmind (https://www.businessinsider.com/google-deepmind-ai-predictions-coronavirus-2020-3?op=1&r=US&IR=T). 

Your point is fair that even though they are tackling the same problem (Coronavirus), the solution they are providing are very different. 

I don't think that it does not invalidate my point that good communication with simple stats can have greater impact. Obviously there are things that good communication can not solve and that advanced ML can.",NA
"403",402,29,"This is my favorite explanation of matplotlib, which walks through both of the APIs in parallel, as well as explaining how to conceptualize the (in my opinion very confusing) figure/axis hierarchy. 

[https://realpython.com/python-matplotlib-guide/](https://realpython.com/python-matplotlib-guide/)",NA
"404",403,29,"I would suggest you use a library that has a sane API and tries to follow the Grammar of visual design, like [https://altair-viz.github.io/](https://altair-viz.github.io/)",NA
"405",404,29,"There is [this](https://matplotlib.org/tutorials/introductory/lifecycle.html#sphx-glr-tutorials-introductory-lifecycle-py) tutorial from matplotlib's documentation.",NA
"406",405,29,"I like [this guide](https://pbpython.com/effective-matplotlib.html) and reference it a lot.",NA
"407",406,29,"I always recommend these 2 resources, hands-down, the best and all you pretty much need:
  
[1](https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb)
  
[2](https://pbpython.com/effective-matplotlib.html)",NA
"408",407,29,"This is good but it doesn't answer the question.  Plus - with the exception of seaborn - there's a graveyard of abandoned Python grammar of graphics plotting libraries.  I always feel nervous using one of these because I don't know if it'll still be supported in three years.",NA
"409",408,29,"From what I've heard, once used to Matplotlib is alright via the OOP interface. I've heard it's clunkier than it needs to be, but Im willing to learn that in order to use something stable. 

&#x200B;

What you've linked looks a bit like plotly or something?",NA
"410",409,29,"Agreed, but Altair has [Jake VanderPlas](https://www.youtube.com/watch?v=vTingdk_pVM) as a main contributor so hopefully it will be around a for awhile. I enjoy using Altair, especially for interactive plots. I would say the hardest part of using it is convincing others to use it.",NA
"411",410,29,"It's Altair. A chart library written for python with the grammar of visual design in mind. 

Matplotlib was ported from Matlab. It gets the job done, but it's not modern. I was once where you are, and I wish I had picked up Altair sooner. Best of luck.",NA
"412",411,29,"For exporting to html while keeping crossfiltering, Altair is king. Plotly hasn’t added it because “hey we have dash” ... yeah but you can’t email dash out or save it for offline viewing. Super annoying. Plotly needs to take note.",NA
"413",412,29,"i do take your point - but i feel as though my lack of matplotlib is kind of an issue, and that I should be able to do it in mpl (to an extent) if required. My mpl skills are pretty poor. Thanks for the recommendation though, I'll check it out. 

Do you find that you use MPL to fine tune any of the plots from altair? do they return mpl objects (like seaborn and pandas) or are they completely separate things? 

Also - is altair your go-to? Or do you find yourself between it / seaborn / mpl

&#x200B;

thanks :)",NA
"414",413,29,"As a Data Scientists, I started with MPL; I wrestled with MPL, and it got the job done. I tried Seaborn, but as soon you need to customize a plot—i.e. a manager asks if you can move a legend, tick-mark, axis-spacing, make custom facets, etc—it quickly becomes rigid. Seaborn is a wrapper on MPL (shiny lipstick on a pig), so if you can't do it in MPL, it's going to be near impossible to do it in Seaborn. 

To answer your question: No you can't go back and forth. They are separate libraries. MPL being a port from Matlab, with a clunky API. It will output to jpg, png, and pdf. Altair shines because it was built on top of Vega-Lite, which is a wrapper for D3. So out of the box, you get interactive plots that output to standalone HTML! as well as jpg, png, pdf. 

To get an overview of plotting libraries in python, watch this ""[The Python Visualization Landscape](https://www.youtube.com/watch?v=FytuB8nFHPQ)"" I  would also highly recommend you watch the first video I linked to regarding visualization grammar. Once you understand why making graphs with the Grammar of Graphics makes so much sense, MPL will look like a rusty swiss army knife; Altair, a Mari Kondo organized toolbox.",NA
"415",414,32,"I think Qliksense or Qlikview has 3D graphs. If not natively, there should be an extension for it.",NA
"416",415,32,"Plotly/dash?",NA
"417",416,32,"Look into Mayavi 

https://docs.enthought.com/mayavi/mayavi/",NA
"418",417,32,"Thanks I will check it out!",NA
"419",418,32,"Thanks I will check it out!",NA
"420",419,32,"Thanks I will check it out!",NA
"421",420,34,"They will use AI to measure your interview performance. Direct eye contact and facial expressions are very important",NA
"422",421,34,"Yes. Decline to do it.",NA
"423",422,36,"Context.",NA
"424",423,36,"Edited. :)",NA
"425",424,37,"Usually, the validation set is used for tuning hyperparameters of your model repeatedly until you reach a satisfying model performance. Then finally you use the test/holdout set to asses model performance. 

The difference here is that some data is leaked from the validation set through manually and repeatedly tweaking hyperparameters. From the test set nothing should have leaked for model training (if you did a separate preproc of the sets).",NA
"426",425,39,"Modeling/analyzing data is the easy part. Retrieving and cleaning data is half the job.",NA
"427",426,39,"SQL, A/B testing, most of my Python skills, a heavy dose of managing stakeholder expectations (graduate analyst)",NA
"428",427,39,"* People will pay a shocking amount of money for shitty data analytics.

* It doesn't matter how good your model is if you can't communicate the results.",NA
"429",428,39,"A lot of business folks will come to our team with very little data and ask for an analysis “where the computer figures out the next big business opportunity.” I’ve had to learn to set expectations about what machine learning is and what it is not, and help people understand that often we don’t need AI, we need a simple dashboard.",NA
"430",429,39,"Yeah during my MSc I was trying to get the highest accuracy as possible, now I just go for good enough.


Also matplotlib used to be enough, now I use tableau.",NA
"431",430,39,"The email/PPT/write up you do to present the model is often more important than the model itself.  If you can't talk in ""exec boss lingo"" how awesome the model is won't matter.

Also, if you can attach a $ value to what your model does, you and the higher up peeps suddenly became best friends.",NA
"432",431,39,"Data science is not magic.  It is a tool, just like business accumen and best practices.  Use it wisely, but dont just wave it around like a wand.",NA
"433",432,39,"Time series analysis sucks",NA
"434",433,39,"How models work in a production setting. I learned that applying the algorithms is the easy part. Being able to use the model over a long period and ensuring the data that will be used for predictions is always consistent is the hard part.",NA
"435",434,39,"Simpler is better

Code rots",NA
"436",435,39,"A good amount of Python, calling API endpoints to get data also a tonne of cloud tech (AWS)",NA
"437",436,39,"Lol straight up",NA
"438",437,39,"Why? I've not actually done any day science work yet.",NA
"439",438,40,"That's really smooth, man! Congrats! Will you share it on git?",NA
"440",439,40,"The code for the layout is pretty long. I tried applying functions with loops but what I got was 22 dropdown menu overlapping one another. That is why I hard coded it myself and ya it took like 10 minutes with some simple copy and paste. Anyone got any solution for that do let me know. :D",NA
"441",440,40,"Plotly is the best!",NA
"442",441,40,"Is this possible with ggplot? Like having the interactive flags?",NA
"443",442,40,"Let’s riot until Plotly supports crossfilter for exported html graphics. Dash doesn’t work if you want to email it!",NA
"444",443,40,"Great work, also go for forecasting algos",NA
"445",444,40,"This is really cool. I would love to see what the graph would look like if all the currencies were normalized to start with the value 1",NA
"446",445,40,"Congrats dude.",NA
"447",446,40,"[https://medium.com/@heeman007/coronavirus-analysis-for-italy-dataset-using-python-data-visualization-eceea6a64840](https://medium.com/@heeman007/coronavirus-analysis-for-italy-dataset-using-python-data-visualization-eceea6a64840)",NA
"448",447,40,"As I am just starting my journey to become a data analyst, I usually work on one data visualisation project every week. I am keen to create a community of people with similar interest and goal. I have created a [discord server](https://discord.gg/MznnbD) for this purpose. We will be working on similar datasets every week and will do code review for one another. Do join if you are interested and please help me spread the word since it's very new.",NA
"449",448,40,"Nice. Next level is live charts!",NA
"450",449,40,"Sweet",NA
"451",450,40,"Yes sure. You can get it [here](https://github.com/kianweelee/Time-series-chart--Foreign-Exchange-Rates/blob/master/README.md).",NA
"452",451,40,"[R Shiny example](https://blakeawb.shinyapps.io/algobeta/)

connected to yahoo finance/more interactive because you can type in any ticker",NA
"453",452,40,"You can use the ggplotly method to convert a static plot created in ggplot into an interactive plotly plot",NA
"454",453,40,"Have you tried subplots yet? That’s a joyous time",NA
"455",454,40,"Shiny widgets? Or R2D3?",NA
"456",455,40,"close! [dygraphs](https://rstudio.github.io/dygraphs/)!",NA
"457",456,40,"There's always another one! So many wrappers / interfaces for javascript with shiny. I'll have to look into that one, it looks much simpler than R2D3.",NA
"458",457,40,"Yep I initially explored using D3 but liked dygraphs for this use case",NA
"459",458,40,"Yeah, D3 gives a lot of flexibility, but the learning curve is steep. Still on my list of things to learn though. This looks fast and clean, so a good option. Very cool, thanks for the new library!",NA
"460",459,42,"9 years in. I used Google this morning. I have to look up details of math or methods used in most papers I read. I make basic coding errors. Welcome to machine learning. We’re all still learning and figuring it out.",NA
"461",460,42,"When I started, I had about five years experience with Python, working in a small company where Excel was the only thing they had ever known. I remember once a Project Manager dumped several dozens csvs on me to run some basic analysis on. He even added sarcastically ""I want it done before you go home tonight"".

    for i in os.listdir(""./csv""):
      df = pd.read_csv(""./csv/""+i)
      do_stuff(df)

I went back to him just over an hour later with all the info and charts. His comment was: ""wow, congratulations, it would have taken me about three days to do all that"".

^(Edit: formatting, code.)",NA
"462",461,42,"I'm only 2 years in to working but I'm an order of magnitude more competent. Second job I required almost no handholding at all. First job I couldn't do anything without direction. The soft skills gained in my first job (\~16 months) were invaluable despite not being a very data oriented company.",NA
"463",462,42,"I’ve been a data scientist for 3 years and what I learned is the data scientist job is very vague right now. You often spend more time just connecting business problems and how ML or data can help solve it than building models. With that said, I spent a year with training wheels before things clicked and I was able to become a senior member of my team. Now I lead R&D projects and oversee a small team.",NA
"464",463,42,"How were you compared to your first position in the field though? I’m sure your Googling skills are much more efficient now than before. But glad to hear your comment; cheers~",NA
"465",464,42,"Guess he wasn't aware that Python is about to destroy his career.",NA
"466",465,42,"Chances are what they wanted wouldn't be that complicated in excel either.",NA
"467",466,42,"It probably won't though. Just because it can be done in python easier than Excel doesn't mean it will catch on. Lots of places perceive it as significantly more difficult and most businesses are pretty slow to change at all especially big changes like that.",NA
"468",467,43,"Wow you're in 11th and already doing internships that's amazing",NA
"469",468,43,"Hm, got any ideas?",NA
"470",469,43,"Imo don't do internships rather focus on improving your skills. Do some research maybe and publish a paper",NA
"471",470,43,"I'm looking to work with a professor to write a research paper, any thoughts on that?",NA
"472",471,43,"Try applying to some IITs. They have summer internship programs",NA
"473",472,43,"I checked, but they seem to be only for undergrad students",NA
"474",473,43,"Never hurts to ask. Worst case scenario, you're exactly where you are right now.",NA
"475",474,43,"I mean the requirement is having an undergraduate degree or 2/3 years in uni, you can't apply if you don't have one of those.",NA
"476",475,44,"Wow, congrats! I don’t know what program would be best, but I am really inspired by your career change at 49! I’ve just turned 40 and am thinking about a PhD program, but I’m torn because of how long it will take, and whether I’d be subject to ageism once I finished the program.",NA
"477",476,44,"I would honestly pick whichever based on which of those countries I would rather live in. Scotland is short flights to all over Europe, New Zealand absolutely beautiful outdoors activities",NA
"478",477,44,"http://www.omscs.gatech.edu",NA
"479",478,44,"Look into the MSc in Analytics from Georgia Tech. By far best value in this field.",NA
"480",479,44,"Can you expand on what your pursuits are, do you want to do more Individual Contributor role or Management?

Given your level of experience over the past years, do you think you have a solid/good grasp on data applications, tools and scripts (like Python and its libraries, Excel, R-Studio, etc.)?",NA
"481",480,44,"New Zealand is where I’d go solely based on location weather and the good work they’re doing with data, open data and in general over there.",NA
"482",481,44,"And here I am at 26 finishing MS in data analytics thinking about if I should do a PhD but scared of the time it takes",NA
"483",482,44,"Thanks! At least for me, a PhD is really too long. But if it is your call, of course you can!",NA
"484",483,44,"Thanks. I’ll take a deep look at it.",NA
"485",484,44,"I have manager experience. At this point I am not really sure about the work prospects. I am open to both paths but I think the manager one could be more “logical”.

I have a decent knowledge of python, finished Datacamps “data scientist track” (which is not really useful) and I’m currently taking Andrew NG deep learning specialization on Coursera.

Ive always loved programming and I’m kind of fascinated with AI ( specially computer vision).

I have no problem earning less than what I used to.

Thanks!",NA
"486",485,44,"The time will pass either way.  There is something special about having a terminal degree.",NA
"487",486,44,"fuck ""logical"".

I am doing the thing where you wish you could do something so you go around encouraging others to do it. In my mind being controlled or even guided by where you have experience is what you do when you need to find a job to support yourself. If that's not a main concern, I don't really see why you wouldn't take the time and effort to follow a path of pure interest (which might align with your experience, that'd be sweet.",NA
"488",487,44,"I'd say go for AI given your experience. The program is more geared towards mathematical/theoretical knowledge given the curriculum, but given your experience and knowledge in terms of application, you'll benefit a lot more from it versus the Data Science program.",NA
"489",488,45,"The question of whether its a ""good"" class really depends on what you would do instead if you didn't take this class. Anyways, casting that aside, typically database concepts are extremely valuable for a data scientist. 

Many data sets are poorly structured for analytical purposes and/or are stored in multiple legacy systems. This is particularly true in larger and older companies.  Thus, as a data scientist a fair amount of your work will be identifying what data exists, obtaining those data sets, combining them and then cleaning them. All of this occurs before you can even begin your analysis. 

Databases can be a great tool for combining and organizing these disparate data sets. Thus, some knowledge of database design plus very strong SQL knowledge is definitely advisable for most entry-level data scientists. 

Many new data scientists think that their success will depend on their knowledge of the latest and great machine learning algorithms. However, in my experience, the most successful junior data scientists are the ones who have knowledge of data engineering, data cleaning, good software development practices and the ability to productionalize their models.

Hopefully this was helpful!",NA
"490",489,45,"Got it! Thanks a lot :))",NA
"491",490,46,"Have you tried using tabulate?",NA
"492",491,46,"I have not, but I'm wondering if its a setting somewhere I need to change because it prints fine on the gnome terminal on my vm.",NA
"493",492,46,"One option is   
`pd.set_option('display.height',1000)` 

`pd.set_option('display.max_rows',500)` 

`pd.set_option('display.max_columns',500)` 

`pd.set_option('display.width',1000)`  


or   


`pip install tabulate`  
`print(tabulate(df, headers='keys', tablefmt='psql'))`",NA
"494",493,47,"Why dont you find the data for this and perform an analysis to answer your question?",NA
"495",494,47,"In my (limited) research, Chicago and parts of Texas appear to be up there. For similar qualifications and experience levels, salaries look close to Bay Area / NYC levels. I’m not completely sure about different cities in Texas, but I know in Chicago you can find rent not super far from the cbd for significantly cheaper than in those two.",NA
"496",495,47,"I guess you're missing one factor, which is, whether you're looking at medians or say like the 95th percentile. For example, in bay area/nyc, if you're around the median (say 90-130k?), you're not left with a lot after taxes and cost of living. But if you make ~250k (or more), it's a very big difference if you don't increase your spending.

I have no idea what the high end of salaries look like, at lower cost-of-living places.",NA
"497",496,47,"Netherlands. Find a job in the Amsterdam, go live in some other town like Utrecht for cheaper rent. Nice public transports, prices are pretty ok.",NA
"498",497,47,"Work in NYC and live in NJ. Just be okay with a 3 hr daily commute. But you're looking at $300k.",NA
"499",498,47,"This is an excellent data science project. Post your results when done and I'll point you to the answer.",NA
"500",499,47,"I'd guess Berlin is pretty good. Salaries are fairly low, but cost of living is really low for a big city.",NA
"501",500,47,"AFAIK, Berlin.",NA
"502",501,47,"I'd give you a gold medal if I had one :P",NA
"503",502,49,"Not for on site jobs, no. For remote, no idea. 

Will it crush the market, in certain areas and in others data will become very important as you need to understand where to cut and where to invest.",NA
"504",503,49,"We are temporarily mandating online interviews for all rounds, all roles and I imagine many others are as well. But prior to this many managers didn’t consider online at all even for a first interview (after initial screening by phone). There have been a few hires here and there without an in-person interview at some point, for research positions where the manager and candidate had already met at conferences or collaborated in some way. I know others have said they are reluctant to make a final offer without an in-person. 

I expect when thing start returning to normal many managers will be more open to online interviews because a lot of reluctance has been either avoiding the unfamiliar or based on bad experiences several years back, and they will see that it is fine. But I don’t expect much increase in offers without an in-person meeting at some point in the process once we are back to normal.",NA
"505",504,49,"I just finished up the recruitment process with a company, had five interviews all online/over video. I don't think this was the norm for them though, from my understanding, most of the employees were working remotely as well.",NA
"506",505,50,"Every companies stack is different so yes. There’s many company’s using gitlab and many using Azure.",NA
"507",506,50,"Thanks!",NA
"508",507,51,"Look for a political science research methods textbook. Like most fields of science, political science makes extensive use of traditional statistical methods.",NA
"509",508,52,"Don’t take this as a sign that statistics is not important anymore. Just a few days ago I responded to a thread about using p values to do feature selection. This is clearly a bad idea, but only if you understand how the inferential process works. 

Don’t get me wrong; AI and ML are amazing and are capable of incredible things. But they are not a wholesale replacement for scientific thought, and that includes statistical and inferential thought.",NA
"510",509,52,"Which movie came out at the end of 2011?? /s",NA
"511",510,52,"This graph is flawed; it doesn't say that statistics is dropping in overall popularity. All it says is that relative to AI, it hasn't kept up speed. It's very likely that stats, at an absolute level, has also become more popular.",NA
"512",511,52,"For a lot of businesses ML has been great because you don't need to spend as much time doing research and modeling work. It learns from the data and there is a lot of data available these days thanks to technology advancements.

Traditional statistics was often developed for smaller datasets where you have to include some prior knowledge, such as to assume a family of distributions.

Also, I'd argue some statistics concepts have been claimed by AI, however, they're still well within the body of knowledge that is statistics. Particularly from the Bayesian realm with MCMC and Bayesian nets and whatnot.

I caution anyone who assumes you can simply go all in AI and forget about the statistics. It's true that the practical results coming from ML are running in front of statistical theory right now, but without statistics we'll never understand why some of the more cutting-edge ML algorithms really work.

There's something to be said for complex adaptive systems or computational intelligence work as well. They'll likely help us understand more about what learning is and how various systems achieve it.",NA
"513",512,52,"Is this just random people looking up things or is it the things data science people are looking up? 

I work in the field and I find myself looking up a lot of stats I should really remember from school.",NA
"514",513,52,"Why are searches for statistics so cyclical?? It’s almost the exact same shape over and over and over again.

I wonder if it has anything to do with searches for stats spiking during academic semesters of which there are two each year, then dipping in the summer. FWIW the pattern seems to fit that explanation.",NA
"515",514,52,"Doesn't this mainly show that marketing hype beats technical when it comes to Google searches?",NA
"516",515,52,"I’m questioning the reliability of this data on the grounds that AI appears to be a common word in another language.

This would also explain why the AI line is so flat (except for the one month where a song was released with AI in the title [(see here)](https://trends.google.com/trends/explore?date=all&q=AI,ai%20se%20te%20pego) ) despite the fact that most people’s intuition would tell them it’s greatly increased in popularity over the past 5 years.",NA
"517",516,52,"So why is statistics most popular on New year? Or in general periodically?",NA
"518",517,52,"Well said!",NA
"519",518,52,"I'm surprised that ds and ml didn't spike more in the last few years",NA
"520",519,52,"Looks like a Gaussian process to me.",NA
"521",520,52,"Obligatory AI ≠ ML ≠ Statistics",NA
"522",521,52,"singularity is coming",NA
"523",522,52,"I'd imagine a lot of this can generally be attributed to three hype behind the Data Scence and AI/ML buzzwords by people who couldn't tell you what a Neural Network is.

Last year everybody was talking about blockchain, before that cybersecurity, etc.",NA
"524",523,52,"I might be really missing something, but isn't AI stats? Like isn't almost all AI an inherently stats based process while not all stats are AI?",NA
"525",524,52,"Is it me or shouldn’t we see growth through 2019 with AI? And even a spike in 2018 and 2019 as the IoT took off. Looks relatively flat from 2016 through 2019 which I would anecdotally say is off.",NA
"526",525,52,"The fluctuation in the Statistics graph is interesting- does anyone have I guess what the course for that is ?",NA
"527",526,52,"Correlation not causation",NA
"528",527,52,"For sure!!!! 100%. This is really just hype IMO. AI is not better than stats. My opinion, all these to hand in hand.",NA
"529",528,52,"Were they picking features that only had a small p-value ?

Could you elaborate on why this is a bad idea ? Thanks.",NA
"530",529,52,"I think knowledge of statistics is what separates a good data scientist from the best one.",NA
"531",530,52,"I need to start learning stats...",NA
"532",531,52,"I was curious so I dig a little digging; it was a song not a movie.

“ai se eu te pego”

The spike on the graph corresponds to the release date of the song. The related searches for the song also spiked.

[comparison of the search terms](https://trends.google.com/trends/explore?date=all&q=AI,ai%20se%20te%20pego)",NA
"533",532,52,"The Alvin and the Chipmunks movie",NA
"534",533,52,"Exactly, it's all relative!",NA
"535",534,52,"True, though we don't know how Google defines ""popularity"". It may be as simple as ""number of searches containing the string"".

Also, if you filter for any english speaking nation or Scandinavia, statistics are still ahead - though on the decline. This may indicate that this graph generated by a 30 second Google Trends search may not be scientifically sound.

I suspect the two letter term ""AI"" vs the term ""Statistics"" is not the most fair.",NA
"536",535,52,"Absolutely a possibility. But compared to AI in absolute terms, it is now lower.",NA
"537",536,52,">Also, I'd argue some statistics concepts have been claimed by AI, however, they're still well within the body of knowledge that is statistics.

Well, considering ML is all just applied statistics, it seems silly that people think statistics has become less important or something.

I was just in a data science class, actually. The number of people completely unconcerned with understanding statistics, who then go on to try to apply ML algorithms to things and then don't understand why it doesn't work, is laughable.

Even worse is when it technically works - i.e., it spits out an output that looks like something useful - but is still applied wrong. God that's scary and makes me worry that there are policy wonks out there doing that, who are then informing policy makers with their pretty-but-incorrect graphs... God damn that is scary to me.

And the worst part is, if you are hired on as the only data scientist for a company or organization? Who is going to be able to parse your choices apart and tell you if you are wrong?

The worst part of this whole ""AI revolution"" is that it isn't even AI. It's just statistical methods with pretty python wrappers so MBAs who got all Cs through school can mis-apply them more easily.",NA
"538",537,52,"Traditional Statistics is just as important for large datasets. For example, look at how this dataset is biased. Back in 2004, Google was not used as much by the general population and was more likely to be used by researchers and students, hence more searches for statistics. Science, technology, engineering, mathematics, chemistry, biology, and physics are seven other Google search terms that have seen similar sharp drops since 2004, for similar reasons. AI has become more popular within all groups since 2004, as well as becoming a buzzword that is commonly used by the general population.

If you neglect Statistics, you might incorrectly think based on this graphic that Statistics is less popular now than it was in 2004.",NA
"539",538,52,"I am considering whether what we're seeing is not something replacing something else, but rather that the distinctions and definitions of various fields are moving.

Right now there is this thing happening where there is a lot of overlap between computer science, statistics, optimization, adaptive systems, biology and control theory.

One of the things coming out of this mix of fields is AI (or ML or whatever you want to call it). There are other non-ai ideas being born out of this melting pot as well.

I expect that we will see new categorizations of the same underlying science within 10 or so years, just like what happened with computational biology.

It just doesn't make sense for a modern statistics graduate to not know some AI, and it certainly doesn't make sense for a Data Science grad to not know statistics. Both Statistics and DS benefit greatly from learning optimization, and computer science is a must for both.

Eventually you get to a point where the amount of implied additional fields a statistician is expected to know makes it more convenient to just redraw the lines.

These kinds of shifts are nothing new. The word ""engineer"" initially meant ""someone who works with engines"", after all.",NA
"540",539,52,"Just want to state my appreciation for this. This was a great comment. Thanks.",NA
"541",540,52,"Google searches. Back in 2004, Google was not used as much by the general population and was more likely to be used by researchers and students, hence more searches for statistics. Science, technology, engineering, mathematics, chemistry, biology, and physics are seven other Google search terms that have seen similar sharp drops since 2004, for similar reasons. AI has become more popular within all groups since 2004, as well as becoming a buzzword that is commonly used by the general population.",NA
"542",541,52,"Could be either. We don’t know anything about the people.",NA
"543",542,52,"Yes. Some other comments have said similar things. Fall and spring semesters picking up with lows in the summer.",NA
"544",543,52,"There are many many much more influential explanations IMHO.

How about summer vacation in companies?         
Stock reports?           
Sports events?        
Political events?           
Public health events?",NA
"545",544,52,"Marketing hype is more popular for sure",NA
"546",545,52,"It's Portuguese, ""Ai"" is not exactly a word, more like an onomatopeia.

It can mean the same as ""Ouch"", or something like ""Oh"" in surprise.

In this song the lyrics go ""Ai, ai, se eu te pego"", which can be translated roughly as ""Oh, oh, if I get my hands on you"" (In a sexual but only a little rapey way, not a fistycuffs way).",NA
"547",546,52,"You’ll notice it spikes in April and November. Someone mentioned that’s when midterm/final exams might be. It shrinks in July (summer break). That’s one theory.",NA
"548",547,52,"Obligatory, why not?",NA
"549",548,52,"Yes.

But with the power of solving much more difficult problems at costs of reduced explainability.
If talking about NN.

However many algorithms don't fit into the traditional statistics realm.",NA
"550",549,52,"That’s how I feel. Albeit, I’m lesser knowledgeable in AI of all these subjects.",NA
"551",550,52,"Maybe school related searches?",NA
"552",551,52,"ugh.",NA
"553",552,52,"-_-",NA
"554",553,52,"Honestly, what is the difference between machine learning and statistics? They seem the same to me.",NA
"555",554,52,"What are the years on the data labels for the stats(blue) series?",NA
"556",555,52,"https://www.reddit.com/r/datascience/comments/ffyqns/when_to_use_statistical_tests/fk1ntx0/?context=3",NA
"557",556,52,"See this stackoverflow post

https://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection?noredirect=1&lq=1

Someone said that if you're using automatic selection to avoid having to think, then what are you being paid for? I tend to agree with that view.",NA
"558",557,52,"You and about every other data scientist I've met.  To be fair, I'm quite anal about stats and so take all that I say with the knowledge that I like stats more than I like ML.  I see stats done poorly all the time in scenarios where it really should be done defensibly (like in medicine) and so I like to be vocal about it.",NA
"559",558,52,"This why we have to be careful of the assumptions we make when doing bag of words analysis.",NA
"560",559,52,"Now do Nov 2014",NA
"561",560,52,"I love that song!! SO many good memories dancing around in college ahh",NA
"562",561,52,"Sure, but that doesn't really imply the fall of Stats. It could just be the rise of AI. Simple, statistical observation from the graph: no AI involved :)",NA
"563",562,52,"Yeah I agree. ML is new branding for things that were being studied in multiple areas.

I think the main problem is that statistical learning theory doesn't seem to jive with some empirical results right now from, for example, neural nets. So some people have the mistaken idea you can simply abandon statistics because CS is ""getting results"".

I hate to break it to them, CS is also applied math. A lot of people think you can simply learn to code or hook things together and skip over the hard stuff.

Even more concerning, there are legitimately people who think we can forget all about understanding ""why"" something works as long as it does (or appears to).",NA
"564",563,52,"Have a BS and MS in Data Analytics, spent years building the mathematic and statistical skills to understand the inner workings of probabilistic models from scratch.

It is staggering how many people refuse to even see the relationship between statistics and machine learning.

More infuriating is the people that go to a data camp, learn how to do some basic EDA in R and then run out and apply to every data science job they can find.

I’m sorry, 6 weeks working on ‘bikes of San Francisco’, iris characteristics and titanic dataset does not make someone a data scientist.  These camps are bad for data science as an industry.  It cheapens the name and when they inevitably mislead some business leader with an overfit model then fail (bUT tHE PrEcIsIoN wAs 97), it is data science and machine learning that take the fall, not the person who didn’t understand the tools they were using.",NA
"565",564,52,"Great insight !!!",NA
"566",565,52,"I disagree with that hypothesis. In 2004, [google](https://www.zdnet.com/article/comscore-on-top-search-engines-for-december-2004-google-35-yah00-32/) was already the dominant search engine with a market share of 44%.",NA
"567",566,52,"Well we know about the data, or at least we should. If it's just a report of searched terms then it's everyone. If it's a more specific survey then we should know more about the population.",NA
"568",567,52,"Exactly. Very cool.

I wonder if more specific stats jargon would be similarly distributed. Like variance or skewness. I would guess so, provided they are terms one might reasonably encounter in HS/undergrad stats classes.",NA
"569",568,52,"You can also see that they are more likely be taught during the fall semesters (which is the default at all three university in my home area) with the retake exam at the end of the spring semester",NA
"570",569,52,"I might be inclined to think that it also lines up with American election times as a factor in search like:

""Election statistics""

""Midterm statistics""

Ect.",NA
"571",570,52,"Oh, that makes sense.",NA
"572",571,52,"The reduced explainability is a very good point. And I know they’re not traditional, but for the most part doesn’t that make them a subset?",NA
"573",572,52,"Yeah I suppose this is in many ways a big hype thing.",NA
"574",573,52,"they are",NA
"575",574,52,">Honestly, what is the difference between machine learning and statistics?

Intent.",NA
"576",575,52,"exactly!",NA
"577",576,52,"Machine learning is a term you use about statistical methods when you don't feel like explaining them.

Also when the code takes long to run. My feeling has always been, if you think about it, linear regression is machine learning. We just don't think about it as such since the line fit is so quick.",NA
"578",577,52,"I think you are in the wrong sub tho",NA
"579",578,52,"Thank you!",NA
"580",579,52,"Google Trends does show a drop in Statistics searches without being compared with anything.

https://trends.google.com/trends/explore?date=all&geo=US&q=Statistics

The explanation is that sixteen years ago, Google Search was used more by students and researchers, and those rich enough to afford a PC. These groups are more likely to be interested in Statistics. Now the search pool is much more biased toward the general population. If anything, statistics is just as important. I just used statistical analysis to explain my points.",NA
"581",580,52,"As far as I know you're a bot buddy.",NA
"582",581,52,">Even more concerning, there are legitimately people who think we can forget all about understanding ""why"" something works as long as it does (or appears to).

I think they found out about things like PCA and were like ""Oh none of it matters any more! I don't have to know what features mean! Yay!""",NA
"583",582,52,"There is a big difference between predictive modeling and inferential modeling! You hit the nail right on. I think inferential modeling is still v. important in research and business decisions with few, discrete outcomes and few observations. Folks in academia def. get that.",NA
"584",583,52,"It really is tarnishing the name of the proper graduates who have studied and can explain the statistics. 

I'm from Australia, and it seems like noone knows fuck all except that ""hey cLasSifIcAtIon AccUrAcY wAs 98.4%"" (yes you muppet fuck if you train using your train+test and then test on test you're going to overfit)",NA
"585",584,52,"Right. But in order to use Google (or any search engine), you have to have a computer. Computer ownership has risen significantly since 2004, meaning that more people, not just the rich and educated, can do Google searches. That's the trend I was trying to point out.

https://www.statista.com/statistics/748551/worldwide-households-with-computer/

As a side note, the PC penetration numbers do not include mobile devices, which can also perform Google searches and were virtually non-existent in 2004.",NA
"586",585,52,"Oh well it’s google users which could be anyone. Sorry I think I miss understood your first comment.",NA
"587",586,52,"No, statistics is both inference and prediction. Always has been. Machine learning is computational statistics. The difference is increased emphasis using algorithms for variable selection and transforms, not just calibration.",NA
"588",587,52,"I'm sure you love to flaunt your MS (Geostatistics) flair when you insult people without responding to their points.",NA
"589",588,52,">""hey cLasSifIcAtIon AccUrAcY wAs 98.4%"" (yes you muppet fuck if you train using your train+test and then test on test you're going to overfit)

That and not accounting for class imbalances. If you're dealing with a binary classification problem where only 2% of your data is the target class, you can achieve 98% ""AccUrAcY"" by saying that instances which are in fact the target class are not, effectively accomplishing dick.

Weight (if necessary), train, test on validation data, THEN test on your hold out set dawg. Use confusion matrices, not just the AUC for evaluating classification. Do a fuckton of various tests to determine how robust your model is, then do them again if there isn't a strict deadline to adhere to.

If you fail to follow these you will likely cost some business quite a bit of money when you inevitably screw the pooch.",NA
"590",589,52,"Lol ""muppet"". Obviosly aussie.",NA
"591",590,52,"Haha, it's fine. I kind of figured that was the case.",NA
"592",591,52,"Honestly, that you and I would disagree on this has no bearing on how either would be used.  At this point, its all semantics.",NA
"593",592,52,"I'm not exactly sure what you mean there, did you think I was trying to insult /u/AsianJim_96 ?

They made a comment about simple observation without any need for AI. I just made a joke that from my point of view he could might actualy be an artificial intelligence (a bot), since I have no way to tell.",NA
"594",593,52,"Oof that went over your head.",NA
"595",594,52,"Worst part is that this is all pretty much common sense really, you don't really need to be good at statistics to understand why you need to do this.

As a Geologist I read a lot of papers applying ML to geology problems and very often the methodology is fo flawed I don't even understand how it got published. Things like ""our regression model achieved an R² of 0.98"" and then you look and see it's the training dataset.",NA
"596",595,52,"> Do a fuckton of various tests to determine how robust your model is, then do them again if there isn't a strict deadline to adhere to.

Could I pick your brain on this? Could you elaborate. I'm having some difficulty picturing what you mean here. If you could give some examples that would be great!

Would you incorporate those tests into unit-tests before launching a model in production?",NA
"597",596,52,"Simple example: You have a multivariate regression model. After training and testing on validation data,  you want to do tests such as the Breusch-Pagan test for heteroskedasticity, the VIF test to check for  collinearity/multicollinearity, the Ramsey RESET test, etc.

&#x200B;

Not as simple example: Adversarial attacks to determine the robustness of an image recognition program which utilizes a neural network. See  [https://www.tensorflow.org/tutorials/generative/adversarial\_fgsm](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm).",NA
"598",597,52,"Thanks for the reply!
I figured as much for a regression setting. Didn't think about non-parametric robustness tests.

Would you do the same robustness tests for multivariate regression as you would in a MANOVA? (Did most of my robustness checking on smallish sample sizes there, main goal was inference though).

Also, isn't it better practice to do multicol checking beforehand, or is it even better practice to do before and after? Kind of ashamed I havent heard anyone in my department talk about VIF though, thought I was the only one inspecting those values.",NA
"599",598,53,"Did they stealth release this? There's nothing I could find about it but it seems there are three devs already certified.

EDIT: Whoa, it was just announced during the livestream.",NA
"600",599,53,"Anyone attempt it? How was the experience? What kind of questions/ problems in the test? Share few questions samples?",NA
"601",600,53,"I'm kind of curious how difficult the test is. Anyone want to pony up the $100 and report back?

I don't do recruiting, but I do take part in hiring and it could serve as a ""feather in the cap."" Though, in place of real world experience, I certainly would prefer a well stocked github or demonstrable experience with Kaggle.",NA
"602",601,55,"Islr",NA
"603",602,55,"Introduction to Statistical Learning

Edit: see ISLR comment",NA
"604",603,55,"Kevin Murphy’s Machine Learning: A Probabilistic Perspective. It’s about as comprehensive (probably more so) than Bishop’s classic Pattern Recognition and Machine Learning (which I’d also recommend).",NA
"605",604,55,"Hands on machine learning 2nd edition",NA
"606",605,55,"Definitely this one, not even close.",NA
"607",606,56,"A quick search of job requirements will give you an answer. Many roles require a MSc/PhD as a minimum.",NA
"608",607,57,"I assume you've seen this thread? 
https://stats.stackexchange.com/a/357749

I think the emphasis here is *can have high variance* - while I would likely have had to experiment with the modeling algorithm it's intuitive that some models and datasets are especially impacted by outliers, and LOOCV will by definition have folds where an important outlier is excluded.",NA
"609",608,57,"So I want to go through the stack overflow post, but I am pretty sure ESL s point is 

""An average of B i.i.d. random variables, each with varianceσ2, has variance 1/ B σ^2. If the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation ρ, the variance of the average is (Exercise 15.1) ρσ^2 + (1−ρ)/ B σ^2. (15.1)

On page 588 in discussing random forest.

Would be worth going through the post example fitting this...",NA
"610",609,57,"OP linked that exact thread actually.",NA
"611",610,59,"Make your mathematics and statistical part strong. Learn a python or R. Learn the important libraries like pandas, tensorflow, numpy etc choose the andrew ang course for the start it gives you the best intuition about the mathematics.",NA
"612",611,59,"Um so if it is not useful, don’t continue. You can find much more useful things to put on your resume!",NA
"613",612,59,"If you're not getting anything out of those resources, search for those same topics on youtube, or start another free course. Datacamp is a really popular one. StatQuest and 3Blue1Brown on youtube are great for stats. I think building your first project on Github would be a better use of time than a certificate; it directly showcases your abilities.",NA
"614",613,59,"I’m currently going the book route after learning some python first. I was considering pursuing that certificate too but time is scarce. Would you mind elaborating on why it feels useless to you? Cuz I had similar feelings towards other online courses but thought this one would be different.",NA
"615",614,59,"Like projects that you have done by yourself in your free time",NA
"616",615,59,"I will check it out for sure.Thanks for the comment.",NA
"617",616,59,"Can you give an example ?",NA
"618",617,59,"I figured that aswell.Thank you for the comment.",NA
"619",618,59,"Yes, I also wanted to take that course and would love a honest review. I know they use IBM WATSON. 
There is also john hopkins course which teaches data science using R, would that be an better option?",NA
"620",619,59,"Well after the second course there are a lot of errors,like sometimes they use say stuff but show another on the screen,for the second course in the specialization its about open source tools for data science,and the links are all over the place the instructions arent clear,and almost everyone had trouble actually getting to the point of opening a project in these tools.The third course is about the data science methodology the case study they gave to demonstrate is really bad and you just can't understand it I went through it 3 times then checked out if people were also finding trouble with this and again almost everyone did the explanations were just missing.The fourth course is about python,now I know python but I watched the very short videos to refresh my memory and the explanation was very bad,barely touches the surface and fast.",NA
"621",620,59,"Maybe I didn't write a good review,but simply its not well made.",NA
"622",621,60,"I took it Fall of 2019.  The ML course has some math, but way easier and also unlimited attempts at the problem sets. The videos aren't as long, it's an easier class all around.

F that prob and stats class, it's way more work than the others.",NA
"623",622,60,"Thanks! Great to hear! The course is kicking my butt, though I have two units left. I will end up with 100% on the quizzes and programming,  because of the dropped negative grades (which I needed a few!).  Oddly, the hardest problems have been gotcha style multiple-choice questions. And on the occasional lecture, I would spend all weekend on one quiz to fail it!




I am really nervous about the final, I feel like no matter how much I study I select the wrong math to solve a problems, especially ones that start off with ""Let 𝑋, 𝑌 be two independent __ random variables."" I'm fine with one - Ha!




Did you take the Spark course too?




If you have any additional advice, I'd welcome it.",NA
"624",623,60,"I did the practice exam until I got everything correct. Half of the questions I had to look at the answers, then make sure I had the right notes and formulas, then did the question again.

I also did a couple extra problem sets to replace some bad grades. I had high 50s in points before the final. This meant I only needed a 15 or so on the final to pass.

You will be very very relieved when it's over.

Haven't take spark yet.",NA
"625",624,60,"Thanks!",NA
"626",625,61,"Depends on you tbh. The program is ok, gives you an idea of what to do. It’s putting it together and doing it that’s hard. I find graduates of those programs definitely are still entry level so the question is are you willing to be an entry level DS?",NA
"627",626,61,"I think so. I could take a pay cut to an extent. I work for a large organization which places a lot of value on employee development. We do have a business intelligence area so I do think there is some opportunities to do a secondment and gain some experience in the field",NA
"628",627,61,"That's probably your best path forward then, take the certificate program and see if you can get an internal transfer. Your internal knowledge of the data and systems will make you a valuable asset to a data science team compared to hiring a newbie who doesn't know the data or data science but you'd probably still be considered entry level.",NA
"629",628,63,"Interesting analysis. Expected the London bias to be well over 55%.",NA
"630",629,63,"Damn, I really need to get on learning Python. I have a great SQL role, which is great for experience, but to move up I definitely need to add Python on the mix",NA
"631",630,63,"I agree, but then i considered the likely variance in salaries too. I'd have thought that london accounts for 55% of postings, but plausibly closer to 70% of offered salaries",NA
"632",631,64,"Your cumulative plot is only fairly linear if you look from far away enough. 

When would you use ARIMA vs. standard regression? When the level at which you need to make predictions/analysis is a level at which there is an underlying repeating pattern that is impacting your data.

Here, for example, it looks like you have a pattern that repeats every X time intervals (hard to say exactly from your plot). Let's say it's every 4 hours. 

If you need to give predictions every (some multiple of 4 hours), then the pattern that you're seeing within those 4 hour intervals are irrelevant - and therefore you don't *need* to use time series because you don't need to model that repetitive pattern in your data. This is often the case with hourly data - if you need to make predictions recommendations at the monthly level, then the fact that there is a time series that repeats every day is irrelevant to you.

But if you're producing hourly updates, then you need to be able to deal with that pattern explicitly, and that is where ARIMA comes into play.",NA
"633",632,64,"Have you tried any other methods outside of ARIMA, such as exponential smoothing or Holt-Winters? I’d also look into a dynamic regression model if you want to add external regressors to your ARIMA model. I build my forecasts primarily in R, but I know facebook’s prophet library is a great tool to simplify some of these things in both R and Python.",NA
"634",633,64,"In reality, the answer is always (if these are your two choices). Whatever your linear model is, the typical distributions for your p-values, std. deviations and etc hold only when the residuals are white noise. So in most cases you at least want to make sure that no correlations remain by ARMA'ing the OLS residuals. This is usually done simultaneously through, e.g., ARIMAX, but can be also done in a two-step approach.",NA
"635",634,64,"Thanks for the response, I'm plotting my time series data per week, the spikes can be explained that in every first week of the month, this specific product is bough. My forecasting should be done weekly.

From what you see, do you think my ARIMA model is forecasting reasonably good (RMSE seems to be high), or is there any other way to approach this? Thanks again.",NA
"636",635,64,"( fyi) Facebook prophet is basically linear regression with non linear inputs: dummy variables for holidays etc and sine waves for weekly, monthly periodicity",NA
"637",636,64,"Disclaimer: Not a time series expert.

If your peaks are 4-5 weeks apart depending on how many weeks there are in a month, then there are probably methods you can find to capture that because it looks like that's what causing your issue - the model is picking up two medium-sized peaks instead of one large one.",NA
"638",637,64,"Thanks I think its a very valid point, if I sum up those two values then my forecasting would be better I think, do you have an idea on what are the other methods ?",NA
"639",638,64,"Unfortunately I don't.",NA
"640",639,65,"Double check for data leakage.  Is your response variable somehow ending up in the training data?

I'm speaking from personal experience here.  I once had a neural net with 99.7% accuracy on an NLP problem, and eventually figured out that it was because the label was being appended as the last character in the text.",NA
"641",640,65,"Is it possible that some unseen issue is making your test set *very* similar to the train? I once had this issue with a frame classifier for videos, some frames that were only milliseconds apart were in test and train and so were essentially identical. I wound up having to split the train and test into separate videos rather than just randomly split all frames.",NA
"642",641,65,"There's a few possibilities. Data leakage, imbalanced data, you are just looking at an easy dataset - I'd say explore all three. If you're looking at MNIST you might just be getting 99%.",NA
"643",642,65,"This. You likely have data leakage somewhere.",NA
"644",643,65,"Same thing happened to me once. It was a rogue character in the data that was throwing it off.",NA
"645",644,66,"Did you google it?

https://www.cdc.gov/flu/weekly/fluactivitysurv.htm",NA
"646",645,66,"Not having enough people with symptoms seems to be most peoples problem. You dont realize you encourage more testing and hope for more infections but this lowers mortality rate which the panic pushers are heavily relying on.

When people appear with symptoms... there will be more tests. You should encourage the Karen's of Facebook to stop wasting laboratories time hoping for a positive case to share with there friend. Its sick.

Between 1/18 - 3/6 CDC Labs performed 3,791 tests.

Between 2/6 - 3/10 US Public Health Labs performed 7,288 tests

Total is 11,079 (there are still approx 2,200 test results pending)

www.cdc.gov/coronavirus/2019-ncov/testing-in-us.html",NA
"647",646,66,"> https://www.cdc.gov/flu/weekly/fluactivitysurv.htm

Apparently not well enough. Thank you for sharing.",NA
"648",647,67,"I used data until March 9 to build a simple model based on exponential growth, using averages. It has been very close to the actual number released over the past two days. I sure hope it doesn't continue on this streak.",NA
"649",648,68,"Looks very nice. Unfortunately many schools have still exams or lectures during that time...",NA
"650",649,68,"Fuck my thesis is on one of those days!!!",NA
"651",650,69,"Neat! I am building a book recommender system.",NA
"652",651,70,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [Advanced Pandas (PandasVault) previously removed from r\/learnpython because of moderator difficulties but a great resource nonetheless. (r\/DataScience)](https://www.reddit.com/r/datascienceproject/comments/fhr9ua/advanced_pandas_pandasvault_previously_removed/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"653",652,71,"On point 3, you can definitely get a good job afterwards with a PHd in computer vision. Maybe working for some video supplier with masses of data or any company that relies on satellite data (think military, insurance for example).",NA
"654",653,72,"Agree with u/stuvet \- I think a lot of people are not understanding the value of dbplyr. And not to get on a soapbox, but I think this is at the core of the R vs. Python argument in general.

R (in this case exemplified by dbplyr) doesn't generally aim to get you the fastest route to an optimized solution. What R generally aims to do is get you the fastest route to a solution. 

Yes - relying on dbplyr's translation of your code into SQL queries can lead to highly inefficient queries. And if your process' biggest bottleneck will be how expensive your SQL queries are, then no, you shouldn't use dbplyr.

If you are only going to write one query, and you know exactly what you need out of it, then you should probably also just write it in SQL.

However, if the SQL part of your process is complex but light (e.g., you have to join 12 different tables, but most of them are small and all the joins are easy, and the part that is complicated are all the different calculations and transformations you need to make), and you are going to be testing a ton of different variations of things that you want to calculate, then dbplyr *greatly* speeds that process up, because editing dbplyr commands without breaking your ""query"" is a hell of a lot easier than doing so with SQL.

Having to edit SQL queries, re-run code to import them, manipulate, and then iterate is very poorly supported by most languages because SQL is external to those languages. You find yourself either going to your SQL IDE to run the queries and test them, or you need to run blind queries and troubleshoot the results from within your scripting language - which is shitty because you have very little of the SQL support you need to truly move fast with it. And if you're parametrizing queries from within R or Python, it's even worse because there is no easy way to translate the parametrization over to the pure SQL side.

In addition to that - for those saying ""just download all the data into memory and then use dplyr"", that is often not feasible, and in many cases it can be feasible but *highly* inefficient in a prototyping stage where you are still trying to figure out what is useful and how.

Once you're proven your model/process/whatever, you can then turn around and take your finalized dbplyr command and rewrite it as an optimized query. In fact, this is a workflow that I've used often in the past - prototype with dbplyr, and then optimize in pure SQL. 

TL;DR: Even if you know a lot of SQL, dbplyr can save you a lot of time early in projects.",NA
"655",654,72,"Think those commenting that the OP should 'just use SQL' have missed the point here. 

Writing code in R using dplyr (& Dbplyr) means a unified interface with e.g. flat files, Spark, Hadoop, SQLite, PostgresQL & Bigquery, translating between SQL dialects as needed, & allows you to provide your own SQL query if you like. It makes it simple to develop & test code locally in-memory then use exactly the same code to run efficient queries at scale on e.g. a Hadoop cluster.

No idea on equivalent Python packages, but just suggesting that 'just learn SQL' may not be the most useful response.",NA
"656",655,72,"SQLAlchemy is Python’s attempt at this.

Look up the Intro to Databases with Python on DataCamp for a decent starting point",NA
"657",656,72,"pandas plus any combination of pyodbc/turbodbc (for general ODBC connections) or jaydebeapi (for general JDBC connections) or DB specific libraries like psycopg2, mssqlserver, ibm_db, etc will be equivalent to dbplyr.  If you want to avoid SQL or need a way to switch DB  backends without worrying about updating DB specific SQL, then Python users can use an ORM library like sqlalchemy.  Partially related, but as others have stated, data ppl should be familiar with SQL, but also be aware of what ORM is and when/why you would use ORM.",NA
"658",657,72,"dfply?",NA
"659",658,72,"I am also in the ""ORMs are bad"" camp, but plenty of people smarter than me would point you at SQLAlchemy as the best python equivalent.",NA
"660",659,72,"It's kind of a non-answer (or at least probably not very useful), but in most cases I've seen, the solution in Python has been to build a custom API to interact with the DB layer. Often these end up being relatively thin wrappers that rely on SQLAlchemy or some other ORM and then further build on the ORM's grammar to meet your needs.

It's also important to recognize that dbplyr doesn't just automagically interact with the database. It relies on DBIs specific to your given database flavor.

In this case, the R model (as I understand it) is essentially:

> dplyr -> dbplyr -> DBI -> DB

In python, it may look something like this:

> Whatever you want to do -> Custom API -> SQLAlchemy -> DB adapter -> DB

There are really 2 main challenges when searching for R equivalents in python. Firstly, the tidyverse is pretty unique among analytical languages, and it has a very distinct and relatively strict grammar because it's almost entirely controlled by Hadley. This type of stability allows people to create tools for specific implementations or a relatively small family of verbs. These verbs give great coverage for common data tasks, but they can't do everything. dbplyr does a good job implementing the dplyr grammar as SQL queries, but I would argue it can only efficiently do so because of the relatively small number of verbs used by dplyr. Even with that narrow grammar, if you look at the raw SQL queries generated by dbplyr, they're often a trainwreck.

Secondly, a whole lot of implementations in R are informed by the language's limitations, such as needing data to be memory-resident. dbplyr's implementation is heavily informed by this. Python in general doesn't have these limitations, and memory residence usually only becomes a problem if you're using pandas, so you can often use less efficient methods (in terms of DB access) to achieve the same results. If you do want to use pandas for whatever reason, there are a bunch of tools to deal with memory limitation challenges, most of which are implemented in pytables, but the philosophy is functionally different than that used for dbplyr.",NA
"661",660,72,"I think you can use pandas read_sql function combined with sqlalchemy connection for the db type you are querying from. You can either pass it raw sql or use the orm to build models tied to data sources. 

I wrote a library to combine these called [sqlsorcery](https://sqlsorcery.readthedocs.io/en/latest/) but its still in early development.",NA
"662",661,72,"Sounds like you’re just looking for an ORM. Sqlalchemy is very popular",NA
"663",662,72,"In my experience the queries that dplyr makes aren’t optimized enough; it’s better to be make the SQL for these queries. 

It will make SQL that looks like:

Select something from (
  Select something2 as something from (
     Select something_else from temp1) temp2) temp3

For SQLite this might be OK, but for other db backends might not be so efficient. 

To actually answer your question for Python, though, an ORM might be the way to go, like SQLalchemy. 

An ORM is a way to use a table in a database like it’s a class in Python. So, say you have a customers table in your db, in Python you would relate that to a class Customers that had the same fields. Say there was a customer_id field in your table, you could do cust = Customers() and then do cust.customer_id to get the value. 

You can also run queries, too. So you could do something like california_customers = cust.query.filter(cust.state == ‘CA’). 

The main reason people use an ORM though is since it’s more secure than running straight SQL in an application and also might make more logical sense to work with an object than to have SQL queries everywhere. 

I know you just want to run pandas code against a DB but I don’t think that exists yet, I think the easiest thing is, instead of needing to learn a new query syntax with an ORM is just use pandas’ read_sql method to directly read your data to a dataframe. Even if it seems cumbersome, generally writing in SQL is the quickest path for a solution.  I even see people who don’t like the ORMs because it gets really hard once you have a semi-difficult query.

Edit: if you’re looking for laziness try Dask. Not sure how it works with DBs, but it works well with S3 if you can get your data there (or something similar).",NA
"664",663,72,"Isn't there a library that lets you embed R code in Python? I think I've seen that.",NA
"665",664,72,"Sql",NA
"666",665,72,"Siuba is a new port of dplyr and has this functionality.",NA
"667",666,72,"It looks like either blaze or ibis will do what you want (translate a series of python methods into a SQL statement and execute it lazily), ibis uses apache arrow which is nice.",NA
"668",667,72,"I don't know why you think ibis has ""gone stale some years ago""? The last commit was yesterday, the latest version was released two weeks ago.",NA
"669",668,72,"Dataset is built on SQLAlchemy but is designed 'for lazy people'. Id say that's probably the closest thing to what you're describing.",NA
"670",669,72,"Sqlalchemy or pysql might be what you are looking for.",NA
"671",670,72,"Several others have mentioned sqlalchemy (SA) in this post, and there have been comments about using ORMs. I just wanted to chime in on this a bit. I use SA a lot, and it's a handy tool. This would be my recommendation too. I suggest you start (as the docs state) with the SQL expression language portion of SA, and look into the ORM later if you need/want it. Unfortunately, SA doesn't share syntax like dplyr and dbplyr, so you'll have to learn how to use it. The docs are challenging to learn from, so you may need other resources. Good luck!",NA
"672",671,72,"Dplyr is pandas for R",NA
"673",672,72,"Pandas, SQLalchemy, SQL. It sounds nice that dplyr is would take care of SQL for you but you really need to know SQL as a data scientist.   
  
There's also Pyspark but it's usage is a bit more specific.",NA
"674",673,72,"I know it's not the specific answer, but I've found that dbplyr is MUCH slower than just running straight SQL. Are you sure you need that?",NA
"675",674,72,"Python philosophy is to provide you the building blocks to do everything yourself in a simple way in a few lines of code.

What commonly happens is that there is a tool X that does everything you need it to do except this one little but critical thing. So you are forced to abandon X and do it yourself anyway.

Instead of providing you plug&play they make it really easy to create the tools for yourself just the way you want them.

Python doesn't attempt to be similar to other languages. It does its own thing. If you want to use something else then just use something else, nobody is forcing you to use python.",NA
"676",675,72,"I agree, OP is looking for a nicer way to interact with their DB and I don’t know many ways in python to get that analytical workflow without using SQL. Plain SQL+Python has been my primary workflow for 6 years now. Ibis looks promising but appears to have limited support right now, while dbplyr seems like a very mature library. 

Totally unrelated to the core of OP’s question, there won’t ever be a Python direct equivalent API because of how R parses expressions as lazily evaluated S expressions. The Python API will always be a little more awkward and not let you perform transforms on undeclared variables like dplyr, R formulas, and a lot of R functions use. Even Julia isn’t quite as ergonomic in that regard right now.",NA
"677",676,72,"Thank you for clarifying my question. I was having trouble communicating what I was looking for, with some commenters even assuming I don't know SQL. You summarised it perfectly.",NA
"678",677,72,"I’ve never found the “test locally and use the same code in production” claims even remotely relevant.  Learn how to do things in production and you will be an actual contributing member of the team.",NA
"679",678,72,"That and I already know SQL.

I need a way to transform data in pandas thats just not so fucking ugly.",NA
"680",679,72,"I am not very familiar with the R tool, but I believe SQL alchemy does something similar to Dbplyr. But yeah, he should learn SQL, lol.",NA
"681",680,72,"This is the correct answer.",NA
"682",681,72,"An alternative to SQLAlchemy is [Peewee](http://docs.peewee-orm.com/en/latest/). I’ve never used it for analytical needs but it’s easy to write apps with, and it supports windows functions. It’s especially nice with SQLite, but generally is good for other backends.",NA
"683",682,72,"What's an ORM? Does Sqlalchemy support abstraction of SQL queries using dplyr-like verbs or Ibis expressions and is it lazy?   


I'm new to this subreddit and to Python, but I tried to stick by the sub's rules. I was using R for exploratory data analysis of data stored in a database and found dbplyr really useful, found out about it just 3 weeks ago. However, I was asked to switch to Python, which I have almost zero experience with, hence my question.",NA
"684",683,72,"I get that.  But does it support *dbplyr* (not dplyr) functionalities?",NA
"685",684,72,"Pandas is Dplyr for Python.",NA
"686",685,72,"Pandas remind me of base R the most! Perhaps he’s looking for something that will allow him to group the data by variables and create/mutate variables easier",NA
"687",686,72,"Thank you. I know SQL, I just want to spend more time doing actual analysis and less time writing long and error-prone SQL queries. As the [Ibis docs](https://docs.ibis-project.org/index.html) say:  


> Its goal is to simplify analytical workflows and make you more productive.   
(...)  
 We have a handful of specific priority focus areas:  
>  
>\- Enable data analysts to translate local, single-node data idioms to scalable computation representations (e.g. SQL or Spark)  
>  
>\- Integration with pandas and other Python data ecosystem components  
>  
>\- Provide high level analytics APIs and workflow tools to enhance productivity and streamline common or tedious tasks.  
>  
>\- Integration with community standard data formats (e.g. Parquet and Avro)  
>  
>\- Abstract away database-specific SQL differences",NA
"688",687,72,"Actually, my employer wants me to do this task using Python. I would gladly do it in R.",NA
"689",688,72,"I'm intrigued about the property of R that makes it this flexible - can you elaborate further? I understand that R and Julia are naturally more 'functional' languages. I hear 'lisp-based' often in regards to Julia. I've also had some experience of maybe the opposite extreme in trying to use Go for some toy numerical computing, but trying to understand what makes a language this flexible (or inflexible!).",NA
"690",689,72,"Of course you need skills that are relevant for production, and of course we need to know SQL, Spark etc. 

It takes relatively little time to optimise an SQL query after you know the analysis is sound. It may not be valuable in your environment, but that doesn't mean such tools don't have a valuable place in some/many environments. Seems short-sighted to write them off completely.

Also makes me wonder why we don't do all our work in Assembly/Bytecode.",NA
"691",690,72,"I know it won't fix everything, but the .query method in pandas can help a little bit",NA
"692",691,72,"I know SQL, thank you.",NA
"693",692,72,"An ORM is an objection relational mapping. It maps databases objects to, in this case, python objects.

For instance, after creating a connection, you could create a `Table` object in python that maps to a table in your database.  Much like `tbl(conn, 'db.table_name')` does in dplyr

You probably want to look at `Query` [here](https://docs.sqlalchemy.org/en/13/orm/query.html#the-query-object) - this will create a query object that generates sql when you execute it (similar to dblyr). This nice part about this is the SQL now abstracted as python... so if you wanted to switch to a different SQL dialect, like from postgres to mariaDB or something, all you would need to do is create a new engine.  

SA is a bit more complex than dplyr, its more often used to programmatically manage databases rather than quickly preform EDA. Be prepared to invest some time figuring it out. Once you have it its a pretty powerful tool.  

Also, ignore everyone telling you to use pandas or ""learn SQL"".",NA
"694",693,72,"I have no idea what Ibis is but you really need to learn SQL as a data scientist. You shouldn't be constraining your toolset for a workaround. Pandas + SQLalchemy is pretty standard.",NA
"695",694,72,"I think [dfply](https://github.com/kieferk/dfply) is Dplyr for Python. AFAIK, Pandas doesn't support chaining nor is it centered around  a

>consistent set of verbs that help you solve the most common data manipulation challenges:  
>  
>[mutate()](https://dplyr.tidyverse.org/reference/mutate.html)  
 adds new variables that are functions of existing variables  
>  
>[select()](https://dplyr.tidyverse.org/reference/select.html)  
 picks variables based on their names.  
>  
>[filter()](https://dplyr.tidyverse.org/reference/filter.html)  
 picks cases based on their values.  
>  
>[summarise()](https://dplyr.tidyverse.org/reference/summarise.html)  
 reduces multiple values down to a single summary.  
>  
>[arrange()](https://dplyr.tidyverse.org/reference/arrange.html)  
 changes the ordering of the rows.",NA
"696",695,72,"You mean Pandas?",NA
"697",696,72,"Sort of. I've already found dplyr alternatives for Python, such as dfply, dplython and pandas-ply, but those are for working with in-memory data-frames. I'm looking for something that does what you say, but with data stored in remote databases, and translates those actions to SQL.",NA
"698",697,72,"Instead of long queries, do a pull of what you need and manipulate locally in Pandas. I think trying to find a solution to this is going to be a lot more work for you than just pulling data. Keep in mind that depending on the complexity of your transforms, your server may not even be able to handle the operations. And if your server does have the hardware for it then check out Spark.",NA
"699",698,72,"As one of the creators of blaze, I understand what you’re actually asking for (unlike everyone in this thread telling you to use sqlalchemy 😉) 
I think ibis is probably what you actually want.  Alternatively, for “remote dataframes”, look at Dask. But for compile-table-express-to-SQL, I’m not aware of other mature-ish things in Python at this moment.",NA
"700",699,72,"What makes R so flexible is the lazy evaluation of your code which allows R to capture the code itself as an object and can directly interact with it at runtime.

When you run code in R, it goes line by line and creates a list of expressions that can be evaluated. Expressions in R are actually [S-Expressions](https://en.wikipedia.org/wiki/S-expression), that is they're a list made up of symbols, constants, function calls, and more S-Expressions (in reality it's a little bit more complex than this). When R parses code into s-expressions, it's rewriting your code in a Lisp. By default, after an expression is parsed it is immediately evaluated and the return value is inserted into the parent expression. As an example you may have the code `a <- max(x, y, z) + sin(1:10)`, which would parse to an expression like `(assign, a, (plus, (max, x, y, z), (sin, 1:10)))`, then be evaluated to `(assign, a, (plus, XXX, YYY))` and so forth until it has computed all the values from deepest parentheses to shallowest, and finally assigning 'a' to some value.

There is a special process that allows you to ""quote"" an expression, and instead of being evaluated it will be returned as the actual expression object. You can interact with quoted expressions just like it was any other list in R, allowing you to introspect it and see what it contains, or completely rewrite the expression any way you want. You can also construct your own expression objects. That's how dplyr is able to cleverly set column names to match the expression that created them, and a lot of R's base functions take advantage of this. 

When you evaluate a quoted expression it's as if you paste the expression into the scope you're evaluating it in and then run that code. That means you could write an expression which references variables that don't exist yet, but as long as you evaluate it in an environment where they do exist everything is fine. You can also bind an expression to a scope/environment that allows the expression to be computed later using the variables that are in that scope, these are called promises. Formulas in R are promises and using the tilde operator automatically creates them.

So now finally, the thing that makes R so flexible: normally in most programming languages functions access the values of the arguments passed to them, but in R arguments to functions are implicitly converted into promises and those are passed to the function. That means you have access to the code for all of your function arguments and if you evaluate them all the data they connect to is still maintained via the promise's environment. 

When you use dplyr and you write `subset(sample_df, b == c)`, the subset function is able to rewrite the code to `sample_df[sample_df$b == sample_df$c, ]`, and in most other programming languages that's a syntax error. It enables some pretty cool stuff to happen, like variables inside formulas that are built from transforms like `y ~ log(x) + 1`. 

Julia is similar to R in that it parses your code into a Lisp too. Julia is more complicated because everything in R is happening at runtime, but in Julia there is parse time and run time which can become confusing and have performance tradeoffs. But like R, Julia converts your code into expressions which can be quoted, and quoted expressions can be modified, inspected, rewritten, or dynamically created. Julia does not have promises like R does. Additionally to R, Julia has macros which are functions that rewrite parsed expressions while parsing is ongoing. This allows you to write powerful rules of abstraction but with some pretty heavy restrictions and caveats (e.g. everything must be known at parse time). TBH I don't think I know enough to articulate why expressions+macros in Julia hasn't led to similar amounts of flexibility of R, it could just be the syntactic sugar that R provides or the lack or promises. In some ways Julia's macros have opened a lot of interesting opportunities like the model DSL in [Soss](https://github.com/cscherrer/Soss.jl), but they've also as yet [failed to reproduce the conciseness and flexibility of the formula API in R](https://github.com/JuliaStats/StatsModels.jl/issues/75) (I don't mean to call out StatsModels as not good, I'm a big fan of that project, it just turns out to be a really hard problem to solve and hasn't had nearly the time or love that R has had put into it yet).

Anyway, that was a huge wall of text, but I had fun writing it and hopefully you learn something you find interesting there. If you want more [Hadley Wickam's Advanced R has some great info](http://adv-r.had.co.nz/Computing-on-the-language.html#nse), and the [Julia metaprogramming page is good](https://docs.julialang.org/en/v1/manual/metaprogramming/).",NA
"701",700,72,"It's not the same has basically having a query language syntax. Dplyr is practically SQL for R and its amazing.",NA
"702",701,72,"I didn’t mean to be flip. Python is very slow and the best practice is to typically do everything in SQL you can reasonably do and then use Python for the portion you have to.

You were seeming to ask for a python package that simplified writing SQL queries. While those do exist I am not sure there is one that specifically does what you want. There is SQL Alchemy, but that is more a database management tool. In my mind and experience, the best solution is to pass SQL expressions and queries to the server. It seems like you already know how to do that, so you are set!",NA
"703",702,72,"But he is right in one thing, if you abstract yourself from the code, focusing yourself only in the theory of the problem maybe you find a faster solution because not need to deal with the problems of the code or how to write this idea that I have in mind.

But I haven't work enough with R so not sure how much easier it is, for me is much easier to draw the ideas and the general solution in a plastic whiteboard on my room, and from there write it in python by modules.",NA
"704",703,72,"`pandas` does definitely support chaining via method chaining. When no method is defined for the task you want to accomplish you can still use `pipe()` and chain any lambda or defined function that takes a dataframe as an argument.

And while it is not as consistent about the selection of _verbs_ as `dplyr`, basically:

+ `mutate()` -> `.assign()`
+ `select()` -> `[[]]` (a method would be nice I agree)
+ `filter()` -> `.loc[]`, or `.query()` 
+ `summarise()` -> `.agg()`
+ `arrange()` -> `.sort_values()`

And many more. I actually am in love with R's `ggplot` so i use [`plotnine`](https://github.com/has2k1/plotnine) which emulates it almost perfectly and usually I end up chaining a call to it using `.pipe()` after several method chains.


EDIT: Either way, I see the advantage of `dbplyr` and it would be awesome if something akin to that was implemented with `pandas` in Python, but there is not really anything exactly like that in Python as far as I know.",NA
"705",704,72,"SQLAlchemy, Psycopg, Spark, Koalas, Pandas (has SQL to dataframe input option). Tons of options to import remote db data into a python environment",NA
"706",705,72,"I have very limited knowledge of python( recently started learning it), but you might want to look into SQLite",NA
"707",706,72,"Well, my original plan was to let the server/DBMS do all the computations and just pull the data that I really needed (which I don't know beforehand, since it involves doing some aggregation operations and other exploratory stuff) , due to the the size of the tables. But I will give it a go at just pulling the tables to pandas dataframes and working with those.",NA
"708",707,72,"This is a good start to a blog post. If you write it up then every time someone asks why something in R is easier than in Python, we can refer to it.",NA
"709",708,72,"This is fascinating. I've read it a few times already. At first I thought S-expressions were to do with the S language, so I've learnt at least one thing here! I find it interesting how fundamentals of a language can have such a knock-on effect on it's capabilities and ease of use in ways that might be hard to see if you're designing the language. It blew my mind when I realised I couldn't divide a time period by a number in Go, and that highlighted to me that different languages have different strengths. Admittedly I still haven't quite got the hang of quoting and unquoting in R (which seems to be useful to create functions that use ggplot), and I guess that's because I don't understand yet what is going on under the hood - though those two links are helping!",NA
"710",709,72,"The data is already in a database, likely something more robust than SQLite and its single file database.  (Don't get me wrong, SQLite is great at its job of an being an efficient light-weight easy to use relational DB).",NA
"711",710,72,"I may do that. There are some great blog posts about how this works in R, but I haven’t seen one comparing it to Julia or really outlining what flexibility it gives over Python.",NA
"712",711,73,"R Markdown is amazing and is another benefit - I haven't found anything that competes with it.

Being able to quickly put together html/pdf documentation in a professional and reproducible format, with embedded R code, LaTeX, etc. has helped me out so much at work.",NA
"713",712,73,"R is made for data analysis and statical modelling. I haven't found anything that beats the tidyverse in python",NA
"714",713,73,"R is a great language. That said, when I talk to folks in industry, they usually cite licensing issues as a reason for not using it. I think R is issued on GPL, which means that anything you write in it can't become proprietary down the line. This is important to businesses that might want to protect their methods and IP. Here's a blog with a little more info: [R and GPL](https://www.r-bloggers.com/how-gpl-makes-me-leave-r-for-python/)",NA
"715",714,73,"hmm fair but can i write production software with it",NA
"716",715,73,"You are doing a disfavor for people that are starting with data science with such posts.  


I'd agree with first part about data wrangling - other points are just silly. Not to mention you didn't compare R with anything else in specific.  


Visualization - Python has just fine libraries for that.  


Specifity - there are other languages than R that do that, not only Python.  


""Data science is all about the prediction."" - that's not even true - you mentioned visualization before, which is sometimes the end goal of data science tasks.  


Availability - again, Python and Julia also have that, with Python having arguably bigger community.",NA
"717",716,73,"I am currently doing so, and I am needing to use a Java wrapper to improve performance for concurrent users. So, yes and no. Yes you can write production software with R, and no you can't write production software with only R.",NA
"718",717,74,"If you want to build custom neural nets from scratch, then you should use Tensorflow or PyTorch.",NA
"719",718,75,"What was your answer?",NA
"720",719,75,"Adding to your regularization approach. You could have also talked about Ockham's razor and balance between complexity and model performance. Could also give an overview of Measures like Adjusted R^2 and the financial cost of complex models. How much does a percentual point increase in performance costs and whether its worth the trouble.",NA
"721",720,75,"I don't use this approach but there a few who do : first split your data into train and test. Keep the test data aside and then split your train data into train and validate. Here you can talk about regularization and cross validation. Use the validate to test data. And then finally when you get a good model use the test data which is untouched to get final score/accuracy.",NA
"722",721,75,"I feel that this is sort of a trick question where the correct answer is ""you dont"". You overfit and then you regularize. After all, overfitting is good: you have found a model that fits the data. It just fits it a bit too well, so next step is to fix this. But in general, overfitting is much better than underfitting and at the point where your model is overfitting you're pretty much at the finish line of modeling process.

EDIT: most likely next question would be ""how would you regularize"" where they would expect you to talk about L1 & L2 regularization, dropout,  outlier elimination, variable selection and cross validation depending on the modeling case.",NA
"723",722,75,"https://elitedatascience.com/overfitting-in-machine-learning  
https://medium.com/datadriveninvestor/how-to-handle-overfitting-and-underfitting-470a1f7389fe  
https://www.kdnuggets.com/2015/01/clever-methods-overfitting-avoid.html  
https://www.researchgate.net/post/How_can_we_solve_an_overfitting_problem",NA
"724",723,75,"I feel like the most correct answer is model specific. Different technique for knn than log regression for example.",NA
"725",724,75,"I talked about regularization, analyzing how many predictors should be used based on number of observations, and using cross validation testing.",NA
"726",725,75,"I actually haven’t studied Ockham’s razor much, I’m still  learning a lot.  These are great suggestions! And when you talk about percent increase vs performance cost, is that the same as improved model accuracy vs computational cost?",NA
"727",726,75,"Regularization is something I would look for. 

number of predictors vs observations is definitely not what I would look for.

Cross validation is a great talking point. I would ask you to expand on it. This is what I would talk about in an interview. I would talk about field generation, how I would generate a leave out set, how much I would leave out, different methods I would compare with cross validation, and different types of cross validation possible (including my favorite).",NA
"728",727,75,"Good advice! I’ll study these so I can improve my answer.  Also, can you expand on what you mean by field generation?",NA
"729",728,75,"Lets say I have financial data. It may make sense to look at median transaction amount. It may also make sense to do maximum transaction amount. It may make sense to look at each percentile as well. That being said, this can easily lead to over fitting. When generating the fields used in the model, it is important that you have a general idea of what is best without trying everything under the sun.",NA
"730",729,75,"Oh right.  So I’m that example, the overfitting would be caused by multicollinearity issues from the generation of correlated fields?",NA
"731",730,75,"not necessarily. random forests and tree based models can easily overfit but are not subject to multicollinearity issues.",NA
"732",731,76,"Experiment.

I'm willing to wager everyone here has done it both ways on accident at some point or another.",NA
"733",732,78,"Doesn't sound optimal, but when you first start out, it's not the worst thing in the world to gain experience and hone your chops with real-world data in a low-pressure setting.  No one really expects a fresh grad to singlehandedly contribute millions of dollars to the bottom line.  In general, big corporate is not super amenable to change, especially on the technical side.  Learn as much as you can, hit the one year mark and re-evaluate.  

Also realize that you are currently not in a situation to impose change from the top.  I agree with one of the other comments: start with a tightly scoped project that addresses a specific pain point and requires little outside assistance.  Get a few of those implemented and you'll have more credibility to shoot for the high impact ones.  You'll also be learning valuable project management and relationship building skills which are just as, maybe even more important, than technical knowledge in real-world data science.",NA
"734",733,78,"Most hiring managers know that you ""grade on a curve"" for people that have been working in a less mature environment. That is, getting *any* model near production when it's the first data science model is going to be a much bigger achievement than getting the 20th data science model into production. 

As far as your resume, the difference between ""Identified $5+ MM in cost savings using a scoring algorithm"" and ""Generated $5+MM in cost savings using a scoring algorithm"" is relatively small. Data Scientists understand that you can take a horse to water, but you can't make it drink. 

And your mentor is right - early in this journey, it's tough to get the first project to go through. If you're having trouble getting buy-in, try to work in pilots: do a small version of the project on a subset of the business with limited IT support to prove out that it works.

In general, try to focus on projects that require as little outside support as possible and that are as ""partitionable"" as possible, so you can run your project for one item/region/segment/whatever if needed in order to prove it works and get ammo to deploy further.",NA
"735",734,78,"tl;dr You are in a career-killing position.  Clean up your resume and move on when you can.

The things you are describing are red flags for poor leadership.  It sounds like your executive leadership lacks vision.  I'm assuming that this ""large corporation"" is an established industry contender?  They wouldn't be large if they weren't.  But they are still running on an entrenched 20th century strategy.  I'd be willing to bet that this is the reason they jumped on the data science bandwagon so late.  They're giving you the training to do your job but not the support to accomplish anything.

And your manager has no cajones.  It's their job to communicate the company's needs to you and your team, and to communicate your needs to executives.  No one here has the patience overcome the urge to not kill an idea too soon, but rather to nurture it.  I wouldn't stay in that position too long if I were you.",NA
"736",735,78,"Your 5 million a month algorithm, did you review it to see if it would actually have saved that much money? If not, you should and show it to the leader. Get feedback of why they are against it. Work with them to build the next revision of it so they are invested in getting results  and trust that there is cool stuff being done by you.

If you do none of this, it is a ""he said, they said, situation"" and it makes sense that they dont trust you... you havent given them reason to.

For me, Data science is 10% data science, 40% data cleaning, 20% getting people to use your results, and 30% building partnerships using really basic analytics and alcohol.

A big part of the use case and partnership should be supported by your manager. That being said, if they are failing, you should pick up the slack.",NA
"737",736,78,"To be fair, I was an analyst for a few years prior! But yeah still my first DS role so not entirely different. 

I've been in line with that same thinking of learning as much as possible and then seeking new opportunities when the well runs dry. 

Like I mentioned above I really have learned a ton and being exposed to actual data and projects has been beneficial. Totally agree with your thoughts and I appreciate your feedback.",NA
"738",737,78,"You're spot on about this curve of trying to implement one model, and it really does feel that way.

But I see what you mean. It's comforting to hear about the slight difference in the resume as other data scientists have experienced this. But yeah, the ""buy-in"" is what hes mentioned along with the other points you've said and I know this isn't always the case but I do feel lucky I was/am able to learn a ton. 

Awesome idea about the project though! It's been a little challenging as random projects just pop up which come from the top. However I like that approach and I agree.

  
Thank you!",NA
"739",738,78,"I have mixed feelings about this advice.

On on hand, I agree and 3 months ago I was in a similar position. But I also am timing my resignation because I'm moving in with my gf meaning I needed to stick around a while longer.

It turns out that eventually a lot of the stuff that was bugging us (i.e. supporting marketing with BI products) eventually went a little quiet so we could work on some things. I'm actually getting my first model into production (just LOWESS smoothing, but hey, it's something) .

Thing is, pack up to go... where exactly? 95% of companies are exactly in this rut, even smaller companies tech companies like my own suffer from this.

And the truth is that it would be convenient for us if HiPPOs would just lie down, but they never will. In my experience as well as a n00b, that skepticism is warranted. I've made a lot of mistakes because I was too brazen that something would just work.

But really, you just have to make that first success. Once you get your first model in then people get it and you build trust. You need to build this at any company. (But also having a stifling culture isnt to be underestimated.)

My advice to /u/TheGasBoi might well be to get ready to jump ship, but to also show some patience because some roles do get better and the grass really isnt always greener on the other side. I'm slightly sad that I'm handing in my resignation at the end of the month but at least I can say to my Boss with a straight face that things were just getting good.

But hey, this is my first role, so who knows? I just want to throw in that not all cases that seem lost really are so. Sometimes it takes a lot of patience, strategy, politics people skills and letting small wins and investments grow over time.",NA
"740",739,78,"There’s a spectrum for data usage within this company and you rarely can come in and start building ML models. Kudos to the manager for recognizing that and helping to get what the business needs done first and then moving to ML. 

It does sound like there’s a mismatch between what the company wants and what you want to do so it may not be the right role for you.",NA
"741",740,78,"To be fair, my manager has been vocal, as he used to be an executive in charge of analytics for a prior company  (retiree now, taking some work for a little bit longer).

However he also won't be here for much longer but he really is trying to give me everything he knows. But yeah hard to disagree with your point about the training but no the support (other than my manager to be fair). 

Figured it would be good to learn as much as I could and check other opportunities, but completely understand your points. Appreciate the feedback.",NA
"742",741,78,"The other thing you can do is try to turn the ""random projects"" and turn them into legitimate ones. That is, if one of those requests that comes from the top has room to be better, bigger, more valuable, try to build a business case for it. The fact that it came from the top means at least someone is already interested in, so you have some leg work that was already done for you.",NA
"743",742,79,"These look quite advanced to me so yeah, I guess you would need some JavaScript for this. If you want just to try this out you could give dash a try: https://dash-gallery.plotly.host/Portal/",NA
"744",743,79,"Bleeding edge stuff is all JS. If you’re looking for better ways to display your info day to day, either R or Python will work for what you need.",NA
"745",744,79,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [How to make data viz websites? Links provided (r\/DataScience)](https://www.reddit.com/r/datascienceproject/comments/fhr9up/how_to_make_data_viz_websites_links_provided/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"746",745,79,"these are great. thank you! so is this all created using python? 
**Yea it would be years until I get that good but for now I guess i'm just deciding if I need to know Javascript and/or python. Also idk if there's a way to use them together? I'm really new to this",NA
"747",746,79,"Regarding dash, yeah it's python. It uses html calls for layout and plotly for graphs, tables and what not. It's quite easy to get the hang of so you could try it out.

As for js+python, sure. Python can provide all the calculations if that's easier for you and then you can plot all the plots with JavaScript. But if you're doing just infographics JavaScript alone is enough. 

At the end, what I would suggest is find a tutorial try to replicate it then do your project based on that. If you have any programming knowledge you will be fine. Just take your time and start small",NA
"748",747,79,"You're right. One project at a time!",NA
"749",748,80,"I definitely recommend Applied Data Science with Python from the University of Michigan. Unfortunately, I got swamped with work and couldn't continue past the second course, but the first two are great.",NA
"750",749,80,"Thanks for the link.

I see some courses from a school called ""John Hopkins"", is that like the dollar store version of Johns Hopkins?",NA
"751",750,80,"Thanks for posting this!",NA
"752",751,80,"Between ""Applied Data Science using python"" (From Uni. Of Michigan) and ""IBM Data Science Specialization"", which one would you guys suggest? 
And also why if I may ask?",NA
"753",752,80,"US only",NA
"754",753,80,"Thanks, I'm not from the US though
And can I also get",NA
"755",754,80,"How much is it after the first month?",NA
"756",755,80,"Does it charge you after the 7-day free trial or charges you right away?",NA
"757",756,80,"Nope, created by John Hopkins University and hosted on Coursera",NA
"758",757,80,"[I'm not sure this one is on Coursera rather than yourself.](https://www.coursera.org/jhu)",NA
"759",758,80,"This one is only for the US. If you are interested in IBM Data science and machine learning programs, you can get 30 days of free access following [this process](https://pythoncoursesonline.com/IBMDataScienceAI)",NA
"760",759,80,"I think it is $39/month",NA
"761",760,80,"Right away",NA
"762",761,80,"He was making a joke. Its JohnS Hopkins not John Hopkins.

If you've ever been there they like to drive home that distinction.",NA
"763",762,80,"So maybe I can do the 7 day trial and then sign up?",NA
"764",763,80,"Yeah, that's exactly it.  

And this is not a criticism of the OP, but rather the linked page.",NA
"765",764,82,"Look if your baby came out of the womb not knowing Python, R, real analysis, and a MITx micromasters certificate I question your parenting skills. If they don't have a double PhD in CS and statistics by age 5, I'm calling CPS.",NA
"766",765,82,"That book exists? Lol",NA
"767",766,82,"New show idea: Are you smarter than a 5 month old?",NA
"768",767,82,"My husband’s coworkers gave us one called “ML for babies and toddlers.”",NA
"769",768,82,"A month ago I added all of his books to my baby registry. 

I am not pregnant. Nor do I have a child.",NA
"770",769,82,"Yep this is the official birth of skynet. Thanks OP.",NA
"771",770,82,"You sir, are a good parent. Just make sure to give your baby ethics and morals along with that early trained intelligence. The more you make him/ her interested in learning and by giving him/ her more to learn you’ll create a prodigy. Also, try to make it his/ her choice by merely guiding your baby through the knowledge out there. By making it a chore you’ll most likely hinder the development and creativity.

I’m not going to tell you how to raise your baby, it’s your choice. I just wanted to give you advice from what I’ve seen and read. Good luck!",NA
"772",771,82,"Next edition: neural networks for fetuses",NA
"773",772,82,"Probably no longer going to be the hot method when he’s older though. Would be like your parents buying you a “chaos theory for dummies” book when you were a baby. Or “genetic algorithms”",NA
"774",773,82,"That’s cute as fuck. I know growing up I loved all the animal books my dad gave me (he’s a biologist) 

His passion and early exposure is the reason why I went to university. 

Keep up the good work data dad!",NA
"775",774,82,"Fun fact:  I learned HTML at an early age from child's picture book called the ABCs of HTML.",NA
"776",775,82,"I’m sure I could learn a lot from reading that too! Can I borrow that next?",NA
"777",776,82,"OMG, millennials are killing Good Night Moon.",NA
"778",777,82,"That’s tight lol",NA
"779",778,82,"Baby be checking your work like ”ahem 🧐, mother, I've been looking at your model’s performance. We need to talk.”",NA
"780",779,82,"Is this one of those photoshop baby book cover memes?",NA
"781",780,82,"Megamind: the Early Years",NA
"782",781,82,"Upvoted for cuteness.",NA
"783",782,82,"Do you have some motivating pictures labeled ""never too late""?",NA
"784",783,82,"My 5 and 4 year old both learned about their ""special parts"" from my medical textbooks.",NA
"785",784,82,"Definitely getting into the Info Systems program at CSUMB, if she is already learning about neural networks, better study up on coding languages just to be safe though.",NA
"786",785,82,"looks like alien language. the aliens are programming her for colonization!",NA
"787",786,82,"While in reality what gets baby the job is if he can swipe through the ipad menu",NA
"788",787,82,"Funny you mention the micromaster. Worth getting?",NA
"789",788,82,"I got a headache imagining that world",NA
"790",789,82,"Yep [Neural Network for Babies](https://www.amazon.com/Neural-Networks-Babies-Baby-University/dp/1492671207/ref=nodl_)",NA
"791",790,82,"It's always good to be prepared!",NA
"792",791,82,"I don't see any downside here.",NA
"793",792,82,"tryna meet some smarter babies, eh?",NA
"794",793,82,"*teaches baby machine learning*

*grows up to make snapchat filters*",NA
"795",794,82,"""Learn to develop a neural network before you even have developed your own neural network!""",NA
"796",795,82,"I still believe genetic algorithms haven't truly seen their day yet.",NA
"797",796,82,"Too bad its not the Registers of Assembly",NA
"798",797,82,"Good Morning Data",NA
"799",798,82,"It's legit lol. Google the title",NA
"800",799,82,"Oh wow. I hope they follow this up with Python and TensorFlow for babies.",NA
"801",800,82,"This guy made while series of these books. I am a physicist by training, so someone gave me the version about quantum physics. I really like reading it to my 11 month old, and he likes the colours and shapes.",NA
"802",801,82,"holy shit these books are *AWESOME*",NA
"803",802,82,"Me neither lmao",NA
"804",803,82,"*grows up to develop the tiktok algorithm*",NA
"805",804,82,"second this, distributed compute gets cheaper each day making genetic solutions ever more viable. Some hard search problems are solved relatively easily by GAs, albeit with a hefty cost of power relative to other methods... but still! I'd have appreciated that book as a baby lol",NA
"806",805,82,"In the great green room
There was a Jupyter notebook and a GPU
And there were two little numpy arrays sitting on chairs",NA
"807",806,82,"I found one in the same series that was ""Bayesian Probability for Babies"". Might need to grab that one and see if I missed anything.",NA
"808",807,82,"What do you think the upper limit on age is? My friends have a 2.5 ish year old. Wonder if these are too baby oriented.",NA
"809",808,82,"Great ;)
Nice twist : We synthesized this using neural networks
https://youtu.be/Jw02N9mYiCU

So I just had to read your little poem with her voice ;)",NA
"810",809,82,"This all sounds like a great way to make money. I'm gonna start working on ""Quantum mechanics for Babies"". Any buyers?",NA
"811",810,82,"I have both and likes the Bayesian Probability one better.",NA
"812",811,82,"I need this one.. BayesBaby",NA
"813",812,82,"at that age, they're past staring at shapes and more interested in animals and learning their colors. and they want a story with a beginning, middle, and end. babies just like staring at how shapes meet up and relate to each other.

 I babysat a kid who went from the ""lookit the shapes"" stage to ""what color is that"" to ""name the animal"" to ""tell me a story"". Amazing how his little mind changed so quickly over the months and how he got bored with books and demanded I read him something else, which he previously hated.",NA
"814",813,82,"That's a good question. I don't know how to answer it though, because I only have an 11 month old son.",NA
"815",814,82,"Already a thing! Well, nearly.[Quantum Physics for Babies](https://www.amazon.com/Quantum-Physics-Babies-Baby-University/dp/1492656224/ref=mp_s_a_1_4?keywords=quantum+physics+for+babies&qid=1583814136&sprefix=quantum&sr=8-4)",NA
"816",815,82,"I'd be both happy and disappointed if you write it.",NA
"817",816,82,"I saw \[this\]([https://www.amazon.com/Baby-Loves-Quantum-Physics-Science/dp/158089769X](https://www.amazon.com/Baby-Loves-Quantum-Physics-Science/dp/158089769X)) in a store two days ago",NA
"818",817,82,"They have it in the For Dummies series. It was a great gift I got after graduating...with a degree in that field. Thanks fam!",NA
"819",818,82,"Oh comon... And it even seems to be doing quite well.",NA
"820",819,83,"I dont know anything about dataquest but I can say that 1-2 hours is fine if you are doing it along side data science based work. 1-2 is also fine if you are the smartest person on earth and playing with mud. If it is enough for you  is hard for me to say.",NA
"821",820,83,"Generally studying 10hrs a week is enough to get you into any career within a couple of years.",NA
"822",821,83,"Studying that much is awesome! I will say, though, something nearly as important as the things you know is who you know! I'm sure you've heard this before, but it's totally true!",NA
"823",822,83,"Is that the way you learn? Maybe you could take on a home-project close to your interests and learn faster with a book, some online resources and an actual data problem you'd take interest in solving.",NA
"824",823,83,"The answer to that question depends entirely on how you study, how you learn, and how quickly you want to break in to analytics. I studied psychology as well, and several of my classmates went straight in to data analyst roles. If you took a stats course, you could probably convince someone to hire you. You might as well apply to jobs while you're learning - at some point your skill level will hit that threshold.",NA
"825",824,84,"That is literally its purpose.",NA
"826",825,84," [https://www.freecodecamp.org/news/what-is-git-and-how-to-use-it-c341b049ae61/](https://www.freecodecamp.org/news/what-is-git-and-how-to-use-it-c341b049ae61/)   
you can read this introduction to see what it does.",NA
"827",826,84,"As another commenter said, that is the purpose of GitHub. What GitHub focuses on mainly though is being a version control system in a collaborative setting. It allows you to save code at “checkpoints” (known as commits) which allow you to see what code you have changed/added/omitted from older commits. It also allows collaborators to offer changes or suggestions to code and GitHub then allows you to review the proposed changes in a succinct display. It is incredibly useful and almost all employers utilize GitHub because of its ability to allow code revision and preservation.",NA
"828",827,84,"Yes, just make sure not to leave any credentials in the code",NA
"829",828,84,"Always!",NA
"830",829,84,"Yes.",NA
"831",830,84,"Sweet... Now I am super excited for some reason.

Thanks!",NA
"832",831,84,"Am sorry, but what sort of credentials are you referring to?",NA
"833",832,84,"Welcome and good luck on the journey! 👍",NA
"834",833,84,"Credentials as in private login information to a database/data source, or API keys that someone else could take.",NA
"835",834,84,"Oh sure, thanks! Will keep that in mind!",NA
"836",835,84,"Yeah it’s called ‘making a chocolate fireman’",NA
"837",836,85,"Yeah, but I think discord would be easier. If you'll go through with this dm me, would love to join",NA
"838",837,86,"Delta Lake (delta.io) is an open source extension to parquet that can do this.",NA
"839",838,86,"We did something like this, without Delta Lake, by doing the following:

\- figuring out a time-based partition that made sense (in our case, date, but it could also be  a larger or smaller interval)

\- using application logic to filter all the data by that partition

\- coalesce(1) to avoid sub-partitions in the data

\- writing parquet to something like ""../data\_table/date=<YYYY-MM-DD>""

\- reading from the parent folder ""../data\_table""

been meaning to write a blog post about it sometime, as this is a common problem that nobody seems to be posting solutions for.",NA
"840",839,86,"Parquet is not really designed for this.

Parquet works best when you have all of your data in memory (in Arrow, Spark or Pandas DataFrame, etc), then you dedupe it, and then write it to Parquet.

I've never been able to find a way to stream to Parquet in real-time. Plus in order to dedupe, you'd have to load in all your previous data to compare it to the new observation. Only Spark has a parquet append mode, but it doesn't seem to work by appending to a parquet file, it makes a new file and appends it to the dataset (ie. collection of files). This is very inefficient, because Parquet's speed comes from having a large group size.

I've been looking into using Kudu as a transactional storage layer but retaining the columnar format like Parquet. Right now I'm using Apache Drill for the query engine, and with the built-in storage plugin, it should be able to handle Kudu seamlessly.",NA
"841",840,86,"Interesting, looks a bit heavy but is getting into the data snapping business.   


I wonder if they handle consistency contexts and dag structures? I don't see it in the docs. So bassically repeated calls in one version context should always return the same results.  So reading from the external layer necessitates pushing the latest-read envelope.  


Maybe they call this something else.",NA
"842",841,86,"Yes! This is a common problem that is sort of simple but ends up involving a lot of book-keeping and FS faffing.   


The layer on top of that in my usual setup is the snapping over a discrete time window (monotonic clock). Everything after this is derived data. You just need code and the coalesced version to reproduce if you've capture ALL the state.",NA
"843",842,86,"Yeah, that is sort of my point. You \*assume\* that you know a good partitioning that gives you in memory chunks. groupby parition on new data and zip through the compares and land the appends in the partitions.  


This is based on two assumptions, a) data arrives in small batches (typical) and b) human knows good partitioning (reasonable).",NA
"844",843,86,"The whole point is data consistency for ML projects. You're right is not lightweight. Requires at least some data engineering.",NA
"845",844,86,"It's not clear to me that it actually has that built in though. The fundmental building block (for me) for consistency driven DAGs is a discrete clock at ""the edge"" of the DAG where data enters the system. The clock ticks and you can register new data hashes in that window. Then everything derived (pure functions of that data an immutables like strings) can be compute ASOF that window version.   


Maybe that is their basis, but it seemed from my quick read they are more toward monolithic streaming applications (push, live systems) and not lazy merkle dags (pull, analytics).",NA
"846",845,87,"For any analysis that you want to be persistent and be re-usable, code beats every single graphical tool I've ever looked at.  Every.  Single.  One.

There's a really simple reason:  source code control.  When you write Python or R, you create a human-readable record of the logic that went in to your analysis or visualization.  You can put that code on github, submit a pull request, and get the code reviewed by a colleague.  You can modify that code, and clearly understand the history of modifications through text diffs.

Many graphical tool vendors try to replicate this functionality through their own specialized tooling, but it is never as effective as a source of record than simple, elegant, code.  Those graphical tools have their place, but they will never, ever replace writing code.",NA
"847",846,87,"Well...where to start. BI in general is to python/R as Guitar Hero is to Eric Clapton and Jimmy Page.

1. These tools very much depend on cleaned and organised data. To get it to that point, you need other tools. R, but especially python is ideal for this task (amongst others, Hadoop, Spark....depending on your pipeline). Throw any real raw data at tableau and it will fall over. Yes I know they have wrangling, cleaning tools etc, but this only goes so far.
2. ""in a world where I can create a chart by dragging/dropping fields"" - same answer
3. ""run a statistical analysis by highlighting a few columns and pressing a button"". Only a very basic one, based on a preset menu, with limited ability to drill into details on an adhoc basis
4. What you don't get from any of those tools are machine learning capabilities or any advanced sort of scientific or operations based capabilities. Try running a fourier transform or advanced linear optimisation routine in any BI tool. Python would be used to manage these through libraries such as Scypi, SKLearn, Gurobi and so on. For example doing music genre classification using Librosa and Tensor flow is a classic ML problem and use case for Python. You wouldn't describe a BI tool as the wrong choice for this, you'd say ""it's not EVEN wrong...it's just not capable of doing it""
5. R especially has an incredibly deep and wide statistical and analysis library set (although it could be argued python is catching up...depending on your point of view and area of interest). In terms of analysis capabilities BI tools cover probably less than 1% of what is available in either R or Python. Both in general have huge numbers of libraries available to do just about anything you want. And if it can't, you can write it yourself.
6. Both are programming languages - Python explicitly so. Python was used to build Instagram, Pinterest, Youtube and Spotify. Reddit is written in Python (!!!!!). Tableau would be ""not even wrong"" here as well.

Etc...",NA
"848",847,87,"(Prepare yourself from a bunch of sassy answers btw).

I think there are a couple of advantages to R and Python over something like Domo, but some are going to be relevant to your job today, some to your career, and some to your organization.

Current job:

* A scripting language's biggest advantage over a GUI tool is going to be flexibility. GUIs normally support a finite set of ""stuff"", and if your use case does not fall in that range, you will normally have to spend a disproportionate amount of effort trying to coerce it to do what you want it to do.
* The second biggest advantage of a scripting language is going to be that it's easy to reproduce and extend work that you have already done. If you did an analysis for one product and you now need to do it for 3000 products, sometimes that will require you doing the same thing 3000 times in a GUI, whereas in a scripting language it will require you adding like 2 lines of code. In addition, when you go through a GUI workflow, your process is not stored. That means that if 3 months from now you need to do the same thing again, you may need to go through the same sequence of steps and may not remember them exactly. In a scripting world, you will have your script from 3 month ago and you should be able to reproduce your results.
* The third biggest advantage is that new developments come first to R and Python. If there's a new model, tool, database, functionality, etc., it's going to make it's way to the open source tools first - and much earlier - than it will to paid solutions.

Career:

* The biggest advantage of R and Python over Domo is that comparatively speaking, very few companies use Domo. So if all you know is Domo and then you go to a company that uses DataRobot, or RapidMiner, or Alteryx, then you're back to square one. Python and R are always going to be the same and available pretty much everywhere you go.

Organization: 

* It is a really bad idea for an organization to become tied too closely to any one software provider. If all your analytical capabilities rely on Domo, then when Domo goes out of business/changes their offering/lags the competition/gets acquired by Microsoft/etc, you are potentially shit out of luck. And there are very few offerings in this space that can be counted on to be known quantities forever.
* Personal opinion: It's also a really bad idea to spend a bunch of money for stuff that can be done for free.
* It is also a really bad idea to open up black box analytics solutions to an organization unless you can guarantee that every person using them has at least a baseline level of understanding of basic analytics best practices. Otherwise what you may be creating is the uncontrolled proliferation of bad models and analysis.",NA
"849",848,87,"Can't speak to the others - but Domo has a data science toolkit where you can kick out things to Python/R, process them and export the data to a dataset.",NA
"850",849,87,"The short answer is that you can do *more*. You can also do it faster; computation wise, not necessarily user wise.",NA
"851",850,87,"If you want skills that will transfer to another company is the biggest reason to diversify your skill set.",NA
"852",851,87,"It’s to know who actually knows how to do things",NA
"853",852,87,"It is a good question, and there is more to it than just CLI vs GUI.

All of the above can be used to do analysis for decision support. But R and python allow you to not just be a *user"" of tools for analysis, but a *builder* of products from the ground up for others to use.",NA
"854",853,87,">*BI in general is to python/R as Guitar Hero is to Eric Clapton and Jimmy Page.*

This is the greatest quote of all time.

I appreciate the comments.  Some of this I think I already knew, but I don't know that I trust my own opinions just yet.  It helps to see it echoed by someone else.",NA
"855",854,87,"    

>*(Prepare yourself from a bunch of sassy answers btw).*

Fear not, my snark-o-meter is on full power.  I'm new to DS, but not Reddit.  :-)

Thanks for taking the time to share so much info.  Like I mentioned in my reply to u/jimmythenewsmith above, a lot of this I think I kind of knew in my bones, but don't fully trust my own opinion just yet.  

I am in a unique position, as (at the moment, at least) I am not looking switch companies.  After 20+ years, I am wearing a pretty hefty set of golden handcuffs.  So, in that respect it would seem that learning our own product would be the right path to take.  We, of course, like to 'drink our own champagne' (or eat our own dog food, based on your opinion), and being skilled in our own product seems like ti would have distinct advantages.  

I guess my question at this point is:  
**Does knowing R / Python make one a better DS?**",NA
"856",855,87,"Ok, so that's a workflow I hadn't considered, and confuses me even more.  

If you can do your analysis / visualization in Domo, *why* would you want to export to Python (unless it would be to import into something else).",NA
"857",856,87,"So, in that respect, it *does* sounds like the command line vs GUI argument.",NA
"858",857,87,"That makes sense - the more you know, *the more you know,* right?

There's just SO much to learn.  I spent an hour last night in R trying to do what would take me 10 seconds in Excel.  While it would be easy to pop into Excel, make the changes I wanted, and start again in R, I know that being able to do it in code is the right way to go (if I'm truly trying to learn the language). 

I know that the skills will come, it's just slow going.",NA
"859",858,87,"Does knowing R / Python make one a better DS?

It's pretty much a prerequisite in 2020.",NA
"860",859,87,"Others will disagree with me, but I think that is going to be purely a factor of what work you're doing. If Domo has the ability to support the data science work that you're doing and there is no one pushing you to learn R or Python, then I don't think learning R or Python should be a priority for you.

Does it make you a better DS?

It doesn't until it does. That is, as long as Domo supports your needs, odds are that knowing R or Python won't provide a ton of value add.

The value comes when you run into limitations with Domo, i.e., if Domo doesn't support what you *need* to do (assuming R and Python do), and as a result of that you just cannot do it, then yes, knowing R and Python would at that point make you a better DS.

More generally speaking, yes, knowing a general scripting language like R or Python makes you a better DS because it gives you more options. It means you can do anything that someone familiar with Domo can do + (other stuff). Know, whether that other stuff is valuable today or not is a different story.",NA
"861",860,87,"Their Data science tools are in their ""Magic ETL,"" so basically you ingest a dataset, do any ""data science"" type calculations in a Python/R tile and then create a new dataset that you can use for visualizations.",NA
"862",861,87,"Tableau is mostly used for data analysis and visualisation. In that regard, it's quite similar to Excel. You can do some simple data manipulation and create some trend lines, but you can't use it to do complex feature engineering and build state of the art classification, regression and text analysis models. In Python and R, you can.",NA
"863",862,87,"Excel can only handle a million rows. The current project I'm working with is handling 55 million rows. You can't scale your analytics and skill sets within Excel.",NA
"864",863,87,"I'd say that the benefit would be knowing the underlying allows to build more complex and personalized tools.",NA
"865",864,87,"

Thanks for the insight, it's much appreciated.",NA
"866",865,87,"Tangentially related, I'd never heard of Domo prior to a graduate class I took recently.  What's your take on them?  Is it worth spending more time on it?    


I found it delightfully simple to use, I'd just never heard of it before.  I'm not sure if they're just not widely used, or if this is further evidence of my lack of experience.",NA
"867",866,87,"Ah. That's something I hadn't considered...",NA
"868",867,87,"I like Domo a lot - it seems to really excel in the mid-market space. Gives companies who can't hire BI people a great BI platform. 

(Full disclosure, I'm a consultant who does mostly Domo now but came out of the MS BI stack before this) 

Domo can be stood up and operational in 10% of the time of a ""traditional"" BI  solution and overall is great to use. I can't say if it's widely used since I deal mostly with companies who are already using Domo.",NA
"869",868,88,"The book Data Science For Business, by Provost and Fawcett, seems a good start.",NA
"870",869,88,"Profit Driven Business Analytics by Verbeke, Baesens, and Bravo was a good read. I have have the pdf if you’d like to read it.",NA
"871",870,88,"What also would be a good start.

Is follow a management consulting course focussed on customers on something like Udemy.

When to use certain approaches it very much based on setting and business model.

To grasp that and know how to proceed it very important to understand the business processes.",NA
"872",871,88,"Machine Learning for Business by Doug Hudgeon, Richard Nichol",NA
"873",872,88,"ebook: Creating value with artificial intelligence. I've been reading this one, it's pretty good",NA
"874",873,88,"That's more of a general how to data science for business leaders. Not so much FMCG/retail analytics which I think is what OP was looking for?",NA
"875",874,88,"Thank you, I will definitely check it out",NA
"876",875,88,"Please share the link for the above Book.",NA
"877",876,88,"I would like to read it, thank you!",NA
"878",877,88,"I'd appreciate a link to the pdf, please!",NA
"879",878,88,"I would also like to have the link.",NA
"880",879,88,"I would also be interested.",NA
"881",880,88,"Yes, I assumed that every problem requires specific approach. I will search for such course/materials too",NA
"882",881,88,"I will check it out, thank you for suggestion",NA
"883",882,88,"Thanks for the reply, I will have a look at this ebook",NA
"884",883,88,"True, it's very introductory. I just mentioned it because this book helped me a lot to comprehend how data science is used for business.",NA
"885",884,88,"Hi,

What business process you want to dig in?",NA
"886",885,88,"Yes it definitely has helped me with consulting. It's a great book",NA
"887",886,88,"Hey,

I am currently working on deriving statistics and insight from dataset of transactions in retail market",NA
"888",887,90,"Not used the one from Alteryx yet but its probably no different than all the other self serve BI/DS type tools. 

It's good for BI teams if there is structured well defined data to overlay it on, but for any more serious data science its quicker, more powerful and more flexible just to code your models and manipulate your data yourself. (More algos, packages, options to tweak for deployment, etc..) 

I got a preview of Azure timeseries insights when I was at microsoft the other week (I think its officially rolled out by now, maybe in beta), and that's pretty cool, can deliver some powerful results quickly, but again, really predicated on having really good, clean, well defined data - i.e. the environment self serve BI type teams work in.",NA
"889",888,90,"its gonna be an addition in price

im not sure its gonna best some current cloud autoML",NA
"890",889,90,"But they explicitly said its not AutoML - they call AutoML a black box vs assisted modeling, where you actually have a view into each step of coming up with a ML model",NA
"891",890,91,"Do you have individual level data or are you working with aggregate data? If you're working with aggregate data, and combining studies to evaluate something this is know as meta analysis and you can use that term to google methods. 

&#x200B;

If you have individual level data, it's a different methodology. You're trying to find the relationship between two variables so it's a regression, I doubt it's linear.",NA
"892",891,91,"Whether or not you should use linear regression really depends on the shape of your data. First step is taking a look at a scatterplot... if the points are in roughly a straight line, a linear model may be appropriate. Or you may find that it tends to approach an asymptote (after all, abilities tend to be bounded), in which case a linear model may not be appropriate. Without actually seeing your data, it's hard to know.",NA
"893",892,91,"I'm not sure, depending on what studies I'll find. I didn't search for the data yet. I think I will be working with individual level data if I find it. From what I understand, individual level data would be having each test subject with the data about their consumption and performance on mental tests while aggregate data would be something like ""most 16 year olds have been shown to do this test in about X more time"", right?",NA
"894",893,91,"Yes to your last question. Individual level data is hard to come by...any of this data is hard to come by because until recently it was illegal in most places. I would first find what data was available and then figure out how to work with it. Right now, you have the cart before the horse.",NA
"895",894,91,"Wtf why was it illegal? Privacy reasons?",NA
"896",895,91,"Cannabis use is illegal so you don’t have good usage data. People will underreport.",NA
"897",896,91,"Oh I thought you meant individual level data is illegal lmao",NA
"898",897,91,"If it violates HIPPA it is. Unless you’re a medical researcher access to that type of data is rare.",NA
"899",898,92," [http://www.stats.org.uk/probability/propensity.html](http://www.stats.org.uk/probability/propensity.html) 

So you would use probability to describe likelihood for someone to buy overall, propensity of likelihood for someone to buy either last year or this year. 

I would start by looking at the propensity distribution of each group and selecting a range that appropriately describes customers from each group. You could then compare the features to either find common characteristics among all customers or distinguishing characteristics between customers of last year and this year.",NA
"900",899,92,"Sure, you might be able to find a match on gender or race. But what happens when there is no exact match for a collection of features? Or if you’re trying to match on a continuous value?

Propensity score matching allows you to collapse all of these variables into a single score you can then match on. You could also look into coarsened exact matching which, if I recall, uses a method more akin to what you have described.",NA
"901",900,92,"so I think that makes sense.  So is propensity matching basically matching 2 people who have the same probability of buying?

so if person  in group 2 has 90% of buying, then it will match with person in group 1 who also has 90% of buying?

In other words, are we saying that person from group 1 who has 90% chance of buying has the same general characteristics as person in group 2 who also has 90% of buying?

&#x200B;

So, are we also saying that if preson in group one has 10% of buying and no one in group 2 has around 10% of buying, then that person will be thrown out because they are not similar?",NA
"902",901,92,"Isn't it better to use mahal for matching when there aren't too many continuous variables that we're matching on?

I've never understood coarsened exact matching, and have always been offput by it because it's not in the R MatchIt package.

And last thing you said that there are different strategies on matching after a propensity score is calculated in another comment. Do you know a good source that will go in further detail about that?",NA
"903",902,92,"Yes, that’s the goal. There are many strategies to match after a propensity score has been produced which I suggest you read up on. It sounds like you get the broader idea now so they should make sense to you",NA
"904",903,92,"Is buying your outcome variable or your treatment variable?",NA
"905",904,92,"Why PSM shouldn't be used -

[https://gking.harvard.edu/files/gking/files/psnot.pdf](https://gking.harvard.edu/files/gking/files/psnot.pdf)

Pretty good high level explanation of CEM -

[https://medium.com/@devmotivation/cem-coarsened-exact-matching-explained-7f4d64acc5ef](https://medium.com/@devmotivation/cem-coarsened-exact-matching-explained-7f4d64acc5ef)

&#x200B;

Ultimately, they're all pretty shit as a point estimation of ground truth and it's trivially easy to show whatever you want to show.

The biggest problem is that outside of a very established program, it's extraordinarily difficult to know, a priori, where all of your sources of bias are coming from.  So you're a week into your program evaluation and find a massive variable you weren't formerly controlling for - now you don't need to just include it but you need to potentially choose a binning strategy (or whatever) that makes sense.  Unfortunately, you're looking at the data and making decisions about your model and you're going to turn right back around and use that model on the same data to evaluate your program.

The whole exercise is maddening and there's zero chance I'd accept a single proposed model/estimate from a vendor.  Using bootstraps \*within\* a model framework isn't good enough and provides a false sense of confidence.  The framework itself has to be 'bootstrapped' as well.",NA
"906",905,92,"it does.  thank you!",NA
"907",906,92,"I don’t think I understand.  Are those not the same thing?",NA
"908",907,92,"okay...I think I know what you are referring to.

'buying' is my outcome variable, but 'last year' is my control while 'this year' is my treatment.

So in the example of the medical field (which a lot of examples are based on):

sick/not sick is my outcome.  
took drug  is my treatement.",NA
"909",908,92,"What do you think is a better alternative then?",NA
"910",909,92,"This is a bit strange since the propensity score is the conditional probability of being assigned to the treatment group conditional on the covariates. Does probability of 'this year' given say age and gender make sense? Probability of smoker given age and gender makes sense, but I'm not so sure about 'this year'. I suppose it depends on what this year actually means.",NA
"911",910,92,"To matching in general or PSM?

Better than matching - RCT.

Better than PSM - CEM.",NA
"912",911,92,"I either dont have a good understanding of 'treatment' vs 'outcome'  or this may not be the best example,

but I guess here is an overview of the problem i'm trying to solve if that helps.

Last year, we had s 200 who shopped but only 100 purchased.

This year, we had 500 who shopped, but ony 200 purchased.

I want to compare the rate of purchase between last year and this year.  

I can simply compare the quotient of both years, but the customer base (characteristics) from last year may be different then this year.  so I figured,  that with PSM,  I want to match customers this year to similar customers last year, and do that comparison.

&#x200B;

Obviously, theres the issue of the unmatched group that can mean many things and need to be considered, but for now, I just want to compare customers this year (who are similar to customers from last year), and see their purchase rate to prior year.",NA
"913",912,92,"and if you can't do a RCT?",NA
"914",913,92,"Which, unfortunately, is ultra common, right?

With a normal turn around time requirement - CEM.

But given that there's still so much potential model dependency around how you bin, what variables you use etc, I'm a fan of doing 1000's - 10,000s of runs where the variables and the bins included are varied.  You end up with a distribution that you can feel much more comfortable with since you're reducing the number of arbitrary decisions you're dependent on.

The downside to this is that there isn't any software out there to do this for you - you have to build it yourself so it really doesn't make a ton of sense for a one off.",NA
"915",914,94," [https://www.worldometers.info/coronavirus/coronavirus-age-sex-demographics/](https://www.worldometers.info/coronavirus/coronavirus-age-sex-demographics/) 

&#x200B;

Find weighted average\^\^

I don't really know your intended application, but mode would be a more appropriate statistic if you are trying to figure out the most affected demographic.",NA
"916",915,94,"Wolfram alpha has individual level data from China.

Download link at bottom of the page, comes across fairly garbled.  [https://datarepository.wolframcloud.com/resources/Patient-Medical-Data-for-Novel-Coronavirus-COVID-19](https://datarepository.wolframcloud.com/resources/Patient-Medical-Data-for-Novel-Coronavirus-COVID-19) 

&#x200B;

However, consider how to account for preferential treatment. In Italy it sounds like they triage based on success rates, so if you have chronic conditions and not likely to survive, you won't get a ventilator anyways which compounds the death rates in the high risk groups.  You cannot just add up and divide by the number of infected here or guess. You need to use epidemiological method here to account for all of these factors.",NA
"917",916,95,"I highly suggest this one:

[University of Washington Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning)

it's more machine learning focused rather than the Data Science with R course that I think you're talking about; which I find to be more Data Analytics.

However for analytics like you find in that course I don't suggest Coursera, I think [Kaggle's tutorials](https://www.kaggle.com/learn/overview) do a really good job of transferring what you'd learn in the R course to Python. I would suggest looking over the Kaggle course and then before you get into Kaggle as a platform, doing the courses I recommended above.",NA
"918",917,95,"Hi thanks for your reply will look into Kaggle courses!",NA
"919",918,96,"> The problem is that if a model is always right in this case it would be unusable 

You're fundamentally misunderstanding something here, but it's not obvious to me what it is.

If I know with absolute certainty who is going to churn and who isn't then I don't have to waste resources on the people who will stay regardless.",NA
"920",919,96,"If it is indeed 98% accuracy, you wouldn't really need to worry about unchurned going to churned. On the onset, you can predict those right? Because if that is the case, you'd definitely tag them with high risk of churning.",NA
"921",920,96,"It sounds like you don't want something that can classify if it is churned or unchurned, but instead predict the risk of churning. Is that right?",NA
"922",921,96,"Classification is prediction with a threshold.  Don't apply the threshold.",NA
"923",922,96,"I am not sure I understand your data right. Is it a type of before and after, and are you trying to predict whether an observation is going to churn or not?",NA
"924",923,96,"Putting aside the question of which customers you should prioritize for targeting retention efforts on (as other posters have mentioned above), I suspect the question can be formulated as - what is the probability of a person churning in the next X months, given his previous Y months of history. 

(Not sure if you've already done, this, but I'll just explain below what I think could be an approach) 

So for every person you have in your dataset, churned or not, I suspect you could chop their history into say 3 month periods (starting from the latest time /time they churned). (The choice of months is domain dependent). You can use every prior 6 months data to predict the probability of the person churning in the next 3 months. Choosing a model that will output a probability /risk allows you to prioritize them (but as others have said, you'd probably want to target those who are high value instead of only those who are likely to churn) .

Then now you apply this to last 6 months data to predict each (currently unchurned) person's probability to churn in the next (future) 3 months. Then you can identify who might churn soon who you'd want to keep.",NA
"925",924,96,"If you have a 95% accuracy, theres a 99.9% chance that your model is overfitting.",NA
"926",925,96,"I don't know if you've properly established the modeling problem to be useful for your business case. You can easily verify your label — whether someone (right this very instant) has churned or not. What you want is not whether they're churned right now, but (e.g.) their probability of churning within the next 2 weeks.

Framing the question this way, you need to build your observations using features from time \[0 -> now - 2 weeks\] and the label from today/right now. Then, you have a simple binary classification model. To deploy, you'd just ""shift"" that window 2 weeks ahead: have your model use up-to-date features and it'll predict churn probability within the next 2 weeks (or whatever time frame you choose for your label).

Edit: Also, consider that accuracy may not be your best metric here. Consider looking into cost-sensitive learning (e.g., weighing the minority class) as well as using recall -- or precision (again, depends on your use-case) -- as your core metric. Look into model evaluation metrics that are robust to class imbalance, like Cohen's kappa.

Edit 2: Also, consider that your model might be performing well for a number of reasons. For starters, it's common for models to just learn the majority class rate and do no better than a dummy estimator that predicts the majority class. If your model does no better than a model that predicts the majority class, your model is basically wasting production clock cycles :)

---

Alternatively, I suppose you could use your current formulation but a different modeling approach to address the anomaly detection task: train the auto-encoder on a bunch of un-churned/non-anomalous (majority class) users, then look at your reconstruction error (your loss). If it's high, this might be your model telling you that it's never seen something like this before, in which case it's likely a churned user.",NA
"927",926,96,"Ensure your model takes into account the discrepancy in number of cases. You may want to generate a balanced data set first and use a separate validation set if you have not already done so.",NA
"928",927,96,"This was a bit of a sobering reply, what you say makes sense.

If the company wanted to know the top 10% of at risk churners I guess I would tune for 90% recall. But yea as you say in terms of resources it might be better just to target the top 2\~5% if thats what the best model is showing.",NA
"929",928,96,"Correct",NA
"930",929,96,"Hi yes this was my idea, all unchurned people fall over the 0.5 threshold - meaning they are classified as unchurned. Although some (presumably those at a higher risk) are falling closer to this threshold (\~0.6). My idea was to manually shift this threshold to \~0.7 or so.",NA
"931",930,96,"Hi thanks for the advice, I've used oversampling to balance out the number of churned and un-churned. (was roughly 15% churned 85% un-churned)",NA
"932",931,96,"> If the company wanted to know the top 10% of at risk churners I guess I would tune for 90% recall

No. That is not how it works.

If you're model is too accurate then you're model is likely to be overfit. 

To get the top 10% of at risk churners you need to first define how to rank users who are predicted to churn, but who have not yet churned (by whatever metric it is that you use to define churn).

Edit:

Why do you have 0 = churn if you're trying to predict churn? Wouldn't you want it the other way around?",NA
"933",932,96,"Not sure what you're using but some binary classifiers output a score instead of a classification by default, others it's just a simple tweak of code. 

predict_proba vs predict in sklearn log reg for example.

This is the commonly accepted approach when dealing with the problem you describe - ie allowing you to prioritize those who would likely churn based on business processes, it is often further tempered by some kind of optimization:

'we can do a mailing campaign for 1000 customers - who should we go after to keep successfully keep the most desirable customers.'

You don't want to go after the $ person with a .90 prob of churn you want the $$$ person with .51 proba.

Edit: added more context.",NA
"934",933,96,"Yea that is right. I did actually use 1 = churn but switched it for the example so I could talk about recall (not sure what the inverse of recall is).

&#x200B;

I am getting probability of churn for each observation (by using functions such as classifier.predict\_proba() in Python) and then selecting from this the top 10% of observations with the highest churn probabilities",NA
"935",934,96,"But does that make sense from a business perspective?

If those top 10% at risk of churning customers predicted user LTV is lower than the cost of the interventions to stop them from churning it doesn't make economic sense to even identify them.

I would want to target the users at risk of churning (i.e. above a certain churn risk threshold) who have the highest revenue potential to the company, rather than just those who are at the highest risk of churning. Why spend resources to minimize churn rates for customers who won't bring in revenue?",NA
"936",935,96,"It might not make sense from a business perspective you're right - as the top comment has mentioned. 

But hey if it's what the business asks for it's what they get right",NA
"937",936,96,"In my experience those making business decisions don't always have a very good grasp of statistics. What they ask for literally is not always what they actually want. 

E.g. I once and an executive ask for a metric of the average age of support tickets in the queue. They thought they wanted the average, but what they actually wanted to do is solve the problem of customers complaining about having to wait for a reply to their support ticket.

The average ticket age was reasonable, but there were a small subset of tickets who had obscenely long wait times due to many factors. It was the presence of that group that the executive was interested in finding.

When executives ask for very specific things I always ask supporting questions to try to understand what problem they are trying to solve or what decision it is they are thinking of making. Often, the data or analysis they ask for is not what they really want.",NA
"938",937,97,"Given any distribution function, F, the inverse of F, say F', maps [0,1] to domain of the distribution.  F will be the distribution of F'(x), if x is random number uniformly distributed over [0,1］.",NA
"939",938,97,"also the distribution in question is obviously a large question, the best way to describe it is you group a bunch of people based on income and some will randomly skew the distribution to a long tail (and no one has less than 0 dollars)",NA
"940",939,97,"if you're analysing income, this should follow a preferential attachment model. In this cases, a pareto distribution should be the way to go.

You can use the method of moments, using the mean and std to estimate parameters, and from the parameters, sample points.",NA
"941",940,97,"This is it. Thank you!",NA
"942",941,97,"or better yet, use a package, such as scipy.stats to calculate the parameters via MLE.",NA
"943",942,97,"I'm gonna use that one potentially, I have a few to try out. Thank you by the way!",NA
"944",943,98,"I'd like to remind people that basing feature selection of the results of a statistical test is a bad bad idea.

Most people will look to a p-value when performing a hypothesis test.  The reason that this is especially bad for feature selection is because it doesn't tell you the extent of the difference.  *I can always construct a test for which we reject the null but the difference is negligible*.  This is easily done by just increasing the number of observations.

This puts ML and hypothesis testing feature selection at odds.  Most models will require lots of data, but more data makes it easier to reject the null, so [everything winds up being selected (conditioned on the fact that the p value is the criterion for selection)](https://stats.stackexchange.com/questions/451480/feature-selection-for-logistic-regression/451576#451576).

If you're going to do feature selection, I think something like LASSO/Ridge is a good first step, but above all else business and domain knowledge should dominate the selection.

Also, many regression methods make no assumptions on the distribution of the predictors, so testing for differences in means or frequencies is irrelevant for most of what we do.  If you are worried about confounding and inference, that is a whole other kettle of fish (though my thoughts on that are much the same).",NA
"945",944,98,"In the real world, you determine whether or not to perform statistical tests based on the project requirements.  If the goal is to determine if a feature is beneficial, you state the pass/fail conditions prior to beginning the project.  These conditions include what statistics will be run and what the level of significance is needed.",NA
"946",945,98,"Statistical tests can be useful to try and cut through the noise and determine how strong a signal is in the data. For example, let's say you've run an A/B or pre-post test and you see a difference in line with your expectations, e.g., a 5% difference in the group means. Well at that point you might want to use statistical testing to determine if the mean difference you see was statistically significant, and not just an artifact of high-leverage influential cases dragging the mean around.",NA
"947",946,98,"95% of the time I just bootstrap whatever test I need.",NA
"948",947,98,"When you have a hypothesis about the data and you want to either disprove or fail to disprove it. It’s a way to suggest relationships you think might be there and explore thoughts",NA
"949",948,98,"Although I myself am new to statistical hypothesis testing, over the past 2 years this is what I have amassed:

Continuous variables, different samples: 2 groups - Independent T-test, > 2 groups - ANOVA

Continuous variables, same sample: 2 groups - Matched pairs T-test, > 2 groups - Repeated measures ANOVA

Categorical variables: Chi-square test for both 2 and > 2 groups

Ordinal variables (not normally distributed): 2 groups - Mann Whitney test and Median test, > 2 groups - Kruskal-Wallis ANOVA",NA
"950",949,98,"I used to teach an entire module on when to choose which statistical test (this was in science but the ideas are the same). There are many factors to consider when selecting a test, such as whether your data is parametric or not, what you want the test to show? How many sets of data you have. Each test has specific types of data that they work with. Chi squared works well with categoric data, and only really works with categoric data. Anova works when you have multiple sets of numeric data (as you can't do multiple t tests). Iirc for an anova to work the data should be normally distributed.",NA
"951",950,98,"What about basically adding some random variables as features, and then do the same tests? Assuming the significance levels of your random features as rejection threshold you can select features easily. It may not be best practice statistically, but it works practically by comparing features with real noise. And also I wonder your thoughts about built-in features importance calculations.",NA
"952",951,98,"There is a library available in R and Python called Boruta, which uses random forests to identify Independent variables that are consistently impacting the dependent variable of interest.  Here is a link to an article where someone explains how it works and how to apply it, https://www.datacamp.com/community/tutorials/feature-selection-R-boruta.
I have found this useful in projects I have worked on in the past to narrow/identify a subset of variables that I could use for modeling—while maintaining the original list of variables (I.e not using PCs or some sort of other factorization)",NA
"953",952,98,"I mean, statistics is just applied logic. You use statistics when you try to figure something out. Use a test when it helps tell you something useful about your data. I know that's tautological, but it's also true.",NA
"954",953,98,"these tests are used IRL definitely all the time when coming up with hypotheses and doing research on your data source",NA
"955",954,98,"Since you're asking for examples of when to use statistical tests, I'll give you the short version of something I'm working on. I'm on a team that is building a discrete event simulator to model a transportation system. We construct random variables so we can sample process durations or, in some cases, timing variances. It's important that we know how well the distributions of samples match the modeled processes and, to that end, statistical tests are helpful. We also perform distribution fitting when we become suspicious about the source data from which we build our random variables. And of course there is the usual descriptive statistics for just understanding various aspects. 

I'm not a statistician, but I do quite like statistics! I hope that helps.",NA
"956",955,98,"When you want to show you have insignificant chance of making a mistake if you believe in your analysis results being somewhere in the ball park of what these results say.  Essentially, you aren't totally out to lunch.",NA
"957",956,98,"As an addendum, for reasons on why stepwise selection is bad, see Harell's book.

Short answer is your statistical tests become overly confident because they are meant as a single test not repeated.",NA
"958",957,98,"Ridge is not gonna do feature selection and LASSO feature selection is only ""variable selection consistent"" if there are strong conditions on the correlation between features. 

Something a bit more complex like elastic net or adaptive Lasso would work though, but I think harder to set the regularization parameter correctly.",NA
"959",958,98,"Excellent reply. would have written something along these lines, but less elaborate. Have an upvote.",NA
"960",959,98,"[deleted]",NA
"961",960,98,"Permutation tests are another trick in that family.",NA
"962",961,98,"This. I did exactly this.

Used a hypothesis test to figure out if the use of incentives (gifts) for customers that made a purchase really brought an impact to sales.

Since we got a high pvalue we did not recommend to keep the incentive going forward.",NA
"963",962,98,"I just want to point out that you can use a mixed model for the exact same design as a repeated measures ANOVA, as well as in scenarios that violate the assumptions of RM-ANOVA.",NA
"964",963,98,"There is no such thing as ""parametric data"". Being parametric is a quality of models, not of data.

And as others say, ANOVA is not concerned with the distribution of the variable. It places an assumption on the distribution of the *residuals* (normality), and is somewhat robust to violations of this assumption w.r.t. strength of violation and sample size.",NA
"965",964,98,"Anova does not require normally distributed raw variables. It does require approximately normally distributed model residuals.",NA
"966",965,98,"ANOVA can be used for large enough samples even if the data is not normally distributed",NA
"967",966,98,"> I know that's tautological, but it's also true.

😒",NA
"968",967,98,"Stepwise is a pox on statistics, and if I could I would rid this world of this awful procedure.",NA
"969",968,98,"Ridge doesn't do feature selection only because the features are never selected out of the model. You can still look at coefficient paths to see which coefficients are being shrunk first. Even considering ridge does not select features, it is a way better approach than p values.",NA
"970",969,98,"What about exploratory factor analysis?",NA
"971",970,98,"Thanks; I have a tendency to forget that many people do not understand that re-requisite.",NA
"972",971,98,"Thank you for the insight. Really appreciate it! :)",NA
"973",972,98,"Yes, but only *approximately*. Like most approaches, it has problems with very long-tailed distributions, llike the ones in finance and insurance.",NA
"974",973,98,"Why is this?",NA
"975",974,98,"Forward selection is bad for inference.  It introduces stochasticity into the model for which the Wald tests can not account for.  This results in p values being too small, confidence intervals too narrow, and coefficients being biased (according to Harrell at least).",NA
"976",975,98,"what is this book that Harrell wrote? 

(I’m scouting all sources I can find to accumulate a reading list and dive into data science)",NA
"977",976,98,"Regression Modelling Strategies",NA
"978",977,99,"Definitely go through that. If you’re not familiar with data warehousing, look into that as well.",NA
"979",978,99,"Good on you for getting into this, still too many organisations are prioritising data/analytics platforms ahead of data/information strategy management.

Would also recommend reading ‘The Information Driven Business”",NA
"980",979,99,"Thank you! Will make sure that I refresh myself on Data Warehouses (and lakes for that matter.)",NA
"981",980,99,"Ill add it to the list! Thanks for your time",NA
"982",981,2,"Remove tenacious and scholarly from the bio and the big Data Scientist at the top. Move education above previous work and projects. Since previous work is latest 2016 it looks like that’s the last time you did related work.",NA
"983",982,2,"Overall, your resume does not look bad. I think maybe it comes off as though you have very limited professional experience. I would try to make that stronger - you should take the often-repeated advice of quantifying your experience. For instance, add some numbers that give your statements meaning. ""Analyzed large amounts of data..."" is honestly a terrible sentence for a data science resume. What kind of data? This is way too vague to be meaningful. I know you said you haven't gotten an internship yet, but that may be your best bet. Having a data science internship would help your professional experience SO much!

The next thing I would recommend is re-arranging the section order. I would have education near the top since this is probably one of the things they'd look to first. Your professional experience is probably more important that your projects, too, and might be better off higher up. I think the shortened URLs look pretty odd to me, I've never seen that. I have done hyperlinks before, but the links don't look good in my opinion. If they are on your Github, that's good enough. They'll take a look. Also, consider using bullets for your projects to match the rest of the resume.

&#x200B;

For skills, instead of calling it ""Technical"" and ""General"" maybe call those sections ""Skills"" and ""Course work"".

&#x200B;

Good luck to you!!",NA
"984",983,2,"I'd remove the word Scholarly from your summary, that may be a detractor in industry. 

Tell me more about your projects. What did you do specifically? What business problem were you addressing? What was the outcome? 

Show end to end capabilities. What data sources did you use? Did you source and cleanse your own data? How did you communicate the results? You want to show that you can take a concept, source and cleanse the data, build valid models, and communicate the results in an approachable, cogent manner. 

Have you looked at consulting organizations? Deloitte and Accenture have huge data science and analytics staff, as do IBM and many others.  This is all Mu Sigma does, and I think Cognizant is a big player in the space as well. 

Good luck and hang in there!",NA
"985",984,2,"One thing you may want to consider is adding your citizenship status. 

Should you have to? No.

But it may improve your response rate.",NA
"986",985,2,"Remove the MNIST project from your resume.  There are two problems with this:  


1.)  It's considered the ""Hello World"" project for any data scientist and isn't really that impressive.

2.) 98.6% is actually not a great score.  99.8% is really considered the baseline, and 99.9% is considered good.  If you're dead set on including this project on your resume, improve your score and be prepared to explain what you did to get those results.",NA
"987",986,2,"You can also make it look little more aesthetically pleasing... There are a couple of websites, you can try visualcv.com. it is free, and has cool templates",NA
"988",987,2,"Hello listen I believe you have to show them education,  skills and projects in that order . As many said your resume is not bad . Look for internship even. NYC is a great city for DS jobs. Shouldn’t be a problem also. Revamp your LinkedIn as well. As friends for referral . Just applying online is not enough anymore. Good luck",NA
"989",988,2,"I’m not a recruiter however I can share my experience what recruiters looked in resume.

Firstly no headlines / motivations at the top put your name and social media handles up only at the top

Secondly put your experience and skills and softwares you used. If you don’t have experience or have experience less than 1 year preferred to put skills at the top 

Thirdly put your education ( only top two not all for e.g. like Masters, phD etc)

Fourth projects ( don’t put common projects which everyone has done it while pursuing their degreee) 

Lastly Awards and certifications ( if any)

Always try to be restrict in one page with no fancy Layout and try to use one column page resume. 

See the template below but try to put experience first before education.

[Resume Template](https://github.com/sb2nov/resume) 

Hope this help! Good Luck.",NA
"990",989,2,"I have no advice as I’m in the same boat as OP, I’m literally losing my hair trying to find a job right now, it’s crazy, but I have my fingers crossed and I hope all of us get what we put in all the work at the unis for.

Best of luck mate.",NA
"991",990,2,"Thank you all so much for the suggestions. I really appreciate it. :)",NA
"992",991,2,"Thank you. I agree with your suggestions.",NA
"993",992,2,"Ignoring the rest of your post as I don't wish to offer any advice to OP, just a sentiment in general.

&#x200B;

It is simply ridiculous that a recent graduate with a relevant M.S., knowledge of industry standard tools, visible projects, and Junior Analyst experience; needs an internship, and re-ordered resume to stand out amongst other candidates. Sure the resume doesn't look the *most* professional, but for a hiring manager to not even reach out? Come on.",NA
"994",993,2,"I put the shortened URLs for quick access to my projects. But you are right, it does seem odd on a resume. I have removed them.",NA
"995",994,2,"I applied to Deloitte, they said no thank you. 

I will try the other organization you listed. Thank you!",NA
"996",995,2,"Is it because of my name? I will try it see what kind of response I get. thank you",NA
"997",996,2,"I used tensorflow for this project that's why I thought it would be a good project to highlight. But i agree with you, its not expressive, i will remove it. Thank you!",NA
"998",997,2,"I believe friends referral is the option. I found an uncle who works at the Amazon Cloud department. \*fingers crossed\* 

Thanks you :)",NA
"999",998,2,"Good luck to you too bro :)

"" it's **darkest before** the **dawn** "" that's my hymn now  lol",NA
"1000",999,2,"I agree, I was surprised too given OP’s education.",NA
"1001",1000,3,"Amazon and Google have canceled onsite interviews but they still do online interviews. My friend has one with Amazon next week. Other friend did one with Google few days ago (online).",NA
"1002",1001,3,"Yes. One role which I was interviewing for has been put on hold due to financial uncertainties.",NA
"1003",1002,3,"The company I work for is still hiring, but not starting people.",NA
"1004",1003,3,"Yep. Google, Facebook and amazon have suspended all interviews until further notice",NA
"1005",1004,3,"Yep. No in-person client meetings or interviews.",NA
"1006",1005,3,"So making offers but delaying the actual start date?",NA
"1007",1006,3,"Don't think this is true. They're doing virtual interviews from what I've read.",NA
"1008",1007,3,"What is your source on this?",NA
"1009",1008,3,"Pretty much.  

Client won’t start anyone right now, even though the positions are available and will be available.",NA
"1010",1009,7," Hi, I'm a Leadership Development Intern at a startup called RippleMatch. It is a startup that matches students with internships based on their resumes and interests. The internships that are matched are from all types of fields and there are also lots of helpful articles for interviewing, resumes, etc. It's really great because it takes about 15 minutes to sign up and 5 minutes to apply to matches unlike the 40 minutes you would spend on job boards and tweaking your cover letter. Students have been directly connected to prestigious opportunities and fast-tracking them to first-round interviews with companies such as BlackRock, Lyft, Estee Lauder, Dell, Pfizer, GAP and many more. I really enjoy my internship and Ill be using the service later on as well. If you're interested you can sign up here: [https://ripplematch.com/index?r=hQp9gw](https://ripplematch.com/index?r=hQp9gw)",NA
"1011",1010,7,"Your linkedin url doesn't work. I think it is missing an 's'",NA
"1012",1011,7,"Thank you! I’ll sign up.",NA
"1013",1012,7,"Yes. Sorry about that. I updated it.",NA
"1014",1013,11,"PM inbound.",NA
"1015",1014,11,"What’s clearance?",NA
"1016",1015,11,"I too would like to know",NA
"1017",1016,11,"Security clearance.",NA
"1018",1017,11,"US citizenship required, you mean?",NA
"1019",1018,11,"US citizenship is required as part of the security clearance.

Picture a very in-depth background check.",NA
"1020",1019,11,"I see, thanks!",NA
"1021",1020,14,"I'm working remote as a data scientist right now. Just recently graduated as well. It can get lonely, but it's not bad at all as long as you keep an open channel of communication with your peers. Do visit the office every few months or so to touch base in person if you can. 

Separate your work place from sleeping place. Don’t be near a television. Get ready in the morning like you’d get ready for work in an office. When it’s 5 or 6, shut everything down like you would if you were in an office. 

There’s lots of pros. I get to wear whatever I want, even my pajamas. I get to eat healthier home food, use my own private bathroom, and there is no commuting so I don’t waste time dealing with traffic. It’s worth it.",NA
"1022",1021,14,"Off topic but I’m curious how did you get a remote ds role. Did It required a lot of exp?

2nd Q - Was it remote at first? Or did the position become remote once trust is established?",NA
"1023",1022,14,"I have a question. In remote work, generally, is it allowed to work out of the home. Like in some coffee shops or any other study place to get some new surroundings.",NA
"1024",1023,14,"They just allowed remote. I had a couple of internships under my belt. It was remote from the start.",NA
"1025",1024,14,"I had the same case with remote research assistant experience with data science role in a bank before getting this offer.",NA
"1026",1025,14,"Yes, but if you work with sensitive data you should use a VPN.",NA
"1027",1026,16,"Would they consider application outside from the U.S?",NA
"1028",1027,16,"It seems yes, but the benefits and perks might be different then from those of U.S. based employees.",NA
"1029",1028,17,"its going to be all right just go super saiyan! bro",NA
"1030",1029,18,"Check this post : [https://www.reddit.com/r/DataScienceJobs/comments/fgcc37/8\_jobs\_hiring\_now/](https://www.reddit.com/r/DataScienceJobs/comments/fgcc37/8_jobs_hiring_now/)  

All the best mate :)",NA
"1031",1030,18,"guru kannadiga na",NA
"1032",1031,18,"DM me. I can refer you.",NA
"1033",1032,18,"Looks decent. Hope you’ll have more luck in getting the calls. All the best",NA
"1034",1033,18,"Oh awesome! Thank you",NA
"1035",1034,18,"Nope. From Andhra",NA
"1036",1035,18,"Thank you. I’m messaging you now",NA
"1037",1036,18,"then Rcb fan??",NA
"1038",1037,19,"Hi guys, some people have questions about our recruiting techniques, links, etc,... here are the answers to some of the questions people have:

1. [You are not a recruiter / you are making affiliate harvesting](https://www.reddit.com/user/kingstar11/comments/exou8p/what_exactly_do_you_do_why_are_you_posting_these/)
2. [What value do you add to all this compares to the old recruiting style?](https://www.reddit.com/user/kingstar11/comments/exowew/what_value_do_you_add_to_the_process)
3. [Why don’t you stop posting here and go?](https://www.reddit.com/user/kingstar11/comments/exoy3i/why_dont_you_stop_posting_here_and_go/)

If you have any other questions about the process or the above job offers I will be happy to help you, regards.",NA
"1039",1038,19,"Sorry guys there are just 4 opportunities, there was a mistake and some of them were duplicated, regards.",NA
"1040",1039,19,"FYI the Lockheed Martin job is in Grand Prairie, west of Dallas. Unless you have other information than what is on the job posting, I’d adjust that. Otherwise, I appreciate the post and format.",NA
"1041",1040,19,"Can you post the jobs in India related to analytics....",NA
"1042",1041,20,"Hey!  


First off, I know the job search can be frustrating, keep your chin up.   


1. Bachelor's in technology -> BS, Masters of Science -> MS
2. You list your top skill as Python, but most of your experience doesn't seem Python related. Either switch your skill order, or add more of your Python experience. 
3. You have really great impact statements
4. I would, personally, remove every non-indented heading for your job postings. They don't add much imo, and were confusing at first. You don't have to, but that's what I would do. Alternatively, you could simply indent them to keep them in your resume.
5. Opinions are split about this advice, I would suggest changing the job titles. Even if your official title was ""Data Analyst II"", write it as ""Data Scientist"" or whatever role you're targeting. Do this across the board to show that your experience is relevant. 
6. It is a bit odd listing a string of ML algorithms, I probably would drop that line.
7. I would consider, for the impact statements, try rewriting the ones that talk about data size (e.g., 600+ rows, 500 records). This is not large data and these numbers (out of context) aren't impressive. However, I bet the results are impressive! Think about the final impact of presenting your findings to management, or the results from the trend analysis forecasting bullet point. 

What sorts of positions have you been applying to? Your resume seems well targeted to entry level positions (0 - 2 yrs exp). What size companies are you looking into? Do you have any contacts at places you worked previously? Do you have a web presence (github, linkedin, twitter, blog)?  


In conclusion, it seems like you have good experience that could be targeted slightly better to specific roles. I think you'll find something, best of luck :)",NA
"1043",1042,20,"This is overwhelming to read. As someone who screens resumes (and also works in analytics) I’d skip over it if I was tired, even if you may be the best fit. 

It shows me you don’t know how to edit down to the most critical stuff. 

White space is good. Sans serif fonts are good. You can use the interview to elaborate. Respect the resume reviewers time, and edit, edit, edit.

If you want to send me a google docs version, I’d be happy to take a whack at it.",NA
"1044",1043,20,"Wtf how are you not getting a job with experience like this",NA
"1045",1044,20,"Put it in a more visually appealing format, most of these jobs are filtered first through HR and/or an agency, they don't want to read that. 

In this case less is more",NA
"1046",1045,20,"1) Is the pdf you're sending to people an image?  Bots need it to be text to crawl it.

2) The title/header is cut off.  Is your email address and phone number and current city you live in (unless you're willing to relocate) listed clearly?

3) I don't care about your education, I care about your previous [work] experience.  I've hired people and I'd skip your resume just due to the ordering of importance.  Reorder your resume from top to bottom:  work experience, technical skills, education, projects.

4) I see an analyst in your resume, not a data scientist.  When I hire a data scientist I'm looking for very specific technical skills, usually NLP or image classification, though there are other data science specialties as well.

You get to choose what kind of data science job you want to be working.  Which ever kind it is, make it obvious on your resume.  Make sure those specific skills stand out in the bullets of your previous work experience and in the technical skills section and hopefully you have projects that show it too.  Even if you fudge a little, it's better to have NLP (for example) in a previous job bullet than Excel.  Excel is an analyst's tool not a data scientist's so mentioning it is neutral at best if not a negative.

edit:  Another way to show what kind of work you want to be doing and advertise yourself is to add a Job Objectives and/or Professional Summary at the top of your resume.  Write it as if you've already got the job you want to get.",NA
"1047",1046,20,"where did you learn data science in India?",NA
"1048",1047,20,"Your resume, experience, and education are not the issue here.

UF is a good university, but the work visa is going to be your biggest hurdle. If that's the case, your resume it's probably getting dumped from the online application before it even goes to the HR people. You gotta get out there, network, meet new people, and it will work out.",NA
"1049",1048,20,"I would include a short summary at the top e.g. ""Recently completed an MSc with a strong data science content and have experience in data analytics roles. Currently looking for an entry level data science role."". Then focus the CV on repeating exactly that......e.g. show you have some experience but not too much. For intern roles allow 1 line each (maybe 2 if it is really interesting). White space is good. Imagine you had 200 of these to read through. You have 30 seconds to make a decision. A wall of text is a reject. Keep it simple.

Are you looking in the US? Do you have right to work? If so then say so on the CV. If not then that is one reason you will struggle to get interviews. It is not personal just the way it is.",NA
"1050",1049,20,"Hey!! Thank you so much for your reply. I really appreciate the effort you put in to provide me your insights, it's really valuable to me.

My resume's previous iterations had no data sizes, but pple asking me to add more numbers and results, which is why added them. I think I should focus more on the result numbers.

Honestly, I was a little hesitant to add the string of ML algorithms but again since I wasn't getting any interview calls I thought i was missing these ML keywords. Maybe I should think of some other way to add them on my resume.

I have been trying hard to land something, so I apply to data analyst, business analyst, Jr.data scientist, data engineer positions.
As for company size, I don't have anything particular in my mind. I would be happy to get a platform where I can really work on large data sets and learn more in practice.
I use LinkedIn to apply for jobs too, I am really not very active in other social media platforms.

Again, Thanks a lot for your reply and I really appreciate your time!!!",NA
"1051",1050,20,"Tell me more about 5. I work for a small company with no understanding of data and have the title of ""data architect"" but it is not at all what I do.",NA
"1052",1051,20,"Yea sure, I can send you one. How do you want me to forward it you?",NA
"1053",1052,20,"haha!! lol",NA
"1054",1053,20,"Hey Thanks for your response. But could you tell me what you mean by a more visually appealing format (an example/sample), that'd be great.

thanks!",NA
"1055",1054,20,"Well thank you for your honest view. I will work on getting it clearer.
I send pdf's, I wasn't able to upload one here, hence the image.",NA
"1056",1055,20,"yea that's a very big issue for me. I keep trying though. I've tried reaching out to HR recruiters and other employees. Some do reply and are very positive in their approach but some don't reply.
I hope get a positive reply one day.
Thanks!",NA
"1057",1056,20,"Hey! Thank you for your response. The summary suggestion is a good one, I'll try incorporating it on my CV.",NA
"1058",1057,20,"No problem. Keep grinding, best of luck.",NA
"1059",1058,20,"I usually think about it as a ""cluster"". There are certain roles where the title is interchangeable.

The point of your resume is to put your best foot forward. This means you should have job titles that line up with the job application. 

Don't lie, but change the title around so it matches. 

In your case, you're probably doing what most would consider ""data engineering"", so if you apply for those roles in the future use that as your title instead of ""data architect"".",NA
"1060",1059,20,"Agree with him! Visually-appealing like adding a professional summary, be concise, quantitative information on resume, highlighting the important information, using clear section headings, creating white-space, using common fonts and more.

Besides, job hunting for data analysis / data science role can be a challenging task for many people, yet we all need to go through that process in order to build a career. Let me share some actionable tips on how to get a job as a data scientist / data analyst:

[https://towardsdatascience.com/how-to-get-a-job-as-a-data-scientist-7-actionable-tips-c8ec166e56bf](https://towardsdatascience.com/how-to-get-a-job-as-a-data-scientist-7-actionable-tips-c8ec166e56bf)",NA
"1061",1060,20,"This book has some good templates for a simple, straightforward resume with sufficient white space.

https://www.amazon.com/Guerrilla-Marketing-Job-Hunters-3-0/dp/1118019091",NA
"1062",1061,20,"Try novoresume.com

They a bunch of free templates that will help organize and make it more palatable for the reader",NA
"1063",1062,20,"Some pdfs are a slide show of images and some are a series of text.  You can tell by opening your pdf and doing ctrl+a to see if it is text or an image.",NA
"1064",1063,20,"Most companies I work with currently refuse to hire those who need visas.  Those that do use visa staff, bring over people that have tenure as overseas employees.  Supposedly there is too much uncertainty and work involved with getting someone a Visa right now to risk it on someone who isn't a known producer.

I would say keep at it, it will be tough but it will take lots of effort.

Some of your skills seem dated but that is just my opinion. (Access, vlookup with excel, etc...) The algorithms seem fairly sparse.  I would remove them entirely and incorporate them into the descriptions of what you have done.  Otherwise it feels like you are just throwing some basic algorithms out and I have no idea if you understand how they work at a deep level or just copied them off a DS website.",NA
"1065",1064,20,"Yea, the visa situation is a hindrance. But, I'll keep at it.
Thank you for your suggestions, I appreciate it!",NA
"1066",1065,23,"Hi, do you sponsor H1B visa?",NA
"1067",1066,23,"IBM will not be providing visa sponsorship for this position now or in the future.  Therefore, in order to be considered for this position, you must have the ability to work without a need for current or future visa sponsorship.",NA
"1068",1067,33,"You're in luck! Because [SharpestMinds.com](https://SharpestMinds.com) exists. All you'll have to offer is 10% of Year 1 salary after hire. I'm about to start it myself. Lmk how it goes!",NA
"1069",1068,33,"Thanks I’ll check it out",NA
"1070",1069,33,"I’m located in Australia. I think this site is helpful for people in Canada & USA",NA
"1071",1070,40,"Why is data scientist in air-quotes?",NA
"1072",1071,41,"👀👀👀",NA
"1073",1072,42,"I was just wondering why would recruiter in the UK/US give you sponsorship for your VISA when they can find an equally talented candidate if not more, alternatively you could also apply for a PR maybe... anyway best of luck and don't hesitate correcting me if I am wrong",NA
"1074",1073,42,"One thing for sure is a 3 page resume won’t help. 1 page is best.",NA
"1075",1074,42,"Forget about the UK and USA man, they are closing down faster than \*insert whitty scenario\*. Even if they needed you desperately, their politics are such that they wouldnt let you in regardless. Take your talent elsewhere.",NA
"1076",1075,42,"\+Canada",NA
"1077",1076,42,"thanks for your reply :). from my side I'm trying don't know whether i'll get visa or not :)",NA
"1078",1077,42,"I tried to make it 1 page but having a 6yrs exp in data science i think 3page is fine :)",NA
"1079",1078,42,"That’s definitely not true of the UK. Companies can and do sponsor talent needing VISAs but they (as I understand) can’t exceed a certain quota. New rules are actually making it very easy for “scientists”  to move to the UK. I’m not sure data science will be necessarily covered in that unless you are applying to pharma/ research.",NA
"1080",1079,42,"How has your job response been?",NA
"1081",1080,42,"You wouldn't know unless youve gone through the immigration system there in the UK, and I have - and its understandable because you guys have had unmittigated immigration from the EU for decades so you had to stem the flow of people somehow, and unfortunately your government chose to stop highly skilled visas (because thats all they could legally do). 

Post Brexit you will see a return to a normalized points based system where people such as data scientists from non-EU countries are allowed to settle in the UK. 

Rahul just work on your portfolio and wait until Jan 2021 when it will all go back to normal. 

Points based system = if your employable, welcome in!",NA
"1082",1081,42,"Yes that's true can you please suggest me any job portal where I can find UK based sponsorship job :)",NA
"1083",1082,42,"it is not good :'( but still from my side i'm trying",NA
"1084",1083,42,"You can try monster.co.uk, Reed.co.uk but of course LinkedIn will also have lots",NA
"1085",1084,42,"Try my advice
With the resume :)",NA
"1086",1085,42,"registered on all websites but not getting any proper response from applied IT firm",NA
"1087",1086,42,"ok I'll make 1page resume",NA
"1088",1087,53,"Prepare for each and every word written on your resume. I've been interviewed for a Graduate Data Science Intern Role lately. They could ask anything under the sun, but prepare your resume thoroughly!",NA
"1089",1088,58,"Fuck that. Next.",NA
"1090",1089,58,"That's a hard pass for me.",NA
"1091",1090,58,"[deleted]",NA
"1092",1091,58,"Swipe left.",NA
"1093",1092,58,"It depends, but the one thing I would say is don't subscribe (if at all) until they hire you. Makes no sense for you to subscribe to their product as a condition of employment.",NA
"1094",1093,58,"That’s not right. Sounds like MLM meets startup. Every time I’ve purchased a subscription or paper an interviewer wrote and mentioned it I’ve gotten a refund (even for a $10 app).",NA
"1095",1094,58,"red flag",NA
"1096",1095,58,"Thanks. Fortunately I have another two offers on the table.",NA
"1097",1096,58,"Thanks for that!",NA
"1098",1097,58,"Thanks. Yeah it sort of forces my hand. The salary they are offering is much lower than market rate as well, even for a startup.",NA
"1099",1098,58,"[deleted]",NA
"1100",1099,58,"Because I think I could negotiate higher. The opportunity is also quite unique in the sort of work I will be doing - very much pure R&D rather than production roles which is more what I've typically been getting offered. However, as many other redditors have confirmed here, I think this is the last straw.",NA
"1101",1100,58,"[deleted]",NA
"1102",1101,58,"That's pretty awesome man. I only have a BSc in Mathematics combined with 2 years as a Data Scientist and 4 years before that as a Statistician. For the reason you mentioned I struggle to get R&D roles because I'm competing with MSc and PhDs. I've been offered £65k + 10% bonus (you have to bear in mind that salaries are lower in the UK) as my best career offer so far. Any tips for pushing my career to the next level? The irony is I'm better at R&D than Engineering / Productionising but I can do both and get offered for the latter more.",NA
"1103",1102,58,"[deleted]",NA
"1104",1103,58,"My experience is mainly in prototyping and bringing to prod an in-house NLP model for analysing user comments for my last company and a Random Forest Ensemble for predicting post-event share price performance. It was IP of the company so I couldn't publish it I don't think. Perhaps I have underestimated the level of R&D required to be taken seriously.

To be honest, you suggesting that I need to go back to study is sort of what I was dreading to hear, even if there is truth to it.",NA
"1105",1104,58,"[deleted]",NA
"1106",1105,58,"It wasn't just Engineering, I preprocessed, cleaned, imputed, did some feature engineering, tested various models and iterated over different hyperparameters in fully documented jupyter notebooks justifying each step. Then also refactored and brought to prod level with full testing and DevOps processes setup. Apologies you're right I should've been clearer - it was simply a random forest (multiple decision trees).

That's sounds like a cool project - was the main purpose in faster diagnosis? Perhaps it was this paper? https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6328156/

I think the first option sounds positive - I think the employer I'm most keen to choose could offer this. They have provisioned this in the past for other Data Scientists.

Thanks again for your thoughts. I can't even imagine being offered a salary anywhere near $200k!",NA
"1107",1106,61,"You have a masters degree and still having difficulty getting a job? Damn that’s scary. I’m starting a masters in statistics later this year with the hopes that it will get me a job in data science. I hope to god that getting a masters degree isn’t a waste of time. Did your school offer any kind of career services?",NA
"1108",1107,61,"[deleted]",NA
"1109",1108,61,"Yeah my school helps out a ton. They offer resume advice, career fairs and even job postings geared towards the students/alumni. Even with all that nothing. Part of me wants to say that it’s because most jobs are hiring during the summer since that’s when a lot of graduates are looking for jobs but who knows.",NA
"1110",1109,61,"I do not have a clearance, but I have the ability to do so. 

I have done a data science internship for a government contractor, a statistician internship for a clinical research company, as well as worked at my schools cyber security office",NA
"1111",1110,91,"When I studied for my master's degree in machine learning, I was earning 150+ USD / hour as a part-time data scientist consultant  ¯\\\_(ツ)\_/¯

What exactly did you have in mind in terms of salary range?",NA
"1112",1111,91,"Most DS and ML Engineer jobs ads are for entry level candidates. 

A recent example:  
Recruiter: Hey why do you demand so much salary. You don't have any DS/ML job XP!  
Me: Nope, dear recruiter I have over 10 years relevant job XP. ""Data Scientist"" is just the new job title buzzword what is used to describe someone who knows math on a computer.

The sad truth is that I can build a ML model that can actually read, process and make prediction on application documents to do a better job than most recruiters.",NA
"1113",1112,91,"At least 130k USD/year.  When I speak with recruiters I'm usually in the range 125k-135k for Senior .NET Full Stack roles depending on benefits & PTO.  Having the salary be less than that for a Data Scientist/Machine Learning role that arguably is far more complicated & difficult than full stack is surprising to me.

When I look at the Glassdoor and ZipRecruiter ranges online the salaries all seem much lower than 130k/year",NA
"1114",1113,93,"For your bullets under SAS institute try to better highlight accomplishments rather than just things you did.

What business value did you bring with your successful work?",NA
"1115",1114,93,"Not sure an entry level data science job would suit your skillset. Data analyst is what you should be focussing on.",NA
"1116",1115,93,"Definitely put that you are a US citizen are the top as managers of small companies that can't sponsor H1B might see your Philippines experience and write you off :(",NA
"1117",1116,93,"Thanks for the advice! 😊",NA
"1118",1117,93,"I realized that so I've been applying for many data analyst roles as well with no luck :(( I'm perfectly fine with starting from the bottom and I know I'll do the work and learn but that requires a company to give me a chance :((",NA
"1119",1118,93,"I was thinking that's why I'm not getting any attention too :(( I can't think of a non-awkward way to put it on there thooo",NA
"1120",1119,93,"Keep working on your skills - just get into the workforce - doesn't have to be in the data space. Eventually you can move towards the data space once you build general experience up.",NA
"1121",1120,94,"Will you have the ability to change roles if you have the capacity to prototype?

If you do not take the position what will you do in the meantime to make yourself a more attractive candidate for prototyping roles?",NA
"1122",1121,94,"Thanks very much for your reply.

It's certainly possible as they are a large company.  

I will do more GitHub projects and Kaggle competitions (already do).",NA
"1123",1122,98,"How many years of experience do you want ? Also how about international candidates who would require visa help?",NA
"1124",1123,98,"Hi there, years are less important than the level of skill, so we're flexible on that. I'm from Canada so I have less direct experience with visas, but know that others have successfully gotten them and that our talent team would be able to help.",NA
"1125",1124,0,"A paper titled  [*Composition-based Multi-relational Graph Convolutional Networks (Vashishth et al., ICLR 2020)*](https://openreview.net/pdf?id=BylA_C4tPr).

The basic idea is that most of the GNN-based methods that are popular are used on undirected and simple graph structures, whereas multi-relational and directed graphs are actually what's important in the real world.

CompGCN (the model proposed in the paper) embeds both the nodes and relations in knowledge graphs in order to incorporate this information. The ""composition"" comes in order to effectively and jointly learn the embeddings for entities and relations. Previous models for multi-relational graphs are limited to only learning entities, due to computational complexity reasons.

These composition operations are basically just taking the two representations/embeddings of nodes and relations and performing arithmetic or passing them through a neural network.",NA
"1126",1125,0,"https://graphdeeplearning.github.io/post/transformers-are-gnns/

Although a good blog post, I think the title is a bit misleading. I think a more accurate title would be ""Multi-headed Self Attention"" (ie: the primary component in transformers) is a kind of ""graph neural network step"", and that the full Transformer architecture is a composition of graph neural networks steps in a somewhat strange way.

In particular, although individually the encoder can be viewed as several graph neural network steps, between each step there's a bipartite graph being constructed between the encoder and the decoder, upon which a GNN step is taken.

Thinking of it in this way really made things click for me for the entire transformers architecture, while this blog post only clarified what each individual self attention layer is doing.",NA
"1127",1126,0,"A paper titled: [Time-aware Large Kernel Convolutions](https://arxiv.org/abs/2002.03184).

The paper suggests an interesting way of modeling sequences without using attention. Specifically, the authors suggest using an adaptive convolution method that instead of learning the kernel weights, learns the size of the kernel. Specifically, the size of the kernel is generated for each input sentence, i.e. each token has its own kernel size. Also, due to the simplicity of the method, the process is faster and has linear time complexity O(n) (Transformers have O(n^2 )).",NA
"1128",1127,0,"Our group is trying to figure out the pros and cons of neural ordinary differential equations. Mostly trying to find learning applications where it fails to compete against the standard recursive structure. We're looking for properties that the target function must follow to be able to be learnt satisfactorily.",NA
"1129",1128,0,"[Active Learning Literature Survey](https://minds.wisconsin.edu/bitstream/handle/1793/60660/TR1648.pdf?sequence=1&isAllowed=y)

Old paper on Active Learning. This is an interesting point in machine learning that I want to dig in deeper.  
I often heard this term and I never forget that data is ML gold.

3 main essentials technics that allow us to have a better training with superviser/semi-supervised learning.",NA
"1130",1129,0,"[UMAP: Uniform ManifoldApproximation and Projection for Dimension Reduction](https://arxiv.org/pdf/1802.03426.pdf).  


It's not as hard as it looks if you read it in proper order.  
In short UMAP is like tSNE but uses several math concepts:  


\- *Riemannian metric* \- it is addressing tSNE's crowding problem. Riemannian metric is something that is used to define distances on manifolds from local information. For each point we estimate mean distance between it and its nearest neighbors, and then use it for computing probability of two points being neighbors (this correspons to uniform part in UMAP, as it makes manifold look like if samples were actually sampled uniformly)  
\- Fuzzy simplicial complexes - these are actually fuzzy graphs (as graphs are 1d simplicial complexes). Fuzzy operations are used to merge local structures into global one.  


Comparing this to tSNE:

* UMAP uses different probabilities on distances in embedding and projected space
* it uses cross entropy loss instead of KL divergence
* initialization is using decomposition of Laplacian of neighborhood graph instead of just random points
* optimization uses stochastic gradient descent with negative sampling for nonadjacent points (comparing to GD for tSNE)
* implementation uses library for fast approximate kNN

&#x200B;

Putting all these together makes UMAP faster and more scalable than tSNE. For other advantages see excellent Nikolay Oskolkov's [posts on medium](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668?source=post_page-----5372608dca74----------------------).  


If you're interested in using UMAP I also encourage you to check out NVidia's rapids cuML library. It has both UMAP and tSNE implementations that can run on GPU (although original authors UMAP code is also pretty fast, he implemented it using numba).",NA
"1131",1130,0,"The [AutoML-Zero](https://arxiv.org/abs/2003.03384) paper. I find the concept of evolving machine learning algorithms fascinating. After all, it is evolution that produced the human brain so I'm inclined to think that evolutionary computation is due for a renaissance, just like neural networks in the 2000s. I've read quite a few papers on evolution in AutoML  now. I still don't understand why they insist on using such simple evolutionary algorithms when dealing with neural networks. I know that crossover is hard to define for graph-like structures, but that doesn't mean that we should ignore it completely and simply head for a mutation-only EA.",NA
"1132",1131,0,"Ok, that's not a bigass arxiv, just a well-written wiki page on nerdy history of science: [https://en.wikipedia.org/wiki/AI\_winter](https://en.wikipedia.org/wiki/AI_winter). Surprisingly entertaining (e.g. ""-the spirit is willing but the flesh is weak- translated back and forth with Russian, it became -the vodka is good but the meat is rotten-"") and informative (e.g. ""implied that many of AI's most successful algorithms would grind to a  halt on real world problems and were only suitable for solving toy  versions"").",NA
"1133",1132,0,"Mhsa plus positional encodings is equal to one conv layer according to some other blog.",NA
"1134",1133,0,"The author gave a retrospective talk titled [Bullshit that I and others have said about Neural ODEs](https://youtu.be/YZ-_E7A3V2w) that may be helpful for you.",NA
"1135",1134,0,"Sounds interesting I have been trying to understand Neural ODE for sometime too. If possible can we have a chat?",NA
"1136",1135,0,"Have you read any other papers on EAs that you found particularly insightful or interesting?",NA
"1137",1136,0,"> big ass-arxiv

***

^(Bleep-bloop, I'm a bot. This comment was inspired by )^[xkcd#37](https://xkcd.com/37)",NA
"1138",1137,0,"Do you have a link?",NA
"1139",1138,0,"I went through that talk already, it's an eye opener. Funny thing, we were already working on neural ODEs for about a month or so, before we found this talk. We used to think we were ""familiar"" with them and the talk got us going again.",NA
"1140",1139,0,"Yeah, why not. Text me on Reddit itself.",NA
"1141",1140,0,"Yes. Even though it’s not SOTA and it’s a bit dated now, the ideas behind [HyperNEAT](https://ieeexplore.ieee.org/document/6792316)  always blow my mind.",NA
"1142",1141,1,"[Learning Discrete Latent Structure](https://duvenaud.github.io/learn-discrete/)",NA
"1143",1142,1," PhD-level course  [EP3260: Fundamentals of Machine Learning Over Networks](https://sites.google.com/view/mlons/home)",NA
"1144",1143,1,"[CS 287: Advanced Robotics, Fall 2019](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/) with Pieter Abbeel is great! It covers a lot of stuff: basic RL, control theory, motion planning, particle filtering, all the way up to state-of-the-art RL algorithms for robotics.",NA
"1145",1144,1,"CMU's [Probablistic Graphical Models](https://m.youtube.com/channel/UCim-E6bNz7lUyKZwhgN6S1A/featured) by Professor Eric Xing.",NA
"1146",1145,1,"CMU 11-747 (Neural Networks for Natural Language Processing) should definitely go in as it's more advanced than CS224n and is also a high-quality course.
http://phontron.com/class/nn4nlp2020",NA
"1147",1146,1,"[CS 330: Deep Multi-Task and Meta Learning](https://cs330.stanford.edu/)",NA
"1148",1147,1,"Why would you say that cs224n is advanced but cs231n, it’s computer vision counterpart is not advanced.

I actually think both courses provide a comprehensive coverage of models used in nlp and CV.",NA
"1149",1148,1,"https://sites.google.com/view/berkeley-cs294-158-sp20/home",NA
"1150",1149,1,"Could anyone recommend a good course on Speech processing?",NA
"1151",1150,1,"Yes. That makes sense. If there a thread for intermediate level courses, it should be added there. I think [CMU 36-702](https://www.youtube.com/playlist?list=PLTB9VQq8WiaCBK2XrtYn5t9uuPdsNm7YE) is advanced enough for this list though",NA
"1152",1151,1,"Deep RL:

[https://www.youtube.com/playlist?list=PLkFD6\_40KJIwhWJpGazJ9VSj9CFMkb79A](https://www.youtube.com/playlist?list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A)",NA
"1153",1152,1,"[CS224W](http://web.stanford.edu/class/cs224w/) is a good one for ML on graphs.",NA
"1154",1153,1,"Another statistical learning theory (taught by PhD from Stanford, now prof at U of T):

[Statistical Learning Theory] (https://erdogdu.github.io/csc2532/)",NA
"1155",1154,1,"How about this?

[https://github.com/avehtari/BDA\_course\_Aalto](https://github.com/avehtari/BDA_course_Aalto)

For me, it's like a hidden gem :-)",NA
"1156",1155,1,"I am taking an Advanced NLP course 6.864 
at MIT right now.  We don’t have videos yet, but I’ll post here when/if we do,",NA
"1157",1156,1,"Topics in Robust and Deployable ML (6.S979) has a variety of slides and notes but no assignments.

https://people.csail.mit.edu/madry/6.S979/",NA
"1158",1157,1,"Would you consider [CS224n](http://web.stanford.edu/class/cs224n/)  an advanced course?",NA
"1159",1158,1,"Anybody have any suggestions for courses on theory or application of recommendation systems?",NA
"1160",1159,1,"Perhaps Emma Brunskill's [CS 234](https://web.stanford.edu/class/cs234).",NA
"1161",1160,1,"Advanced PhD-level topics course, notes (180pg), + presentations:  [https://github.com/dobriban/Topics-in-deep-learning](https://github.com/dobriban/Topics-in-deep-learning) 

Covers advanced topics such as adversarial examples, fairness, graph NNs, modern theory (e.g., neural tangent kernels), applications to chemistry, visual Q+A, etc.",NA
"1162",1161,1,"Anyone knows about any courses on time series forecasting using ML or DL?",NA
"1163",1162,1,"**Topics: Bandits, RL and Deep RL**

[https://rlss.inria.fr/program/](https://rlss.inria.fr/program/)

Reinforcement Learning Summer School in Lille, France (July 2019).

Not video recorded but slides in the timetable URLs.",NA
"1164",1163,1,"These are nice but very problem specific. Maybe have ""specialty"" category?",NA
"1165",1164,1,"Does the Stats 385 course have recorded lectures?",NA
"1166",1165,1,"How did you view the Convex Optimization I - EE364a videos?  When I click on the video link it takes me to a stanford log in page.",NA
"1167",1166,1,"RemindMe! 1 day",NA
"1168",1167,1,"RemindMe! 14 days",NA
"1169",1168,1,"Great initiative, thank you for doing this.",NA
"1170",1169,1,"RemindMe! 30 days",NA
"1171",1170,1,"Is there any Causal stuff that can be added to this list?",NA
"1172",1171,1,"I'd love to see something on time series as well! (Classical statistical inference as well as ML). Thanks!!!",NA
"1173",1172,1,"This is great, thanks!",NA
"1174",1173,1,"Does learning ML on udemy Is better.....?",NA
"1175",1174,1,"ML Theory ->

1.  [Understanding Machine Learning - Shai Ben-David](https://www.youtube.com/playlist?list=PLFze15KrfxbH8SE4FgOHpMSY1h5HiRLMm) 
2.  [CORNELL CS4780 ""Machine Learning for Intelligent Systems""](https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS) 
3.  [Machine Learning Course - CS 156](https://www.youtube.com/playlist?list=PLD63A284B7615313A) 

NLP ->

1.  [Stanford CS224U: Natural Language Understanding | Spring 2019](https://www.youtube.com/watch?v=tZ_Jrc_nRJY&list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20) 

Others:

1.  [Computational Linear Algebra](https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY)",NA
"1176",1175,1," RemindMe! 10 days",NA
"1177",1176,1,"No one has mentioned distributed systems for ML, which I feel like is very important. There are good survey papers for it, but any actual courses? Currently playing with Ray from Berkeley and took an interest in ML systems study.",NA
"1178",1177,1,"thANKS",NA
"1179",1178,1," 

RemindMe! 30 days",NA
"1180",1179,1,"Maybe you can add  [Topics course Mathematics of Deep Learning](https://github.com/joanbruna/MathsDL-spring19) offered by  Joan Bruna.",NA
"1181",1180,1,"Your courses are also very much in one direction, mostly. If we do this, we should have categories (""imagery"", ""video"", ""NLP"", etc) and clearly sort the courses.",NA
"1182",1181,1,"CS224n is advanced towards the end, hence should be added to list.",NA
"1183",1182,1,"Is fastai's  [Deep learning from the foundations](https://course.fast.ai/part2) considered to be an advanced one?  I know that its taught as a part of a master's course but i was wondering how hard it really is",NA
"1184",1183,1,"RemindMe! 45 days",NA
"1185",1184,1,"RemindMe! 30 days",NA
"1186",1185,1,"RemindMe! 8 days",NA
"1187",1186,1,"That was my topics course last year - this year it was [Learning to Search](https://duvenaud.github.io/learn-discrete/).

You might also like Roger Grosse's topics course on [Bayesian neural networks](https://csc2541-f17.github.io/), it also has presenter slides.

We didn't record any lectures, both to avoid putting pressure on the student presenters, and so that we could freely criticize the papers being discussed.",NA
"1188",1187,1,"Now that's what I'm talking about. Read some of his works for Geometric DL, happy to see the course.

edit: Which category should I put it in though? It touches all - ML/DL/RL",NA
"1189",1188,1,"This course looks great but I don't see the lectures anywhere. Do you have a link?

Personally I don't get much out of the slides without the talk that went with them.",NA
"1190",1189,1,"Love me some PGM’s. Probably my favorite class in college.",NA
"1191",1190,1,"tks",NA
"1192",1191,1,"I am also interested in this",NA
"1193",1192,1,"That'll be great. Thanks.",NA
"1194",1193,1,"Good news! Waiting to see video lectures.",NA
"1195",1194,1,"Having watched most of the lectures I would say it's intermediate to advanced level. Depends on your level of knowledge in NLP, linguistics and DL. If you have good DL knowledge you can probably skip some parts and focus on the NLP applications. If you are familiar with classic NLP you can skip those parts and dive into the DL focused parts.",NA
"1196",1195,1,"I'm not very sure. I looked at the contents and more than half of it is introductory stuff that is usually an undergrad/master's course. But their last 5-6 lectures seem quite nice and latest. 

I'll add it for now unless I get some objections :)

I just don't want the list to be inundated with intro-level stuff.",NA
"1197",1196,1,"Makes sense. I'll do that if we have a good number of suggestions.

I put these up because I work with 3D data and was aware of these but courses from any application domain are welcome. :)",NA
"1198",1197,1,"They have videos for 2017 run of the course that I just linked above. Not sure about latest one.",NA
"1199",1198,1,"Ahh! I used to access that through cvx101 link which takes you to lagunita, stanford's MOOC platform, but they are phasing it out currently and moving to edX completely. Meanwhile, you can access the videos with link I updated just now",NA
"1200",1199,1,"Judea Pearl’s Book of Why? Not a course per se but it’s a great read",NA
"1201",1200,1,"CS 6787 from Cornell is pretty good: https://www.cs.cornell.edu/courses/cs6787/2019fa/

From excellent Professor Chris De Sa.",NA
"1202",1201,1,"done. :)",NA
"1203",1202,1,"it's not hard",NA
"1204",1203,1,"[deleted]",NA
"1205",1204,1,"I will be messaging you in 1 month on [**2020-04-19 19:10:13 UTC**](http://www.wolframalpha.com/input/?i=2020-04-19%2019:10:13%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/fjkwq2m/?context=3)

[**14 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Ffdw0ax%2Fd_advanced_courses_update%2Ffjkwq2m%2F%5D%0A%0ARemindMe%21%202020-04-19%2019%3A10%3A13%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20fdw0ax)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",NA
"1206",1205,1,"I'm currently in the class, he's not a great lecturer imo. I get much more out of the homeworks tbh.",NA
"1207",1206,1,"Wrote an exam on it literally today, last written exam of my masters, too",NA
"1208",1207,1,"I think one more advanced course for NLP is the  CS 11-747 ( [Neural Networks](http://www.phontron.com/class/nn4nlp2020/#)  
[for NLP](http://www.phontron.com/class/nn4nlp2020/#) )  from CMU.  
I hope it may help.",NA
"1209",1208,1,"In the realm of NLP it's definitely introductory. The problem is whether NLP is an introductory task in ML/DL, and I think the answer is also increasingly ""yes"" in recent years.",NA
"1210",1209,1,"From my perspective I can add Meta-Learning focused ML resources.",NA
"1211",1210,1,"Thanks",NA
"1212",1211,1,"This looks wonderful. Thank you!",NA
"1213",1212,1,"A lot of the courses listed here are 2xx so basically second year undergraduate, though so I might fit in.
Then again, I’ve sat in on some grad level CS classes that were more intro than my undergrad ones we had to take for engineering.",NA
"1214",1213,1,"Same haha and agreed - wayyy too fast to digest anything. HW 2 extension ftw though 😎",NA
"1215",1214,1,"Hi, How good/rigor this course compares to Probabilistic graphical model of Koller (stanford) on coursera.",NA
"1216",1215,1,"Great. Comment them here or main thread and I'll add them to the post later.",NA
"1217",1216,1,"1. [Meta-Learning book](https://g.co/kgs/fPeynZ) (2nd edition on its way) 
2. [Meta-Learning tutorial](https://sites.google.com/view/icml19metalearning)


That's all I can add from my phone now. Will edit later.",NA
"1218",1217,2,"I agree but also in case somebody doesn't know:

www.sci-hub.tw for free papers",NA
"1219",1218,2,"All research should have free access. People dies from everything, not just COVID19.",NA
"1220",1219,2,"both humans and Al... who's Al, he sounds like an important guy",NA
"1221",1220,2,"Outside of papers, raw data is also weirdly guarded. Supposedly open GISAID in reality requires academic affiliation, few day verification period, and even after that they can ban you for any reason. Istvan Albert (PennU bioinformatician, bioinf folk should recognize him for biostars community) was banned from GISAID after describing, that they don't have any kind of bulk export and file consistency.",NA
"1222",1221,2,"good to see this amidst everything that is happening.",NA
"1223",1222,2,"Hopefully Science and Nature will follow too. Everybody wants to get published there and it's kind of a big deal for some reason.",NA
"1224",1223,2,"What we need are free, anonymized datasets. As a researcher I’ve been requesting datasets from authors that have published instances and so far haven’t heard back. Surging CORVID-19 testing with AI can’t be done democratically with proprietary access to datasets, that only invites vendor lock in and monetization with unverifiable  solutions.",NA
"1225",1224,2,"""Global officials call for..."" FFS ""Global officials"" took your tax money and gave it to the scientists to generate the articles that you want to see. If ""global officials"" gave a hoot more than just virtue signalling they could make free access happen overnight. Typical management. Cause a problem and then come running to the rescue with ideas for other people to have to carry out.",NA
"1226",1225,2,"What’s there to research? If rich Chinese want to eat bat soup, they’re going to eat bat soup.",NA
"1227",1226,2,"That website saved my undergrad back then lol",NA
"1228",1227,2,"So much this. There probably is no non-scientist of 21st century with bigger impact on science than Alexandra Elbakyan. 

#",NA
"1229",1228,2,"Oh my god..... thank you so much",NA
"1230",1229,2,"Also Library Genesis (http://gen.lib.rus.ec/) for free books",NA
"1231",1230,2,"I can call you Betty. And Betty when you call me, you can call me Al.",NA
"1232",1231,2,"[This guy](https://twitter.com/alyankovic/status/1238589589511483392)",NA
"1233",1232,2,"Al Bundy, don't you know him?",NA
"1234",1233,2,"This ^

Note: https://github.com/ieee8023/covid-chestxray-dataset?files=1
We need CT and other imagery though, x-rays are debatable indicators",NA
"1235",1234,2,"Nature opened their COVID-related papers: [https://www.nature.com/collections/hajgidghjb](https://www.nature.com/collections/hajgidghjb)",NA
"1236",1235,2,"thank you for your invaluable contribution to r/MachineLearning",NA
"1237",1236,2,"http://m.nautil.us/issue/83/intelligence/the-man-who-saw-the-pandemic-coming

You should check this out. Some bat species like to nest near humans and create problems. Or they infect other animals.",NA
"1238",1237,2,"Nonono, is AI Gore",NA
"1239",1238,2,"Yw!",NA
"1240",1239,3,"Title:Neural Networks are Surprisingly Modular  

Authors:[Daniel Filan](https://arxiv.org/search/cs?searchtype=author&query=Filan%2C+D), [Shlomi Hod](https://arxiv.org/search/cs?searchtype=author&query=Hod%2C+S), [Cody Wild](https://arxiv.org/search/cs?searchtype=author&query=Wild%2C+C), [Andrew Critch](https://arxiv.org/search/cs?searchtype=author&query=Critch%2C+A), [Stuart Russell](https://arxiv.org/search/cs?searchtype=author&query=Russell%2C+S)  

> Abstract: The learned weights of a neural network are often considered devoid of scrutable internal structure. In order to attempt to discern structure in these weights, we introduce a measurable notion of modularity for multi-layer perceptrons (MLPs), and investigate the modular structure of MLPs trained on datasets of small images. Our notion of modularity comes from the graph clustering literature: a ""module"" is a set of neurons with strong internal connectivity but weak external connectivity. We find that MLPs that undergo training and weight pruning are often significantly more modular than random networks with the same distribution of weights. Interestingly, they are much more modular when trained with dropout. Further analysis shows that this modularity seems to arise mostly for networks trained on learnable datasets. We also present exploratory analyses of the importance of different modules for performance and how modules depend on each other. Understanding the modular structure of neural networks, when such structure exists, will hopefully render their inner workings more interpretable to engineers.  

[PDF Link](https://arxiv.org/pdf/2003.04881v2) | [Landing Page](https://arxiv.org/abs/2003.04881v2) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.04881v2/)",NA
"1241",1240,3,"The paper is well written and easy to understand.

> We will represent our neural network by a weighted undirected graph G. To do this, we identify each neuron, including the inputs and outputs, with an integer between 1 and N, where N is the total number of neurons, and take the set of neurons to be the set V of vertices of G.  Two neurons will have an undirected edge between them if they are in adjacent layers, and the weight of the edge will be equal to the absolute value of the weight of the connection between the two neurons.  We will represent the set of weights by the adjacency matrix A defined by Aij=Aji:=the edge weight between neurons i and j. If there is no edge between i and j, then Aij=Aji:= 0. As such,A encodes all the weight matrices of the neural network, but not the biases.

...

> We will define the n-cut of a network as the n-cut of the clustering that algorithm 1 returns. As such, since n-cut is low when the network is clusterable or modular, we will describe a decrease in n-cut as an increase in modularity or clusterability, and vice versa.

...

Then they identified some important clusters and investigated their effect on the output by setting the incoming weights to a cluster zero.

> Several questions remain: Do these results extend to larger networks trained on larger datasets? What is the appropriate notion of modularity for convolutional neural networks?Why are networks so clusterable, and why does dropout have the effect that it does? Are clusters akin to functional regions of the human brain, and how can we tell? If modularity is desirable, can it be directly regularized for? We hope that follow-up research will shed light on these puzzles.",NA
"1242",1241,3,"There are different approaches taken to study neural networks: lottery ticket hypothesis, [neural persistence](https://openreview.net/pdf?id=ByxkijC5FQ), [this paper](https://arxiv.org/abs/1911.13299) etc and then the current paper. It would be interesting to know how these findings are connected to each other (if at all).",NA
"1243",1242,3,"This sounds somewhat like what is observed with STDP in recurrent spiking networks (see, for example, [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2947928/) figure 6E).

It makes me wonder how much such modularity is an inevitable consequence of an incremental learning algorithm on (weight) matrices, regardless of whether the algorithm is Hebbian (such as STDP) or backprop or some other method.",NA
"1244",1243,3,"Sorry if this is wrong but aren't conv layers mappable onto fully connected layers? i.e. you can transform a conv layer into a MLP? So they should be able to extend the analysis directly?",NA
"1245",1244,3,"My (unsubstantiated?) intuition is that the endpoint for a lot of these results will be some unintuitive and complicated formulation in terms of random matrix theory or something similar, but which has these kinds of phenomena as special cases.",NA
"1246",1245,3,"Yes and they probably will extend this research. It's just not in the scope of this paper.",NA
"1247",1246,3,"Ie the paper corresponding to 

https://github.com/CalculatedContent/WeightWatcher/blob/master/README.md",NA
"1248",1247,3,"Interesting! Let's see how it goes. I am hopeful someone will research into it.",NA
"1249",1248,5,"Maybe I’m wrong, but to me it appears that literally no one in this thread is an actual PhD student in CS.

As an undegrad, you barely know enough to be able to understand research papers. Most undergrads that work in our lab are in the direct supervision of a senior PhD student, and still very few of them actually manage to produce publishable results.

Now suppose that you do this in isolation. To me, it seems straight up impossible with absolutely zero guidance. And even if you do the research and somehow get good results, knowing how to write publishable papers is something that takes years, if ever, to develop. And this is something that your advisor really helps with, and something you can absolutely not do in vacuum.

And even then, the most competitive conferences are, well, extremely competitive. I don’t even have a paper in NeurIPS/ICML, and I’d like to think I do pretty good research. Even my advisor has only 2-3 papers accepted at NeurIPS under his belt (but he is rather young). For these conferences, it also requires a specific presentation of your work to cater to the reviewers expectations.

So, while in theory most of the comments you got here might be correct (there are many available resources online), they are extremely daft. In my opinion, as an undergrad, who is working independently, with zero prior experience in the field/research, it is absolutely impossible that you would be able to publish in NeurIPS.",NA
"1250",1249,5,"You could do the research yourself then present it to a professor in that area at your university. They will hardly ignore it. Make sure you send it to the honest ones, though",NA
"1251",1250,5,"But to answer your questions: Yes, you can do research on your own. There's open programming frameworks, open paper preprints, open papers and journals in general, and the best conferences will allow you to publish without affiliation. The hardest part will be the pay. It's really expensive to publish (>1k min per paper including conferences, unless they are in your area. Major conferences of course). The good news thou is that in ML specifically, most research is on ArXiv. People use it as a primary means of publication nowadays so you can do it for free.

TLDR: There's open software and publication media, so access is ubiquitous. As long as you have the means for hardware and for publishing costs, it is possible to do so indeed.",NA
"1252",1251,5,"Some people have tried it and succeed! Take a look at this great article by Andreas Madsen:

[Becoming an Independent Researcher and getting published in ICLR with spotlight](https://medium.com/@andreas_madsen/becoming-an-independent-researcher-and-getting-published-in-iclr-with-spotlight-c93ef0b39b8b?source=---------2------------------)

He had some great advice for you. The bottom line is: It's hard, avoid it if you have another alternative.",NA
"1253",1252,5,"You can absolutely do research on your own, but your best bet would be to do some preliminary research yourself and make good progress, then write up your work and take it to a professor and make a pitch to join their group. Professors get inundated with students who want to build their resumes and then end up not being very productive. Showing initiative like that would definitely help you stand out.",NA
"1254",1253,5,"Getting into labs isn't as hard as you would think. You should definitely try. 

Barring that, you should blog! There are tons of hobbyists out the there doing cool stuff **all the time**. I've had blog posts cited in academic papers. It looks great, lets you keep your tone a bit more informal, and throw in visualizations + other interactive things that are not possible in papers.

As someone who produces both academic papers and blogs posts, one is fun and the other is work.",NA
"1255",1254,5,"As most said, without graduate degree experience or lab experience, it will be extremely hard to write a paper that get published at top conferences. 

However, doing research doesn't mean writing a top-conference published paper. Some unpublished/rejected papers are totally amazing and there is always the possibility of submitting to a non-top conference or workshop. So do not get discouraged and try for yourself, start your own experiments and try to write a paper on it. First time, it may just be on the level of a school project, but then the next one may get accepted to a workshop, and then the next one to a good conference. You can learn by trying on your own. It's just that your expectations shouldn't be a Neurips conference paper. There is too much pressure nowsadays, people need to relax more. Phd students are really overpressured and you can tell by the way people interact in social media.",NA
"1256",1255,5,"Professor here. Sure u can. If u are a genius, you will figure out something. If you are not,  find a professor and work with him/her. U better have pretty good programming skills.",NA
"1257",1256,5,"I'm ready the enthusiastic answers here and I wonder whether the other posters and I read your question differently. It might come to the definition of ""do research"". What do you mean by that?

In the strictest sense, ""doing research"" means finding a problem you want to solve, reading papers, and solving said problem. In that regard, there is nothing stopping you from doing research on your own. I doubt it will hurt your career down the line.

However, to me, ""doing research"" translates into identifying a relevant problem in the field that will resonate with other researchers in the community, effectively reviewing the relevant papers and connecting with up-to-date work (which is yet unpublished), solving said problem, and presenting your solution to others via seminars, conferences, and manuscripts. That, unfortunately, cannot be done solo. Due to the nature of modern academic research, you pretty much have to belong to a lab and have a PI (preferably a recognized PI) to help you, guide you, introduce you to the right people, and put her or his name on the manuscript.

If you are curious about a career in academia, I think that your best bet is to find a course which includes research. Universities often offer ""seminar"" courses that involve some literature review and a minor project. Start from that, and use that experience to promote yourself with a professor with whom you could work.",NA
"1258",1257,5,"Is your goal to go straight into some kind of ML oriented job, or do you want to pursue a PhD in ML? Or something else entirely?",NA
"1259",1258,5,"You might want to look into research opportunities in other departments. I bet there is at least one faculty member in every science discipline department looking to dabble in ML techniques, and many won't know where to find students to work with. There are probably people in many humanities departments as well. My university has an office of undergraduate research that can help link students with advisors. You can also reach out yourself - so as not to be a nuisance I would recommend picking the most likely faculty member in each department to ask, state your interests and inquire if they have any projects, or know someone else in their department looking for a student with your research interests.

The projects you would do would probably be more applied than you might like, but they would demonstrate your commitment and might help you get your foot in the door in a CS group later on.",NA
"1260",1259,5,"Especially in comp sci, at least the tools are available and you don't need any expensive lab equipment.

The difficulty I've been facing is knowing what to work on exactly, which is where having a lab might help.",NA
"1261",1260,5,"Yes, I've been doing AGI research on my own for 50 years. You can check it out at www.adaptroninc.com",NA
"1262",1261,5,"I mean that's pretty much what you do when writing a thesis, so... sure.",NA
"1263",1262,5,"It will absolutely hurt your ability to publish, but if it is ground breaking enough you should utilize:

- Making your own website
- Having the paper or work locked behind a request form
- Send out a promoted content blast on a social media app targetting certain demographics
- See who strolls by to check out your content
- Enjoy new found success",NA
"1264",1263,5,"I think it's really hard but not impossible. I'm a PhD student now, but when I was doing supervised research as an undergrad, my supervisors didn't really help for shit. I did an reu, but my advisor was going through a personal crisis and was afraid to come into work (with good reason, for details I won't go into). So I just figured stuff out on my own and did a sort of sloppy paper about ML for automated code repair and took it to the author I based my work on. She was impressed and brought me to her reu next year. 

Another research project I put together as an undergrad and took to my advisor when I entered my PhD program. He liked it and gave me funding to work on it, and hopefully that'll get submitted to NeurIPS this year. 

So it's doable if you have a strong math background, read a ton of papers, and pick more out-of-the-way subjects that have more low hanging fruit.",NA
"1265",1264,5,"Although difficult,  I don't think it's as impossible as you say. 

In the current ML landscape, the range of topics has gotten so wide that there are a ton of ""low hanging fruit"" papers published. 

In addition,  fundamentally,  most ML papers do not contain many prerequisites to understand. 

Finally, with resources like ICLR Openreview, it's become a lot easier for independent researchers to learn what reviewers are looking for.

It's certainly not easy, and in some sense I wouldn't even advise it (rec letters are more important than published research). But given the right idea, lots of work, and some luck, it's definitely doable for a smart and motivated undergrad. 

I say this as an undergrad who published a co first author paper at a top vision conference(with only other undergrads as first author) with essentially no supervision from our advising professor.",NA
"1266",1265,5,"This doesn't resonate with me at all. If you want to write a paper to throw on arxiv, just take a pretrained model, pick an interesting application, and get your hands dirty.",NA
"1267",1266,5,">The hardest part will be the pay. It's really expensive to publish

It should also be noted that many conferences pay travel support and many universities support undergrad publications financially",NA
"1268",1267,5,"Agreed. Would also recommend approaching grad students individually because they will likely be the ones you end up working with.

As a former grad student who mentored many undergrads, I would definitely want to hire one that was motivated on their own and could show me some of their code. Kaggle has tons of datasets for old and current challenges. You don’t need to participate in a challenge to use the dataset and you can get ideas from other people’s code that they’ve posted. Good luck!",NA
"1269",1268,5,"Happy cake day!",NA
"1270",1269,5,"This is your answer here, use the department facility. That's what they are there for. You could look at some adjuncts as well for a bit of assistance, they often have less obligations in the department (but also usually have high obligations outside).",NA
"1271",1270,5,"Also reach out to graduate students because they are way more likely to respond to your email.",NA
"1272",1271,5,"i would say both. I feel like the kind of roles I am interested in require some kind of MS or PhD experience to even intern there so I would likely go for a PhD after my undergrad and try to go for a job from there.",NA
"1273",1272,5,"Could you elaborate more on why it would hurt the ability to publish?",NA
"1274",1273,5,"However, this is not in vacuum. I’m even going to ignore the fact that you collaborated with another undegrad (which was not the question asked/what I answered), and say that on the least, the lab/professor you were working with gave you some direction. Someone check up in you, even once or twice (this matters way more than you think), and someone proofread your paper.

While the idea and the work is your own, everything around it, which is actually extremely important to publish a paper, is something people don’t really pay attention to, and this is why I say that, in vacuum, an undergrad simply can’t publish a paper on their own.",NA
"1275",1274,5,"> I say this as an undergrad who published a co first author paper at a top vision conference(with only other undergrads as first author) with essentially no supervision from our advising professor.

#### CVPR, ICCV, ACL, NAACL, and EMNLP are watered-down, applied machine learning engineering conferences. They are practically workshops. My point: it's not hard to get a paper accepted there. If you're a PhD student or postdoc and don't believe me, look at where the new junior faculty at top schools published their papers during their PhD. 

#### Now, for top ML conferences like NIPS, ICML, AISTATS or ICLR, start googling the authors of accepted papers. The accepted papers are listed on the conference websites.

#### Count how many authors are undergrads. Count how many are unaffiliated with a research institution. Then compute your odds. Don't get your hopes up.",NA
"1276",1275,5,"And that “paper” would not get accepted, in a million years, at NeurIPS.",NA
"1277",1276,5,"Networking and many publications not accepting from people not in an accredited university.

It shouldnt be that way, there should be a process to call them up and ask for a secure channel to submit your research.",NA
"1278",1277,5,"I suppose if the situation is: ""can an undergrad locked in a room with no communication with the outside world publish a paper"", that's pretty difficult. 

I interpreted OP's question more as: ""can I get research published without an advisor?"" And I think the answer is yes. 

I suppose it's hard to convince you about the role our advisor played, but he didn't proofread our paper (before submission). He's provided a lot of helpful advice on other things, but on this paper specifically I don't think his advice was crucial. 

I do know some solo author papers from undergrads:

https://arxiv.org/abs/1903.03605

https://openreview.net/forum?id=HJ0NvFzxl

I can't comment on their circumstances, but part of the reason it's so rare is that if a student meets the conditions to be able to publish themselves (ie: smart, motivated,  and has read lots of papers), there's little reason for them not to work with a professor, nor for the professor to not work with them.

The student needs letters of recommendation for PhD applications, and professors are usually happy to work with undergrads that are competent enough to publish.",NA
"1279",1278,5,"This account seems like a troll, but for the record, CVPR has a sub 30% acceptance rate. None of these conferences are “practically workshops” even if they’re doing applied work, rather than theory.",NA
"1280",1279,5,"Uh, do you think new CV and NLP professors aren't publishing in the top CV/NLP conferences?

[Bharath Hariharan](http://home.bharathh.info/)

[Sasha Rush](http://rush-nlp.com/papers/)",NA
"1281",1280,5,"There is no point in arguing further, so I won’t reply after this.

In the first link you posted, a. it is a paper on arXiv, not a published one and b. after the references he thanks Prof. Jelani for suggesting the topic and advising him.

In the second, I really do not know the circumstances, but I highly doubt that a research group wasn’t involved in any way.

I don’t mean to be offensive, but I’m pretty sure you’re an undergrad and you seem to have no actual experience. I’m a PhD student in CS in one of the top universities in the field. Literally the only people I interact with are insanely smart, extremely motived people who read at least 10 papers a week on top of working on research who knows how many hours a week, and whenever any of us gets a paper accepted at NeurIPS everybody celebrates. You just do not grasp how hard and competitive it is.",NA
"1282",1281,5,"The factual error is easy to correct. 

Neurips link for first (was hoping you could assume that I knew the difference...): https://papers.nips.cc/paper/9656-understanding-sparse-jl-for-feature-hashing

Blog post for second paper: http://www.hexahedria.com/2016/11/06/introducing-the-ggt-nn.html

Both of these were spotlight/orals.

It's true that I haven't published at neurips or ICML, but ""no experience"" is a bit harsh. I've gone through the review process for a co-first author paper at a top vision conference. Also, I said that I was an undergrad in my first comment, so not very offensive. 

Maybe ICML and Neurips are just that much harder - we'll see after I get my reviews back for my current paper :)",NA
"1283",1282,6,"Just glancing at the topics in the lecture notes it looks quite good, much focus spent on the basics that many ignore.",NA
"1284",1283,6,"It's astonishing to go back through the catalog from 1968 and see how little has changed at the undergraduate level.",NA
"1285",1284,6,"I took the course in Fall 2018. The lecture calendar looked like this -

Introduction, Overview, Basics

**Supervised learning**

Classification 1: Optimization, Loss Function, Regularization

Classification 2: SVMs & Kernels, Bayes Classifier, ROC, Logistic Regression

Classification 3: Naive Bayes, Generalization

Classification 4: VC Dimension

Regression 1: Linear / Polynomial Regression, Kernel, Predictive Distribution

Regression 2: Bayesian, Shrinkage, Regularization and SGD

Neural networks (feed-forward): Theory, Representation Theorem

Neural networks: Optimization

Neural networks: Structured prediction, Language Modeling Word2vec

Neural networks: Robustness to Dataset Shift

**Unsupervised learning**

Dimensionality Reduction: PCA practice

Dimensionality Reduction: PCA theory, NMF, t-SNE

Matrix Estimation

Clustering: mixture model, K-means

Topic models

Variational Learning

Deep Generative Models

Probabilistic modeling

Sampling, MCMC, Gibbs

Gaussian processes, Using prior knowledge about world

**Decision making**

Acting under Uncertainty, Model Predictive Control

Markov Decision Processes

Reinforcement Learning, Bandits

AlphaGoZero Discussion

Remember that this is a graduate class and the goal is to give a solid theoretical foundation and a bird's eye view of ML to a 1st or 2nd year grad student. The homework and lectures weren't too focused on implementation, performance or specific applications like vision/NLP. There are specialized courses for each of those things.",NA
"1286",1285,6,"Course content looks like a classic to me. Go for it mate. Optimize for the course that can help you gain the most. 
MIT classic courses will give you strong theoretical foundations, give a broad overview. The value maybe initially hard to appreciate but will clear over time. 
A Coursera course on the other hand, will start fancy you might find it easier to finish. Then you will have to do work to get deeper ideas and develop broad view on the field. 
Both are valid paths, choose what do you think works and stick with it for a while.",NA
"1287",1286,6,"[https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning) is pretty popular",NA
"1288",1287,6,"I imagine the concepts would be relevant, but there might be methods missing as a big part of machine learning over the past decade has been people figuring out new ways to structure the programming. If that makes sense. I'm not sure how to word it.",NA
"1289",1288,6,"I still teach my MSc learning course based on duda Hart stork!",NA
"1290",1289,6,"Anyone tried [this](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/about) yet?

I think its MIT's undergraduate version, and has all the lectures and problem sets associated.

Haven't had time to try it for myself yet, but am keen to see what people think of it.",NA
"1291",1290,6,"This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: /r/Serendipity/comments/fivqan/discussion_is_mit_open_course_6867_machine/",NA
"1292",1291,6,"Nah, just jump straight to deep learning, what could go wrong /s",NA
"1293",1292,6,"I work with a machine learning team at a big tech company. I think you should brush up on the most popular software and languages being used, and modern best practices, from coding to strategy for solving problems.",NA
"1294",1293,6,"is arithmetic still relevant after thousands of years?",NA
"1295",1294,6,"They even cover EM!",NA
"1296",1295,6,"If it’s worth teaching, it’s unlikely to stop being relevant.  Stuff like particular programming languages or what not are things you can pick up on your own, but underlying concepts are eternal.",NA
"1297",1296,6,"Thanks a lot for the info! Are there somewhere slides/notes for more updated version of the course?",NA
"1298",1297,6,"Wow thanks!",NA
"1299",1298,6,"I’d go with this one",NA
"1300",1299,6,"A decade of efficiency is available",NA
"1301",1300,6,"The course you linked does seems more tailored for online learning as the assignments are created like Jupyter Notebooks, where you can get instant feedback for each question instead of the tradition problems + solutions paper format.

What's the difference between MIT OpenCourseWare and MIT Open Learning Library?",NA
"1302",1301,6,"Any suggestions for online resources?",NA
"1303",1302,6,"After quick search I found a repository on GitHub with lectures from 2017 - enjoy!

 [https://github.com/ottermegazord/mit6867](https://github.com/ottermegazord/mit6867)",NA
"1304",1303,6,"Problem is, I don't want to use MatLab and Octave tho...",NA
"1305",1304,6,"Nvm, I found the answer to my question on their FAQ page. So basically  MIT Open Learning Library = MITx, but without live support, discussion forum, or certificates.

&#x200B;

Source:

`You can think of the three different resources as a spectrum of offerings utilizing MIT content for different types of learning experiences. On one end is edX.org — MITx courses on edX are end-to-end course experiences with certificates available for you to earn, live teaching support in a discussion forum, and start and end dates. On the other end is MIT OCW, which is a completely self-guided experience containing published content from MIT courses that is open all of the time and licensed for reuse, but that does not include any interactive content. Open Learning Library sits in between. As in many MITx courses, Open Learning Library provides interactive course experiences that include auto-graded assessments that give you instant feedback and allow you to track your progress as you work your way through learning the subject matter. Like OCW, this content is always open and self-guided and includes no live support, discussion forum, or certificates.`",NA
"1306",1305,6,"!remindme 2 days",NA
"1307",1306,6,"Holy crap! How did you hit this gold mine???

Now let's download everything before the repo gets deleted for some reason.",NA
"1308",1307,6,"If you are interested just in learning you don't need to do the homework in that language, use whatever you want. The homework is to get a certificate.",NA
"1309",1308,6,"Ng explains that matlab and octave are very good for modeling machine learning problems since there are a lot of built in functions. Once modeled in matlab/octave and shown to be working correctly, it can be easily translated to another language like python. Starting in a language like python/java at the beginning can be very convoluted. To each their own though. It comes with Matlab Online if you're worried about the purchasing part.",NA
"1310",1309,6,"I will be messaging you in 2 days on [**2020-03-17 13:32:24 UTC**](http://www.wolframalpha.com/input/?i=2020-03-17%2013:32:24%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/filj46/discussion_is_mit_open_course_6867_machine/fkkauxe/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Ffilj46%2Fdiscussion_is_mit_open_course_6867_machine%2Ffkkauxe%2F%5D%0A%0ARemindMe%21%202020-03-17%2013%3A32%3A24%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20filj46)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",NA
"1311",1310,7,"Yes! The canonical example of a place you'd use multiple activation function in a single network is an LSTM. (long short term memory network)

Edit:

Also, this seems more like a question for r/learnmachinelearning. 

I made my fair share of these types of posts here before I was pointed at that subreddit.",NA
"1312",1311,7,"I don't like most of the answers. The output functions are clearly not what OP means, and I'd borderline not call them activation functions as they should be folded into the loss anyways (e.g. softmax-nll).

But yes, sometimes it is beneficial to have different activations functions for different layers, even for a ""straightforward"" CNN. Have a look at [MobileNet v3](https://arxiv.org/abs/1905.02244), it uses both ReLU6 and h-swish for various layers.",NA
"1313",1312,7,"Yes, trivially - both sigmoid and softmax are activation functions. Even if you're hellbent on sticking sigmoid activations in the thinkybits, if you're doing one-vs-all classification, you'll be normalizing the signal in the output layer.

If there's a fan-out pattern in your architecture - for example, you autoencode an image and want to measure both reconstruction and classification - the layers after the split will use independent, and possibly different, activations on the same input layer.

It doesn't really make a ton of sense to use, say, both a ReLU, SELU and Swish on the same layer for the hell of it, since they behave *almost* identically throughout their domains - pick one and stick to it. If the model convergence *depends* on that kind of trick, it would be kind of a red flag in terms of model robustness.",NA
"1314",1313,7,"The other two responses I’ve seen cover two broad general responses to this question:

If you have a good, theoretical reason to (like lstm), then yes! Absolutely.

If you’re just building a feed-forward neural network on a simple task, just using relu is probably fine.

To add another interesting (although maybe less useful in practice) response, there has been some work on actually learning which activation functions to use and where. This paper [Weight Agnostic Neural Networks](https://arxiv.org/abs/1906.04358)  does this and suggests that, interestingly, activation functions themselves can take on a role similar to that of network weights.",NA
"1315",1314,7,"I'd say that I'm fairly well equipped to answer this question, since I spent more than a year working on a research problem related to activation functions. I've ran dozens of experiments on pretty much all the activation functions (standard and exotic) in the literature on multiple datasets of different sizes and domains to get a statistically unbiased estimate of performance, I've come up with new activation functions by hand, I've used search techniques to automatically discover new ones, and there's one thing I can say for certain: we don't really understand activation functions. All this ""ReLU induces sparsity so it disentangles information"" and ""ELU has negative values which push the mean closer to zero"" really breaks down in practice as you find multiple activation functions that look extremely weird but outperform the standard ones on certain problems, suggesting that the shape of a good activation function is problem dependent. To answer your question, you're not going to find much literature on the problem that you're interested in, except for a few niche papers like [this one](https://arxiv.org/abs/1703.07122). This does not mean that this is a problem you shouldn't pursue: again, I think our understanding of activation functions is very incomplete. It could very well be that the use of multiple activation functions turns out to be very useful, and it could also turn out to be useless. I think that you should take an Automated Machine Learning approach to this problem and try to search for heterogeneous neural networks having different activation functions and see what you can conclude.",NA
"1316",1315,7,"There are several good answers already, but it might be worth mentioning that in encoder-decoder networks, the encoder and decoder may be very different architectures, with different activation functions.",NA
"1317",1316,7,"For using different activation functions for different layers, yes this is definitely a common thing. As your data is transformed through your network, you'll find that at different stages different types of transformations work better for the data.

As for your other question, multiple activation functions on the same layer. You could theoretically do this but stacking activation functions directly condenses this layer output to a single value, which is undesirable under normal circumstances.

You have to understand that activation function takes multiple inputs and transforms them into a single value *per node* of the next layer. If we transform the nodes into a single value as well we're just left with a single value which is not really what neural networks are for unless we're talking about the output layer.",NA
"1318",1317,7,"Another interesting angle is, you can use activation functions to restrict parts of your network to be in certain ranges. For example in gating or attention mechanisms.",NA
"1319",1318,7,"In my opinion, there are only coarse-grained principles of using activation functions: use fast converging activation functions at hidden layers, use softmax/sigmoid functions to output probabilities.

There are activation functions with different converging, normalizing and regularizing properties (ELU, RELU, SELU, GELU etc.), but I've never seen before in papers any neural network with mixed activation functions. In my opnion, you won't see the difference in perfomance, if you substitute 10% or 30% of RELU neurons with SELU or GELU, because, these functions have very similar shapes.

Concluding, there are few solid coarse-grained principles and a lot similar functions, which show difference only at large scales.",NA
"1320",1319,7,"If you want to consider another class of computation graphs than neural nets, you might wanna take a look at how genetic programming (GP) operates: it is ALL about having different activation functions (weights are not really a thing in the field (yet?)). 

GP builds computation graphs which flexibility comes exactly from composing different functions (activations, if you prefer). You can fit regression/classification data to obtain small & hopefully interpretable symbolic expressions (see ""program representation""  [wiki for GP](https://en.m.wikipedia.org/wiki/Genetic_programming)). You can use it in combination with other machine learning approaches (blatant self advertising :) [On explaining machine learning models by evolving crucial and compact features](https://arxiv.org/abs/1907.02260)), but also apply to reinforcement learning tasks ([Evolving simple programs for playing Atari games](https://arxiv.org/abs/1806.05695).",NA
"1321",1320,7,"Many commenters are speculating that it does not matter because many activation functions are similar, or that you should just use the same one everywhere. I disagree! I don't think anyone ever looked at this question in detail.

I think it would be an interesting experiment to try different activation functions in different layers. Do not let yourself get discouraged, you ask interesting questions!! Excellent.",NA
"1322",1321,7,"Which activation functions to use for different layers is one of the many hyperparameters you should investigate in your cross validation.",NA
"1323",1322,7,"This paper describes parametric Relu's where the slope (alpha) of the positive part is learned during training, per layer. So, in essence, it has as many activation functions as layers. (That being said this paper came out a while ago and PReLu's don't seem to be widely used so I guess the benefits are marginal)

(This paper is more notable for introducing He intitialization, which is where most of the gains they report come from I think)

https://arxiv.org/abs/1502.01852",NA
"1324",1323,7,"Yes, one example are squeeze-and-excite blocks which use ReLU inside but for the output a (hard-)sigmoid.",NA
"1325",1324,7,"And to learn the intricacies of LSTMs, I strongly recommend this blog post http://colah.github.io/posts/2015-08-Understanding-LSTMs/",NA
"1326",1325,7,"Thank you for the heads up. I actually found out about that forum right after I posted here. But i let the post stay in case it was a good discussion anyways. :)",NA
"1327",1326,7,"Exactly. In the output layer I assumed it was expected. However almost all answers I've come across either say to try it out or that it complicates the network. I appreciate all the answers here though.",NA
"1328",1327,7,"Good point about sigmoid. Often used as output activation, whereas the feature extractor often uses something different, e.g. relu or tanh.",NA
"1329",1328,7,"> I spent more than a year working on a research problem related to activation functions.

Did you publish your results anywhere? Why not?",NA
"1330",1329,7,"Do you have a reference that explains GP from the perspective of a computation graph? Because I really want to read that. I've been doing some work in EC for my MS thesis, and haven't come across that perspective before. The atari paper only mentions the formulation, but didn't really explain it.",NA
"1331",1330,7,"+1 on that post! My go to is Karpathy's [lecture](https://www.youtube.com/watch?v=yCC09vCHzF8).",NA
"1332",1331,7,"In progress.",NA
"1333",1332,7,"

Edit: 
People developing Cartesian GP refer to it from a computation graph perspective - BUT, many other representations in GP are computation graphs as well. Perhaps this can be considered obvious.
If you need any more info/help you can send me a private msg of course :) I did a PhD on GP (am about to defend)",NA
"1334",1333,8,"Interesting thanks for sharing. Curios about the test time design, is there a specific reason that the test time weight matrix was chosen to be the mean of the training environments' matrices?

Ah nevermind you commented on that in the appendix. Thanks for including a bit on negative results that's actually a great idea to include in papers. I hope more people do this in ML.",NA
"1335",1334,8,"Title:Unshuffling Data for Improved Generalization  

Authors:[Damien Teney](https://arxiv.org/search/cs?searchtype=author&query=Teney%2C+D), [Ehsan Abbasnejad](https://arxiv.org/search/cs?searchtype=author&query=Abbasnejad%2C+E), [Anton van den Hengel](https://arxiv.org/search/cs?searchtype=author&query=van+den+Hengel%2C+A)  

> Abstract: The inability to generalize beyond the distribution of a training set is at the core of practical limits of machine learning. We show that the common practice of mixing and shuffling training examples when training deep neural networks is not optimal. On the opposite, partitioning the training data into non-i.i.d. subsets can serve to guide the model to rely on reliable statistical patterns while ignoring spurious correlations in the training data. We demonstrate multiple use cases where these subsets are built using unsupervised clustering, prior knowledge, or other meta-data from existing datasets. The approach is supported by recent results on a causal view of generalization, it is simple to apply, and it demonstrably improves generalization. Applied to the task of visual question answering, we obtain state-of-the-art performance on VQA-CP. We also show improvements over data augmentation using equivalent questions on GQA. Finally, we show a small improvement when training a model simultaneously on VQA v2 and Visual Genome, treating them as two distinct environments rather than one aggregated training set.  

[PDF Link](https://arxiv.org/pdf/2002.11894) | [Landing Page](https://arxiv.org/abs/2002.11894) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2002.11894/)",NA
"1336",1335,8,"I need to read the paper but I find it interesting.  SGD relies on iid assumptions about convergence. (but I guess nobody cares about that.)

Need to read it carefully. Antons team is putting several interesting ideas (yet simple) year after year.

Congratulations",NA
"1337",1336,9,"Yeah absolutely. One way to think about it is BERT combines trained embeddings with a trained classifier, so you benefit from Wikipedia without doing much (more) training.

I just followed [this guide](https://colab.research.google.com/drive/1ixOZTKLz4aAa-MtC6dy_sAvc9HujQmHN) & got better results than embeddings. There’s probably a better way to do it.",NA
"1338",1337,9,"I thought it'd require a fine-tuning, because the texts don't appear to be articles, they might as well be unstructured and poorly written. Thank you for the link!",NA
"1339",1338,9,"No problem. Be sure to keep looking, eg i think [this link](https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed) is slightly better. Use Bert-large-uncased, and I hope you have a GPU because otherwise you’ll take 2+ hrs to run through a 5 min job.",NA
"1340",1339,11,"Oh, this looks familiar! If you have any questions, let me know. 

Also, if you want to click through an interactive demo try the colab: [https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural\_tangents\_cookbook.ipynb](https://colab.sandbox.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb)",NA
"1341",1340,11,"How exactly is this different than just using a gp and training the hyperparameters with gradient descent?",NA
"1342",1341,11,"Is it fair to claim GPs are a ""simpler class of models""? If anything I would guess the opposite. If what I am inferring from the post is true then it would suggest neural networks are an approximation and, in a sense, a subset of GPs - more specifically, some arbitrary GP with an arbitrarily complex kernel.

Following that, I'm trying to realize the significance from an inference perspective; if one converges to a GP what is the benefit of specifying such an infinite network in the first place, as opposed to, simply specifying one of the many previously proposed GP models or approximate GP with an elaborate kernel?",NA
"1343",1342,11,"This is great! I wrote my own little library for NNGP for a cosmology problem. Will try this over the weekend and start shifting my code :)",NA
"1344",1343,11,"What do you mean by infinitely wide? Surely its limited by gpu vram capacity. Is it a really big number like 10^12 neurons in a layer? Not an expert btw.",NA
"1345",1344,11,"Excuse my ignorance, but how does one go about using this? What's the usefulness of infinitely wide networks?

Even if I train an infinitely wide model using a GP, isn't that the same as using a GP in first place to learn the function I want to learn? So, why is this interesting?",NA
"1346",1345,11,"Can someone explain what the implications of this? I am new to deep learning and my understanding of Gaussian processes is not so good.",NA
"1347",1346,11,"This result exists??? This is amazing, this is bigger than the Transformer paper. I’d never really put much thought towards the infinite width limit case. More papers to read yet it seems XD",NA
"1348",1347,11,"Quick question about the research, is it correct to say that infinitely deep and wide neural networks are gaussian processes?",NA
"1349",1348,11,"Happy cake day!",NA
"1350",1349,11,"Mathematically simpler -they're easier to formalise and it's easier to understand what they do.

  


Computationally GPs are still harder than neural networks.",NA
"1351",1350,11,"Using a gaussian kernel to compute infinite dimensional dot product in each layer",NA
"1352",1351,11,"Great question! When we say ""infinite width"" we mean to say that as neural networks get very wide they become a different kind of model (called a Gaussian Process). You can train the corresponding GP without ever actually building any of the weights. In practice for fully-connected networks networks look pretty close to ""infinite"" when their width is about 1024 units.",NA
"1353",1352,11,"Yes that's exactly right! If you initialize a wide neural randomly the distribution \_over\_ different instantiations of the network will be a Gaussian Process. Moreover, the statistics over the course of training will also be a GP if you train with MSE loss.",NA
"1354",1353,11,"An infinitely wide feed foward network can approximate literally any continuous function. A gaussian process requires that a finite subset of features must have a multivariate gaussian distribution.",NA
"1355",1354,11,"GPs are by all means *not* mathematically simpler. Perhaps depending on who you ask, less mathematically messy but not mathematically simpler. Neural networks at their simplest are quite literally multiple levels of generalized linear regression trained with an extremely simple and interpretable loss like MSE. You don't even need to solve a system of equations. GPs have all manner of complications like kernels, marginal likelihoods and predictive distributions. 

Yes *classical* GPs are computationally more difficult than NNs but there are also a plethora of modern, scalable GP methods so the line of whether GPs are ""computationally harder"" is actually very blurry.

This is why my original question arose: it's a very strong claim in one direction to say GPs are a ""simpler, class of models"". This paints a rose-coloured portrait of NNs for someone who has never heard of GPs but raises a lot of questions for someone who is familiar with both.",NA
"1356",1355,11,"Is the ""infinite"" width relative to the input size?",NA
"1357",1356,11,"Maybe it's just me but you shouldn't use the term infinite. It has a purely theoretical meaning in mathematics. By using this you are just confusing the reader even if they are familiar with the GP, ""infinite width"" sounds like nonsense.",NA
"1358",1357,11,"I'm struggling to wrap my head around the implications here.

For a traditional Gaussian Process with a fixed covariance function (rbf, etc.) the correlation/covariance between data points is completely independent of their actual labels.  In some sense you can ""train"" a GP (i.e. compute the inverse of the covariance matrix) without touching your training labels at all, only reaching for the labels during inference during the final dot product.

If neural networks are ""actually Gaussian Processes"", this would seem to suggest that one's posterior over neural networks (except the last layer?) should be completely determined by one's training data, *without* the labels.

I'm feeling pretty confused on the proper way to reason about neural networks as GPs, but is there a formulation for which this is true?  If so does this have implications for how we train traditional neural networks?",NA
"1359",1358,11,"I think you're massively underestimating how much arbitrary function composition of arbitrary depth can screw everything up. This is what makes NNs hard to understand.

In comparison to NNs GPs are much easier to understand just because they're effectively shallow (albeit infinite dimensional/wide). This means that it's possible to analysis if they will actually learn what you want them to, rather than just if it's theoretically possible for them to learn the right thing and to start thinking about rates of convergence.",NA
"1360",1359,11,"GPs aren't Turing complete, which is a pretty strong ""complexity"" dividing line

EDIT: okay I wasn't expecting this statement to be this controversial, but what I mean is this

Suppose I want to a function that returns true for a sequence with n ""a""s, followed by n ""b""s, followed by n ""c""s for any n, and returns false for any other sequence. I know how I would set all the weights and activation functions to make an RNN solve this (recurrent relus to count the number of each letter, and some recurrent hard sigmoids to form a persistent bit that flips if an a isn't at the start or if a b is preceded by something other than a or b, etc). If I set this up correctly, it will be of finite size and return exactly 1 or 0 as needed, no approximation. 

How would you get a GP to solve this problem? What kernel would you use, and what (finite number) of data points would you use to the exact correct answer every time?",NA
"1361",1360,11,"it's the width of the hidden layer.",NA
"1362",1361,11,"OP please correct me if I'm wrong, but it can be shown mathematically that DNNs converge to GPs as their width tends to infinity. That's why they're refered to as ""infinite width"".",NA
"1363",1362,11,"If you are familiar with GP, then you are aware of the [""kernel trick""](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick), which allows you to perform operations over infinite-dimensional spaces.",NA
"1364",1363,11,"What do you mean? If you want to compute the answer, you need to condition on y = f(x) for all (x,y) pairs of your data, otherwise you can't compute the posterior. Those y ARE the label.",NA
"1365",1364,11,"Can you explain a litte more by what it means when u say that ""GPs aren't Turing complete"".",NA
"1366",1365,11,"Not trexdoor, but this has always bothered me, since the effective dimensionality is the size of your dataset in this case.  The ""kernel trick"" is just letting you choose non-linear functions as your basis.",NA
"1367",1366,11,"Sorry I really butchered that.  Let me try again know that I've had some time to think through what I was trying to say.

The formula for the MLE of a GP's posterior (a.k.a. kernel linear regression) is

    K(Z,X) inv(K(X,X) + Ic) y

Where (X, y) are your training set, Z is your test set (or whatever data you're inferring on), and K is your kernel.

I'll rewrite it as:

    weights = K(Z,X) inv(K(X,X) + Ic)
    prediction = np.dot(weights, y)

You're right (of course) that to compute our predictions for Z we need to know y.  But I think the important thing here is that the weights we compute are independent of y.  (Also, incidentally, the variance at each point in our posterior also doesn't depend on y!)

\----------------------------------------------------------------

My main question is whether one can represent an infinitely wide neural network in such a way that one could similarly apply labels ""after the fact"" by computing only the weights.

To put it another way, for a given infinitely-wide network, there exists an f such that

    weights = f(X, Z)
    predictions = np.dot(y, weights)

is equivalent to the predictions of this infinitely wide network (this is true because infinitely wide networks are, apparently, just Gaussian Processes, and the above claim is true about GPs).  The question is how we might go about representing/finding ""f"".

In true 2015 fashion, we might consider replacing ""f"" with neural network.  And my claim is that such a network *ought* not need training labels to train, since the part of the Gaussian Process that it is emulating didn't ever see the labels.

(This was the thought that produced this (frankly unintelligible) paragraph:

>If neural networks are ""actually Gaussian Processes"", this would seem to suggest that one's posterior over neural networks (except the last layer?) should be completely determined by one's training data, *without* the labels.

)

To recap:

An infinitely wide NN is equivalent to a GP, which we can approximate with a neural network and a dot product.  The part of the GP that the neural network is approximating is the part of the GP that doesn't need access to training labels, so one might reasonably assume that when training this network, we don't need access to the training labels.

If true this is *very* surprising to me -- it effectively means that everything but the last layer of a network can be trained without labels (somehow), with labels only being used in the last layer (the dot product with the weights).

If this actually performs as well as ordinary supervised training regimes, then (depending on how one would actually train ""f"") it may open the door to a rigorous way to do unsupervised training.

I guess I'm curious if (and how) this has been tried and whether this has any implications for training ordinary neural networks.",NA
"1368",1367,11,"A recurrent neural network with the right weights (and infinite numerical precision) can execute arbitrary computer programs. Sorting, numerical integration, Google search, compiling code, you name it. It's not necessarily easy to learn those things, but in theory you can run them all. 

A Gaussian Process can only do regression of N->M dimensional functions. As far as I know it can't approximate arbitrarily large sequence information without infinite data",NA
"1369",1368,11,"Depends on the kernel function, really.

Take *three* 100-dimensional vectors and calculate the Gramian matrix (using normal dot products), and then calculate its rank/effective dimensionality. It should be three (or below), right?

Keep increasing the number of vectors and the rank/effective dimensionality will keep increasing, until you reach 100, and then it won't go up anymore, right?

*That* is how you know that the underlying space has dimensionality 100. The highest rank the Gramian matrix can reach is 100.

Now repeat the same thing but calculate the Gramian matrix using an **RBF kernel** (like the Gaussian kernel) instead of a linear dot product.

Do it with 3 vectors, do it with 100 vectors, do it with 1000 vectors... and keep going. Does the rank of the resulting Gramian matrix ever reach a limit? Or is the limit of the rank of the resulting Gramian only limited by the number of vectors you have?

What does that tell you about the dimensionality of the underlying space?

Now try doing the same with a **polynomial kernel** instead of an **RBF kernel**. Do you ever reach a limit in the rank of the Gramian or not?",NA
"1370",1369,11,"Is it that surprising that the weights can't/don't change in an infinitely wide layer, and are data independent? It's not surprising to me at all.",NA
"1371",1370,11,"I'm not sure how Turing completeness is relevant to the discussion. The work in question here is based largely on standard NNs and its connection to what I presume are standard GPs. Regardless, comparing them is like comparing apples and time machines. Sidebar: there have been recurrent GPs proposed which I expect would have analogies to the Turing completeness results for RNNs.",NA
"1372",1371,11,"You're making a category error here by presuming you can't construct a recurrent GP. A recurrent neural network is really just a sequential series of N->M operations. A GP should in theory be as able to generalize the function encapsulating the recurrently applied procedure as a recurrent network. It's certainly not a common application and would probably be a headache to train, but in theory it should be possible. 

A GP is just a distribution over functions, and an RNN is just an iteratively applied function. The exact same way that an RNN has a ""current value of hidden state"" output, a GP could as well.",NA
"1373",1372,11,"I was just trying to point out one way in which one could quantitatively defend the claim ""neural nets are a more complex class of model"". 

I also doubt that recurrent gps are Turing complete unless they have some kind of internal state for memory",NA
"1374",1373,11,"So, I was trying to keep it simple because I was responding to someone who hadn't done any theory of computation. But recurrence isn't actually the issue here. There are plenty of iterative function approximators that not Turing complete.  The issue is memory. Without infinite memory, you can only compute regular languages.  Recurrent neural networks with rational weights and infinite precision can use their hidden state to emulate an infinitely long memory, using matrix multiplications and activation functions to manipulate this memory. *Maybe* it's possible to do this with a GP, but it's really non trivial and requires a substantial proof to show you can emulate a Turing machine with only a finite description length for your function. People have shown a wide variety of neural network architectures to be Turing complete (RNN, Transformer, Differentiable Neural Computers). 
I have seen no such proof for Gaussian Processes.",NA
"1375",1374,11,"Your point about memory capacity is interesting and something I hadn't considered. My angle here is that I think a turing machine can be treated as a bijection from (current state) -> (next state), and a GP should be able to emulate any bijective function. 

I'll be thinking about this more later today, thanks for the food for thought.",NA
"1376",1375,12,"If you happen to work with BERT or use other attention-based approaches, [this excellent TDS article](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1) may be of interest. They mention the viz package [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization)",NA
"1377",1376,12,"Interactivity? Definitely Plotly. Of course you can also use Seaborn and Matplot but they are not interactive as far as I know",NA
"1378",1377,12,"Plotly is great, can be a bit buggy at times but all in all is great. Also look at streamlit especially if you’re experimenting with data and wanting to find trends in it etc",NA
"1379",1378,12,"To echo other recommendations plotly is the best for visualization. I was at a conference where a data scientist from RTI International talked about visualization of NLP. His methodology can be found here: https://modern-text-exploration.netlify.com/",NA
"1380",1379,12,"You'd have to be more specific...

But in general, and just to add to what other people have been writing here, you could have a look at [bokeh](https://docs.bokeh.org/en/latest/index.html)",NA
"1381",1380,12,"To be honest - I'd like to make some visualization of clusters that consist of TF-IDF vectors. So I need to reduce dimensions and show the audience 3D clusters that can be rotated in real time.",NA
"1382",1381,12,"Pyldaviz has some tools for that.",NA
"1383",1382,13,"This is pretty good, just curious will you guys moving to the podcast platforms such as Spotify or Apple Podcast?",NA
"1384",1383,13,"Sending love, will be closely following. Consider Google Podcast app or the iOS counterpart, you'll get bigger audience there (and I will receive notifications about new episodes on the phone :)).",NA
"1385",1384,13,"Thanks for sharing",NA
"1386",1385,13,"Put it on Pocket Casts, please :)",NA
"1387",1386,13,"Listening!",NA
"1388",1387,13,"I would love to see a transcript of the podcast(s)! :-)",NA
"1389",1388,13,"yep, it should already be on apple podcast! Hopefully... this Podbean thing is supposed to get it out to all the major platforms.  


Edit: oh oops, nope it's not -- going to set it up asap!",NA
"1390",1389,13,"You can add the podcast to the podcasts app via its URL, even if it’s not actually listed in the podcasts app",NA
"1391",1390,13,"Thanks! In theory this Podbean thing is supposed to get it out to all the major platforms, will try to get it confirmed soon.",NA
"1392",1391,13,"Our pleasure!",NA
"1393",1392,13,"Will do! I think it's being indexed right now actually.  
I actually use that one, if you copy paste the RSS link into search field you can subscribe that way right now :)

[https://feed.podbean.com/aitalk/feed.xml](https://feed.podbean.com/aitalk/feed.xml)",NA
"1394",1393,13,"Thanks!",NA
"1395",1394,14,"1) To transfer model to TF you need no covert Pytorch -> ONNX -> TF
2) More data is usually better even if it is negative classes. You can try to set higher weight for pixels that are false positive or false negative for people class.
And notice that COCO license allows usage for non-commercial usage.",NA
"1396",1395,14,"Thank you for the info.

I have checked online the license problem, looks like no one is sure if the non-commercial usage covers either only the dataset itself or also a model trained on it. But what about the plenty of pretrained models built in Keras or other frameworks? It would be terrible if they weren't free to use.

BTW, as more data is always better, a better idea can be fine-tuning the model on a custom dataset, I think.",NA
"1397",1396,14,"Fine-tuning is absolutely way to go to improve quality. I have been working with COCO-Stuff several last months and it lacks close portrait shots, bad lightning conditions and number of images with open hand palms with fingers separated from each other is somewhere near 0.
Also I had better transfer learning results by fine-tuning with same initial LR as COCO training.",NA
"1398",1397,15,"Title:Out-of-Distribution Generalization via Risk Extrapolation (REx)  

Authors:[David Krueger](https://arxiv.org/search/cs?searchtype=author&query=Krueger%2C+D), [Ethan Caballero](https://arxiv.org/search/cs?searchtype=author&query=Caballero%2C+E), [Joern-Henrik Jacobsen](https://arxiv.org/search/cs?searchtype=author&query=Jacobsen%2C+J), [Amy Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+A), [Jonathan Binas](https://arxiv.org/search/cs?searchtype=author&query=Binas%2C+J), [Remi Le Priol](https://arxiv.org/search/cs?searchtype=author&query=Priol%2C+R+L), [Aaron Courville](https://arxiv.org/search/cs?searchtype=author&query=Courville%2C+A)  

> Abstract: Generalizing outside of the training distribution is an open challenge for current machine learning systems. A weak form of out-of- distribution (OoD) generalization is the ability to successfully interpolate between multiple observed distributions. One way to achieve this is through robust optimization, which seeks to minimize the worst-case risk over convex combinations of the training distributions. However, a much stronger form of OoD generalization is the ability of models to extrapolate beyond the distributions observed during training. In pursuit of strong OoD generalization, we introduce the principle of Risk Extrapolation (REx). REx can be viewed as encouraging robustness over affine combinations of training risks, by encouraging strict equality between training risks. We show conceptually how this principle enables extrapolation, and demonstrate the effectiveness and scalability of instantiations of REx on various OoD generalization tasks. Our code can be found at [this https URL](https://github.com/capybaralet/REx_code_release).  

[PDF Link](https://arxiv.org/pdf/2003.00688) | [Landing Page](https://arxiv.org/abs/2003.00688) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.00688/)",NA
"1399",1398,15,"From reading the abstract this sounded pretty similar to the IRM framework proposed in 2019 and indeed it follows along those lines. I'm really interested in this line of work but finding that coming up with a novel improvement is really difficult.",NA
"1400",1399,16,"It's clearly unethical, which makes me wonder how many more self-promoting contents are in Wikipedia?",NA
"1401",1400,16,"This happens all the time. I'm in signal processing and there are numerous example of this. There are topics where I would be counted among the world's top authorities and let me tell you that there are many pages on those topics where the papers references and methods mentioned are certainly not the state of the art and sometimes the contents is wrong. I also once caught a colleague tooting his own horn on wikipedia (using my lab's name in the process) and I let him know how I felt about it.

A somewhat amusing story (but not in ML) is the wiki pages for mp3. I talked to James Johnston at the Asilomar conference in 2007 where he was presenting a paper entitled ""Perceptual Audio Coding - A History and Timeline"". For those that don't know, he's one of the two inventors of mp3. He told me the reason he wrote the paper was that the wiki pages were wrong and every time he tried to correct them, he was asked ""what do you know about it?"" and his changes were rejected.

Edit: Typo",NA
"1402",1401,16,"If you are close enough with Wikipedia, you can notice that it's full of politics, personal preferences, ads ranging from SEO to, well, academics, etc. IT and is branches is not that bad - people familiar with those topics are computer-savvy (shit that sounds so 90s), usually can inspect sources or detect spam. But step into any other science and you'll be quickly drowned with tons of BS. From the perspective of my field - biology - Wikipedia entries are often worse than high-school textbooks in terms of both factual correctness and educational clarity. And I'm not even comparing it to academic textbooks or, you know, actual encyclopedias. </ end of rant >",NA
"1403",1402,16,"This is tangentially related, so thought I should share:

Just last night I saw, in the Wikipedia article on the Free Energy Principle (FEP), it's claimed that, ""AI implementations based on the active inference principle have shown advantages over other methods"". This has one citation, which links to a WIRED article, [The Genius Who Might Hold The Key To True AI](https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/). This article tells us, ""[Friston] often sends people to [FEP's] Wikipedia page"" when describing the FEP, but it doesn't lay out these supposed improved RL algorithms in any real detail. I'm not sure if this counts as another instance of self-promoting. I've looked for FEP in RL (last year) but couldn't find anything; maybe others have found something.

It's frustrating, because now my research supervisor wants me and others to investigate the application of the FEP to RL. The FEP, though, *is* a controversial topic.",NA
"1404",1403,16,"https://en.m.wikipedia.org/wiki/Wikipedia:Criteria_for_speedy_deletion#A7
Ask for speedy deletion",NA
"1405",1404,16,"Great job.

Unfortunately, there are way more self-promoting companies, researchers, and individuals influencing Wikipeida than moderators and people like you who uncover such unethical behavior.",NA
"1406",1405,16,"I recently heard of ""services"" that own, author, watch Wikipedia pages for you and immediately revert any changes made by anyone. These services, apparently, cost anywhere between $600 and $$$$$$ every year, based on the popularity and demand. I even read one could write supporting or opposing a particular point or person or entity, create supporting references, related articles, news reports even.

We are living in a free world, I suppose.",NA
"1407",1406,16,"I think the real crime is that ""natural gradient descent"" (which the referred-to paragraph should be about) is actually a technique that has actually (and rightfully) garnered some research attention.

With that said: I agree that this ""kSGD"" sounds like some author just wanted to popularize his own work.Nothing per se wrong with that, but probably against Wikipedia's guidelines.",NA
"1408",1407,16,"This is nothing new. It’s the nature of Wikipedia. Every so often there will be someone who is famous who doesn’t like what someone wrote. Or will add their own spin on things that may not be factual because anyone can write anything. 

Also it’s crowd sourced knowledge which can be powerful but also abused. Think how FB or google data is being used to manipulate. 

It’s also more accurate for the subjects about comp sci since those are the people more comfortable with using it. However I think as time goes one this will change.",NA
"1409",1408,16,"I invented a thing and tried to add a page to Wikipedia.  Did a full page with citations etc., all genuine stuff but it was defo self promotion but with the idea I would maintain the page and keep it factual.

It was rejected and now the page exists but it isn't very factual.  Point is, all content has an element of self or heard promotion, it's up to the Wikipedia mods to maintain it.  I have faith in Wikimedia to be honest...",NA
"1410",1409,16,"

Tbh i sympathize with him. If you are a famous researchers you will get enough attention even your hand guester will contribute. it's very hard for you to accept that you have put your hard and time in a work nobody read. 
Other than that he did his share of evil. 

But the post makes me wonder more about what could be a solution  to prevent such exploitation",NA
"1411",1410,16,"This is one person adding some citation. Unethical sure. But this is just a tip of the iceberg. There are whole fields that are actively engaged in editing wikipedia.

Take for example the field of feminist economics. In Wikipedia the page for [feminist economics](https://en.wikipedia.org/wiki/Feminist_economics) is almost equal in length to that of [economics](https://en.wikipedia.org/wiki/Economics). Then, if you go to the economics page - there is a section called [criticism of economic theory](https://en.wikipedia.org/wiki/Economics#Criticisms_of_assumptions), with criticism from the ""feminist economics"" being the largest in size. However, if you go to the ""feminist economics"" page, there is no criticism section. But you can open up the ""discussion"" tab and find this:

> I don't think this is necessary here. Criticism sections can actually compromise the neutrality of articles (see WP:CRIT).

So they even find Wikipedia rules to defend their double standards. And this ""feminist economics"" clique is just the tiny tip of the iceberg. Just within the feminist circle you can find articles about [feminist geography](https://en.wikipedia.org/wiki/Feminist_geography), [feminist biology](https://en.wikipedia.org/wiki/Feminist_biology), [feminist scientific method](https://en.wikipedia.org/wiki/Feminist_method), etc, that are all edited by the same group.

Another interesting thing to observe is how the Wikipedia pages for certain people and personalities are edited by political cliques in order to smear someone else associated with them. Here is an example I am familiar with (by having read 5 books from the author before this all started) - the page for ""Julius Evola"" [before](https://en.wikipedia.org/w/index.php?title=Julius_Evola&oldid=673273242) and [after](https://en.wikipedia.org/w/index.php?title=Julius_Evola) a guy named Steve Banon mentioned him in the media. They even changed the photo to make him look meaner.

And it goes way deeper than that. Honest volunteers are few and people with agendas are many. I think encyclopaedia that is open for edit from everyone wasn't really such a great idea.",NA
"1412",1411,16,"I agree with you, and I recommend that you go ahead and remove that section, with short explanation of non-notability and possibly self-promotion, as the current paper has only 20 citations.

If this user is watching the article, then they will possibly revert these edits which may possibly lead to an edit war. Following the [recommendations on managing edit wars](https://en.wikipedia.org/wiki/Wikipedia:Edit_warring#Handling_of_edit-warring_behaviors), add a section in the discussion page outlining your concerns, and then edit the article directly, adding an [inline template questioning the notability of the section and redirected users to the talk page](https://en.wikipedia.org/wiki/Wikipedia:Template_index/Cleanup#Importance_and_notability).

For the talk section, you'd want to reference non-notability as the article has only 20 citations and mention the possibility of [WP:COI](https://en.wikipedia.org/wiki/Wikipedia:Conflict_of_interest) and [WP:NOSALESMAN](https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_a_soapbox_or_means_of_promotion) that you mentioned here.

In the talk page, you may want to get a [third opinion](https://en.wikipedia.org/wiki/Wikipedia:Third_opinion) and possibly an [RfC](https://en.wikipedia.org/wiki/Wikipedia:Requests_for_comment#Request_comment_on_articles,_policies,_or_other_non-user_issues) if that fails, hopefully so that an experienced user can step in and help through the process. At this point, the neutral party would have the best advice stepping forward. If the user persists in reverting the article, then this third party may be able to help get administrative action against the user.",NA
"1413",1412,16,"That action of vp314@ is definitely not cool. However, I personally don't think it is unethical.

What if vp314 **actually** believed that their work is good and is worthy for the community to know of, and so they added it to Wikipedia? Let's say after someone else removed their edits, vp314, with their firm belief, tried adding it again. There is no difference between that and other a reviewers/authors rebuttal session, as we all see in our broken peer-reviewed conferences.

Also, does Wikipedia have policies that disallow self-promoting actions? Speaking broader, is self-promotion bad? I personally dislike self-promotion -- I am very much annoyed by people tweeting ""Excited to have our banal-topic-but-pompous-name paper accepted to NeurIPS 2019"". As much as I dislike those actions, I don't see any ethical concern about them. Not cool, for sure. Not fair, for sure -- as some people have more followers on Twitter, just like some users have stronger editing powers on Wikipedia. But not something to be criticized as unethical.

Lastly, Researcher/Professor/everyone needs to get salary, promotions, etc. and some needs fame too. Without these incentives, we probably wouldn't have deep learning today. The internet serves their purpose, and there is nothing wrong about that. I dislike that; you probably also dislike that; but it is **not** wrong.",NA
"1414",1413,16,"Of course I still love you Wikipedia.",NA
"1415",1414,16,"Although I don't disagree that this kind of thing is unethical, the thing about wikipedia that makes small/rarely visited pages vulnerable to abuse is also the thing that makes big/popular pages resistant to abuse. In short, if the ideas promoted on that page ever actually start becoming influential, people like you will start to dig around and eventually discover the fuckery and correct it. 

Again, I'm not saying what he did was right. I'm saying that the nature of wikipedia is that all pages start off heavily biased and low-quality, but they also trend toward less bias and higher quality as the page receives more and more attention. This post right here is a great example of that process (well, it is insofar as the fuckery you've discovered actually ends up being corrected).",NA
"1416",1415,16,"The author is violating Wikipedia's self-promotion policy:
**(WP:SELFPROMOTE)** https://en.wikipedia.org/wiki/WP:SELFPROMOTE

**BE BOLD**
https://en.wikipedia.org/wiki/Wikipedia:Be_bold

Go ahead and delete the self promotional text.",NA
"1417",1416,16,"If you've ever played with Wikidata dumps you'd find that 99% of the pages on Wikipedia are complete garbage about research papers, mountains and plant genes that get added by bots. tbh I'd take the shameless self-promotion",NA
"1418",1417,16,"Unscrupulous",NA
"1419",1418,16,"In general? There's no blanket ban on people referencing their own work [within reason](https://en.wikipedia.org/wiki/Wikipedia:Conflict_of_interest#Citing_yourself) if it merits inclusion.

In this case, though, it sounds like that inclusion was debatable at best, or it wouldn't have been removed. Re-adding it after that, without discussion, would definitely be unethical behavior.",NA
"1420",1419,16,"Astroturfing in tech/sci articles is rampant in wikipedia, especially in tech-opaque areas. In some articles there is constant war of pushing in/pulling out ads for proprietary solutions disguised as technology examples or applications of technology. But that's nothing comparing to mud slinging going in articles about controversial historical events.",NA
"1421",1420,16,"While it may be, as the essay wrote, the person who inserted their paper in Wikipedia is for some personal gains, I would like to caution that there are many things that should be considered before some judgement is declared. Before going on, I clarify that I have no relation whatsoever to any of the people/references mentioned in the Wikipedia page. Also, I did not read the paper, but I know many of the other methods mentioned in the Wikipedia page, and I work in Optimisation, particularly for Deep Neural Networks.

Null, if the writing adds new information and improves our knowledge, then why can it not be added, regardless of gains or no gains?

First, Wikipedia is free and fair for every people, so if the person has evidence for their claim (here a paper), why should they be prevented from adding it in Wikipedia? Can you check whether it is true/not true in the paper they cited yourself, instead of basing on outside judgments (such as other people's opinions, number of citations and so on)? Can you, for service, show us what is wrong with the results/methods in the paper?

Second, many of the other methods named in the Wikipedia, such as - as you named RMSProp and Adam - are only heuristic. Do you have better evidence to support them than the mentioned paper?

Third, the methods added before the paper in the Wikipedia, are you sure they are not for some personal gains? Should we judge all written in that Wikipedia page, and other Wikipedia's pages, on the same footing?

Fourth, could this attitude help with preserving the status-quo?",NA
"1422",1421,16,"**If the author of the paper cited true facts, thank him/her.**

>not that citations are the measure of good science

From there, there should be no more to say.

>My instincts are what this person has done is wrong and taking advantage of Wikipedia

You let jealousy speak for you. This person did not deprive wikipedia of anything. So by no means his/her content should bother you.


>I would love to hear some other perspectives (and maybe get a little less angry). Is there a defensible reason to do so?

Wikipedia is a collaborative encyclopedia. There is nothing wrong with adding content as long as this content is referenced, correct and not out of subject. If it's not correct, the best thing is to edit the page or open a debate about it. But any content that is 100% true does not have to be deleted and even should not because this is just more knowledge. 


>""Its appearance in Wikipedia made it look like an established technique, which is not""

His/her belief of what is wikipedia is what makes it look like an established technique. Instead, thank all authors who write less popular content because this is the only way to get out of your knowledge bubble.

On top of that, you're talking about a science subject, so any content that is 100% logical is scientifically established. If people want to make wikipedia a website where you can find everything about gradient descent methods, these people should rather add content on the page instead of deleting some.

Citations have nothing to do with science. Science is about knowledge. Promotion through citations, prizes, trophees, grants and whatever is politics only. 

**If the author of the paper cited true facts, thank him/her.**

>the only edits that user has done on Wikipedia are related to adding their technique to the Wikipedia page on SGD

The author is not in charge of all the content of wikipedia so this remark leads nowhere. When you write something on wikipedia, you write about what you know best (if not, stop writing). And, above all, you don't write about what is already written (here, the basics of gradient descent). So it absolutely makes sense that this user writes about their technique.",NA
"1423",1422,16,"That's one scab you probably don't want to pick at. 

Not going to name the person as it would be breaking HR guidelines. I had someone interview who said they were well renowned in their field, which surprised me when they couldn't answer basic questions someone in that field could.

They had a wikipedia article which after much digging was completely fabricated by one person (99% sure them). It linked off to articles on different sites as sources which were from the same person, and two accounts making edits and voting for no deletions.",NA
"1424",1423,16,"Tons. In India apparently there's an academic mafia controlling Wikipedia that will edit and mess up your reputation unless you pay them to be in the club.

Take tools to corrupt countries, and... expect corruption to just magically dissapear?

https://twitter.com/Soumyadipta/status/1235098631738281984?s=19",NA
"1425",1424,16,"Sounds like a research paper worthy of a wikipedia mention to me...",NA
"1426",1425,16,"You see this kind of thing all the time if you get off the beaten path.",NA
"1427",1426,16,"A very significant amount. Governments, special interest groups, companies, religious organizations, etc. are all willing to spend real money to maintain information information on wikipedia deemed beneficial to themselves. Thanksfully, over the long run, this seems to just lead to more sources and quality for popular articles, but if the article is not very popular and there aren't a lot of different people representing different interest groups to pick apart and debate these inclusions, they can stay in for quite a long time.

Now, is it actually unethical? I'm not so sure. If you're an expert on an algorithm or technique, then there's no reason why you shouldn't write the initial article on it. It's unethical to knowingly and intentionally twist the facts, but is it wrong for an environmental group to hire a team of scientists to combat the spread of misinformation about climate change on wikipedia? Is it wrong for Microsoft to make regular updates to the Microsoft article as the company evolves? It's a not-for-profit educational resource, which does mean there are some ethical there. But I'm not entirely sure where it is you should draw the line.",NA
"1428",1427,16,"The history section on the deep learning page has definitely been heavily edited by folks in Schmidhuber's camp.",NA
"1429",1428,16,"Why is self-promoting unethical? If self-promoting is actually unethical, shouldn't we ban all researchers who tweet things like ""Excited to see our paper accepted at NeurIPS 2019""?",NA
"1430",1429,16,"The case of ""Jagged 85"" is a great example of the issue. He had 67 000 edits pushing his beliefs for years when he was banned for spreading gibberish. On the surface, his edits looked well cited but once you opened them and started reading - it became clear that they didn't support his claims.

His damage will likely last a very long time since no one will go through and validate all his edits. On top of this, he is just a single individual that came on the radar for being so over the top. How many similar accounts are there?",NA
"1431",1430,16,"Wikipedia is a great lesson in the value of good institutions.   Politics, processes and bureaucracy are not jut dirty words. You need them and you should design them well. 

Long live [Scholarpedia](http://scholarpedia.org/article/Main_Page).",NA
"1432",1431,16,">  IT and is branches is not that bad

While nowhere as bad as the articles involving political issues, niche IT topics have problems with poor quality, poor sources, or random ""not notable enough"" drive-by deletion by fuckwads.",NA
"1433",1432,16,"Wikipedia as a whole is a beautiful thing in concept. It just desperately needs more moderation overall at the micro level.",NA
"1434",1433,16,"Not gonna comment on the wikipedia thing because I know nothing of it (don't like using it tbh, and if I do is to look up the sources), just wanted to say that in markdown language, </field> is the end of the field, so </end of rant> means end of end of rant 😅.

Edit: remind me to when people would do stuff like ""TIL I learned"" or ""RIP in peace"".",NA
"1435",1434,16,"I've been trying to read papers about RL implementations of FEP recently, like the one called ""Deep active inference as variational policy gradients"". It's a preprint from last year. The maths seemed a bit dodgy, I think there were a few mistakes and a bunch of things that just weren't defined. Made it very difficult to follow. That paper does cite a couple of other attempts that I haven't looked into.",NA
"1436",1435,16,"Why can't the section just be removed by anyone? The SGD article isn't protected or anything.

A quick glance at this this ""speedy deletion"" thing seems more for deleting whole articles, not for normal edits? (I might be unfamiliar with something though)",NA
"1437",1436,16,"Simulacra and simulation",NA
"1438",1437,16,">level 1hyhieu2 points · 45 minutes agoThat action of vp314@ is definitely not cool. However, I personally don't think it is unethical.What if vp314 actually believed that their work is good and is worthy for the community to know of, and so they added it to 

Here are the guidelines researchers from David Eppstein, one of wikipedia administrators -- [https://en.wikipedia.org/wiki/Help:Wikipedia\_editing\_for\_researchers,\_scholars,\_and\_academics](https://en.wikipedia.org/wiki/Help:Wikipedia_editing_for_researchers,_scholars,_and_academics) . 

""""""If you develop a reputation as a [self-promoter](https://en.wikipedia.org/wiki/Wikipedia:SELFPROMOTE), you are likely to get yourself [blocked](https://en.wikipedia.org/wiki/Wikipedia:BLOCK) as an editor and your contributions [undone](https://en.wikipedia.org/wiki/Wikipedia:UNDO) or [deleted](https://en.wikipedia.org/wiki/Wikipedia:DPR). As in academia, [conflicts of interest](https://en.wikipedia.org/wiki/Wikipedia:Conflict_of_interest) must be declared.""""""",NA
"1439",1438,16,"This comment gets a -7 now. This really worries me about what those who voted down believe science is about.",NA
"1440",1439,16,"When I was an undergrad, my flat mate edited wikipedia saying he invented Tennis. Now, unfortunately, there are a few articles and even a book (thankfully not a good/popular one) citing that he invented Tennis...",NA
"1441",1440,16,"What... did... I... just... read...?!",NA
"1442",1441,16,"No sources, and the guy is pissed that people are editing Modi's page with criticism? Totally lost me, Modi is literally a fascist and responsible for the violent persecution of non-hindus. This reads more like some alt-right conspiracy than an ""academic mafia"".",NA
"1443",1442,16,"Hello Vp314!",NA
"1444",1443,16,"Schmidhuber was super active.",NA
"1445",1444,16,"An important distinction is whether it is clearly recognizable as self-advertising or not. Same as for ""regular"" advertising.",NA
"1446",1445,16,"Typically, as in your example, it’s not. It is unethical to self-promote on Wikipedia where it will be taken as well-established knowledge by readers",NA
"1447",1446,16,"I thought so too and invested some time into it. However, in many instances the problem was totally the opposite - moderators or even top user with too much power, shaping their favorite topics according to their private beliefs, reverting sourced edits, bombing constructive discussions etc. In that way, for instance, wikipedia ended up with complete mess in biological taxonomy, because there are micromoderators who enforce their personal depiction of taxonomy (irrespective of scientific pubs and taxonomic orgs).",NA
"1448",1447,16,"Thank you! I'll start there, and with its citations.",NA
"1449",1448,16,"If the article is not locked due to an edit war everyone can edit. If there is an edit war, there are procedures for mediation.",NA
"1450",1449,16,"Ask user User:Kiefer.Wolfowitz to do that in his talk page if you do not want to delete the section yourself.",NA
"1451",1450,16,"While funny, there's not many ways he could benefit directly from this, compared to this situation",NA
"1452",1451,16,"From my own experience: In some Eastern European countries for example, teaching academics at the universities are seen as one of the most corrupt groups after police and customs. A mail with some money in it before a test is quite ""normal"" there from what I've heard from students and foreign researchers. So no surprise here.",NA
"1453",1452,16,"This kind of thing has happened on the Swedish wikipedia as well.

There's a motor journalist at Swedish newspaper called Dagens Nyheter who is notable enough to have a wikipedia page. This motor journalist stabbed a guy to death while two of his friends kept the guy from running outside a restaurant called Berns in the 1980s. He got a very light sentence, curiously enough, for manslaughter and was only in prison for nine months.

However, when people wanted to add that to his Wikipedia article, with sources, they were rebuffed, and in fact the article has been protected for eight(!) years.",NA
"1454",1453,16,"Developing countries have mind numbing levels of corruption that you may not have encountered in cushy first world environments or upbringings.

Added link
https://twitter.com/Soumyadipta/status/1235098631738281984?s=19",NA
"1455",1454,16,"Leave it to a Chapo poster to bring up politics in an ML sub.",NA
"1456",1455,16,"Looks like you didnt read his threads. 

Hes saying there are mafia moderators who wont allow any criticism on Modi wiki.

Hes NOT mafia.",NA
"1457",1456,16,"he came up with GANs in 1991, didn't he?

I'm realising now everything I could write will be seen as 'being in his camp'. a bit depressing tbh",NA
"1458",1457,16,"> where it will be taken as well-established knowledge by readers

To be fair wouldn't that be rather dumb. Using Wikipedia is great but if you use it to establish facts, well that seems rather dodgy.",NA
"1459",1458,16,"Interesting. I feel as if the whole thing could eventually come full circle into a typical encyclopedia in an effort to curb all of this.",NA
"1460",1459,16,"To be fair it was the twitter OP who was making an overtly political point, which really had no references to ML at all either. I'm just calling it like i see it.",NA
"1461",1460,16,"The twitter thread brought up politics first.

> They're staunch Leftists and their job is ensure that Wikipedia doesn't say nice things about non-left personalities and media

Also he is claiming that these top editors make 5 lahks a month doing this.  [Which is apparently $500,000 a month.](https://www.google.com/search?client=firefox-b-1-d&q=5+lakhs+to+usd)

but also that

> The top 50 editors are mostly from IT companies with much free time on their hands and they are on Wiki the whole day.",NA
"1462",1461,16,"I think the majority of the users reading articles on Wikipedia assume a certain amount of credibility with the importance Wikipedia has gained over the past decade or so. Which means they most likely won't scrutinize all the sources and take information at face value.

Imagine an established and previously reputable newspaper started slipping in a couple of articles without proper sources. I'm not sure the majority of people would notice, especially on a topic they probably had no previous knowledge of.",NA
"1463",1462,16,"500 lakhs in rupee.
Not $",NA
"1464",1463,16,"Rupees, not dollars. It's only like 6 or 7 thousand dollars. Nothing in the thread seems far-fetched.",NA
"1465",1464,16,"Yeah... I'm sure there are tight-knit groups of frequent contributors for various subjects on Wiki, and I'm sure their politics tend to be left leaning. The idea that they're an evil cabal extorting innocent doe-eyed newcomers to the field seems far-fetched to me, and that's if we ignore the fact that this guy obviously has an agenda.",NA
"1466",1465,16,"No its not. 5lakhs is 6.7k",NA
"1467",1466,16,"> Imagine an established and previously reputable newspaper started slipping in a couple of articles without proper sources.

What do you mean imagine ? I can remember the NYTs during the Iraq war. 

> I'm not sure the majority of people would notice, especially on a topic they probably had no previous knowledge of.

The difference being that Wikipedia is community edited so I am not sure if you should expect they are credible at all. The aggregation of information is often useful.",NA
"1468",1467,16,"The way I see it is, people (and a huge number of them) visit Wikipedia because it has historically built up credibility thanks to its moderation of the editing process. And for the most part it is true; a lot of the information on there is accurate and a good summary of the topics discussed. It is often the first site (at least for me) used to learn about a new thing. If its contents had no credibility, or if people's perception of the site is bad, people would not be using it in the first place.",NA
"1469",1468,16,"But people’s perception shouldn’t really factor into a researchers decision about credibility. Millions of people read tabloid news papers (in the UK) using popularity as in indication of reliability is pretty bad I think. 

By all means use Wikipedia but i think you should be sceptical of it most of the time.",NA
"1470",1469,17,"Search this subreddit and you will find posts about this.",NA
"1471",1470,18,"I'm no expert on this topic,  but I know that the VowpalWabbit folks are such experts. I suggest you get in touch with them through glitter. They are really keen to help on such things. VW is really powerful and super fast!",NA
"1472",1471,18,"Is there an overlap between the batch at time t and the one at t - 1? If not then what you're doing is correct. Again, I'm assuming the CTR is computed only per batch and does not take into account history. The UCB algorithm will take care of history for you.",NA
"1473",1472,18,"> I don't know if particular click has yielded a click or not.

What does this mean exactly?",NA
"1474",1473,18,"Thanks you for the advice!!",NA
"1475",1474,18,"Well actually, at the moment I'm doing a ""Sliding window"" aggregation meaning I'm aggregating the entirety of the counts for 12h by interval of 5 minutes.
So the data that goes as input to the MAB algo is the counts over a period of 12h (not just the count of a 5 minute period)",NA
"1476",1475,18,"Oops sorry, I mean

I don't know if particular impression has yielded a click or not.",NA
"1477",1476,18,"So the output of your bandit algo is just based on the statistics from the last 12 hours?",NA
"1478",1477,18,"Yep, I chose 12h because I thought that each ads probability would change days after days so it would not make sense to keep to much history",NA
"1479",1478,19,"this is awesome; we're planning on launching with cortex very soon. however, GCP is still the ideal platform for us. (we moved to AWS expressly to use cortex.) any ETA on GCP support? thanks again!",NA
"1480",1479,19,"Interesting approach. 
My inference workers are celery workers, so I scale up new infra+workers by monitoring the length of the celery queue using the events framework. 

Disclaimer: I haven't read any Cortex code or docs, but why not use this sort of approach?",NA
"1481",1480,19,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [Request-based autoscaling for inference workloads (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fiadkl/requestbased_autoscaling_for_inference_workloads/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"1482",1481,19,"We did the same, monitoring the RabbitMQ queues, works really well. https://github.com/flowhub/guv",NA
"1483",1482,20,"Not so sure about that. The sigmoid will always focus on the center part, a ReLU on the positive part. The whole thing about attention is that where to focus is learned and is a function of some input",NA
"1484",1483,20,"Sigmoid and relu both focus on the right part. Focusing on the center would be something like a gaussian activation function (which i didn't see before but I think is used in some articles).

But I'm not sure how they can be seen as attention. Attention is commonly seen as focusing on one part of the input among several (for example one image region, or one word) and is usually implemented by giving a score to each part, then softmax this score, and average all inputs with those scores, giving you a weighted representation of your inputs. 
 
That doesn't have much to do with the activation, does it ?",NA
"1485",1484,20,"Attention is a kind of kernel regression. It's doing a kind of learning *in each forward pass*.  I haven't seen studies on it, but I'd wager it gets a significant amount of its power from that. Activation functions don't do that",NA
"1486",1485,20,"Attention is focusing on group of neurons, unlike activation which focusing on range of intensity signal.",NA
"1487",1486,22,"Maybe distributional approaches (like the recent [Fully Parameterized Quantile Function for Distributional Reinforcement Learning](https://arxiv.org/abs/1911.02140)) could give you a boost vs vanilla DQN",NA
"1488",1487,22,"This paper is on my to-read list: https://arxiv.org/abs/1807.02264  
I think they try to learn a baseline that is better than regular value functions because the value functions in their cases have high variances due to the stochastic nature of the environment.

You might also check out other papers from the same authors. I'm really a fan of their research.",NA
"1489",1488,22,"Instead of learning the value function learn the reward function and the state transition function. These can be done separately and should be easier to predict. Then compute the value function directly using: V(X) = R(X) + P(X) V(X). Then update policy using estimate of V(X)

Your method of solving V(X) = R(X) + P(X) V(X) will differ depending on situation but I like to use a sparse approximation of P(X) and then just compute ((I - P(X))^-1) R(X) = V",NA
"1490",1489,22,"This may be of interest: [Deep Variational Reinforcement Learning for POMDPs](https://arxiv.org/abs/1806.02426).

The stochasticity is primarily contained in the random loss of inputs due to flickering, but it seems naturally extendable into randomness in the mechanics - it's all just a form of incomplete information after all.",NA
"1491",1490,22,"I don't see why those algorithms would fail. DQN predicts the mean of the Q-value, but that doesn't preclude its use with stochastic algorithms. Atari implementations usually use \`sticky keys', which do make the environments stochastic, and these have been solved with DQN.

Having said that, it can be advantageous to model the distribution of the Q-values - see the field of distributional RL, e.g. [https://arxiv.org/abs/1710.10044](https://arxiv.org/abs/1710.10044) .",NA
"1492",1491,22,"\> I don't see why those algorithms would fail.

DQN assumes full observability. If that is not given, you have one common case of stochastic systems. Depending on how much that assumption is violated, it will fail royally on those.",NA
"1493",1492,22,"DQN as a regression is highly sensitive to target values noise. In highly stochastic environments the convergence speed is very slow. It's not exactly ""fail"" but becoming impractical due to huge number of samples required. Distributional DQN is more correct way to handle it, but they are not faster.",NA
"1494",1493,23,"I've been putting time into folding@home again. Stats page is down atm sadly :(",NA
"1495",1494,23,"Are there open datasets for COVID-19 incidence?",NA
"1496",1495,23,"Get it folks.  Save the world.",NA
"1497",1496,23,"Hey kudos do you guys",NA
"1498",1497,23,"If anyone has a team and needs some menial wrangling done, or wants some analysis done, I'll try to contribute hours where I can. Send me a PM.

I'm a tenured professor, with decent skills in R and modelling.",NA
"1499",1498,23,"Way back in 2017 I bought a 1080ti to do dEeP lEaRnInG research as a continuation of one of my internships. I tried to install tensorflow but apparently my drivers were too up-to-date for TF, so I ended up just mining for GPUGRID and Folding and never bothered doing research again, lol. 

If I’m not gonna do research then at least I can donate those resources to someone who is!",NA
"1500",1499,23,"Kaggle has one:  [https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset](https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset)",NA
"1501",1500,23,"Tableau has one available: https://www.tableau.com/covid-19-coronavirus-data-resources

Haven't checked for if it's different than what /u/khamisen linked.

Also an individual-case dataset being built up from Liquidata: https://www.dolthub.com/repositories/Liquidata/corona-virus/query/master?q=select%20*%20from%20mortality_rate_by_age_sex

Basically getting these as they come through from Jeremy Howard's Twitter feed.",NA
"1502",1501,23,"A small dataset of chest X-ray images and CT scans: https://github.com/ieee8023/covid-chestxray-dataset",NA
"1503",1502,24,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [Deadline extended for the FDA Open Data Adverse Event Anomalies Challenge (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fiadk5/deadline_extended_for_the_fda_open_data_adverse/)

- [/r/datascienceproject] [Deadline extended for the FDA Open Data Adverse Event Anomalies Challenge (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fit3k3/deadline_extended_for_the_fda_open_data_adverse/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"1504",1503,25,"You’ll probably get a lot of responses saying, ‘you can’t help’ - and directly, right now you probably can’t. Just wash your hands for 20 seconds and avoid public places and don’t touch your face.... but, in line with your community spirit let me be encouraging rather than negative...

Whatever you do now may not help today but could help tomorrow, or more likely next year/next pandemic. Think long term. You will not use ML on your home computer to discover a new wonder drug for Covid-19, even if you have training in this area (then you would use your research facilities anyway). You won’t improve on current epidemiological models, unless you find a new data feature - maybe there is something to discover there. 

My advice is poke around, be creative. Mine data sources out there. Don’t expect to crush this pandemic with your work today. There is much to learn, but it will be for next time. Specifically I think a lot can be learnt after the fact... what stopped transmission? What forms of communication to the public worked best? What predicted life/death... anything beyond the age/health angle? Maybe you learn nothing specific but you’ll learn how to deploy your technics for next time. And while we should right now be mindful of social contact, you should not work alone on problems like this. Find groups. Jonás Salk didn’t eradicate polio single-handedly. That is just how legends are told. It’s not how work is done. 

Good luck, stay healthy and stay focused.",NA
"1505",1504,25,"https://foldingathome.org/2020/03/10/covid19-update/",NA
"1506",1505,25,"Hi there, I asked a similar question a couple of weeks ago, to a mixed response ;-) [https://www.reddit.com/r/MachineLearning/comments/fc3ie6/discussion\_what\_can\_we\_as\_ml\_practitioners\_do\_in/](https://www.reddit.com/r/MachineLearning/comments/fc3ie6/discussion_what_can_we_as_ml_practitioners_do_in/)

I think there is a lot we can potentially do, initially by reaching out in our network to other researchers who might need some quantitative skills for their own research projects. (not all biology departments will have ready access to a statistician or a machine learning engineer).

In terms of working on your own, there is population level data available, John Hopkins update their dataset used for this [visualisation](https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6) every day, its a good place to start working with the data: [https://github.com/CSSEGISandData/COVID-19/tree/master/csse\_covid\_19\_data](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data)

Kaggle have opened up a task around it [https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508&utm\_medium=email&utm\_source=intercom&utm\_campaign=tasks-award-march-2020](https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset/tasks?taskId=508&utm_medium=email&utm_source=intercom&utm_campaign=tasks-award-march-2020)

What would be really useful is if we had patient level data and could determine major risk factors to influence who we should encourage to self-isolate on a longer term basis as this virus will likely be around for a while.

Thinking further down the line there will likely be major disruption to businesses for the next while, so thinking of ways we could help them operate in a more restrictive environment could be an important area to look at.",NA
"1507",1506,25,">Are there public datasets or something 

Yeah:

http://predictioncenter.org/download_area/

The most accurate model is A7D (AlphaFold)

http://predictioncenter.org/casp13/zscores_final.cgi?formula=assessors

The code is available here:

https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13",NA
"1508",1507,25,"https://www.reddit.com/r/COVID19/comments/ferqv1/comment/fjtjn9z",NA
"1509",1508,25,"I've been through the John Hopkins dataset, and the one provided here:  [https://ourworldindata.org/coronavirus-source-data](https://ourworldindata.org/coronavirus-source-data)

WHO situational reports are off too:  [https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200313-sitrep-53-covid-19.pdf?sfvrsn=adb3f72\_2](https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200313-sitrep-53-covid-19.pdf?sfvrsn=adb3f72_2) 

They don't seem consistent with the results here:  [https://www.worldometers.info/coronavirus/](https://www.worldometers.info/coronavirus/) ... which begs to question where they get their data from. Official reports are 11 deaths in UK so their data seems correct.",NA
"1510",1509,26,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [StyleGAN2 notes on training and latent space exploration (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fiadkt/stylegan2_notes_on_training_and_latent_space/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"1511",1510,27,"Before you look at architectures, how are you going to evaluate performance? E.g. I'd start with predicting each of the labels independently and maybe that is good enough (language is certainly independent enough). But it doesn't sound like you have a ground truth to compare against, so you can't evaluate other architectures to see if they work better.",NA
"1512",1511,27,"That's good advice for any project.  I didn't include the evaluation metric in the description.   In our case, we have a very direct way to calculate business impact from a successful model, because the system is replacing an offshore team that do the labeling for us at a rate of n cents per labeling.  Effectively, we take the rough savings from that ticket volume and then discount it by the cost of reconciling false positives and false negatives in each label produced by the model to produce a single optimizing metric that our models can work against on the dev set.",NA
"1513",1512,27,"Ok assuming your training data matches what you will see in production (this is a huuuge assumption in text -- see ""philadelphia"") .....

roughly how much room for improvement is there, compared to the ""independent"" baseline? What AUCs are you getting? I haven't worked much with LSTM so I'm the wrong person to give advice, but it's generally a good idea to exhaust the simple options. You may also want to test BERT instead of fastText. I found BERT (large uncased) works better for classification.",NA
"1514",1513,27,"Thanks!  BERT is new to me, and as you say, its possible this problem could be as simple as unsequenced classification",NA
"1515",1514,28,"I think it is accepted that having good features is key to good performance. At latent space is a more compact and less noisy than the original input. In the autoencoder with reconstruction loss the feature representation is learned at the same time as the part that does the anomaly scoring. However such a feature transformation does not have to be learned end-to-end on the anomaly detection task. There is plenty of work in both image, NLP and audio that shows that it is possible to learn representations that transfer well across tasks. Sometimes these general representations are called 'embeddings'. One can then apply any anomaly detection using the embedding as features. Might still want to try a powerful non-linear model though. 

That said, today there are probably better models for the representation learning part than autoencoders. Self-supervised learning in particular seems to be very efficient. Or if pretrained classifiers are available, their internal feature representations can often be used with success.",NA
"1516",1515,28,"There is of course some risk that what was just noise in the pretraining task (and thus suppressed in the feauture transformation) is important signal in the anomaly detection task. The mentioned dot outside pictures with faces might be an example of that. I'd suspect that in many cases larger anomalies can still be traced though to the latent representation though, and might still be easier to find in the smaller subspace that original input. Have not seen any research on this topic though.",NA
"1517",1516,28,"Yeah, my guess was that this autoencoder anomaly detection method would only work for the outlier signal being some variation of a key inlier signal. Even then though, what is the guarantee that the latent representation would be separate from a variation in the inlier domain? An optimized encoding process would just learn to separate variations in the inlier data, thats all its given to train on. There is no guarantee that the encoder should learn to separate (properly represent) data outside the learned domain. What is the reason that an autoencoder would learn some type of ""null code"" or a way of encoding that a feature variation is outside the expected domain?",NA
"1518",1517,28,"The representation learning processes are not so good that that seems to be a practical concern. The representations are still overcomplete, redundant and extend a bit outside the exact instances in the training set (needed for generalization anyway). Widespread techniques like data augmentation increase this tendency. Anomaly detection is also characterized by modelling of inliers, not the inlier/outlier border (that would binary classification). Ie OneClass SVM versus regular SVM, and IsolationForest versus RandomForest.",NA
"1519",1518,29,"You NEED to present this at AIAI 2020 ( [http://easyconferences.eu/portfolio/aiai-2020/](http://easyconferences.eu/portfolio/aiai-2020/) )",NA
"1520",1519,29,"That's a truly terrible title. Animal agriculture, and in particular ""artificial insemination"" is an ethical nightmare. This of course doesn't have a whole lot to do with the actual point of Your article, but I figured You might want to know that some people may immediately recoil.",NA
"1521",1520,29,"Thanks for the suggestion! I know it's a difficult time for conferences, but I'll try to connect with organisers.",NA
"1522",1521,30,"I'm still pretty fresh, but it depends on the paper. Some I can read for an hour amd barely have an outline. Some I scan in 10 minutes and have a good idea of what is going on.

I can sometimes read a paper and built the architecture in a few hours, sometimes it takes a few days.

I'm way faster now than I was two months ago and I look forward to improving more.",NA
"1523",1522,30,"It depends on the paper. Yesterday I read the Fast and Faster R-CNN papers in a few hours and they were pretty straight forward. When I read the Wasserstein GAN paper, on the other hand, I didn't understand anything even after reading it for more than a day. Blog posts saved me there.

Some papers are just more complicated or, in some instances, just written confusingly. Looking for blogs, discussions or the reviews of the paper help a lot. Otherwise, try to find the code. These tips are obvious, I guess, but the takeaway point is to set back your ego and use secondary sources, too.",NA
"1524",1523,30,"I wouldn't worry about it too much, you get faster the more experience you have in a particular field.",NA
"1525",1524,30,"If it's a paper in my primary research area, it usually takes me 5-10 minutes to have a good picture of what's going on in the paper.

If it's a paper outside my main research direction by still in machine learning, it can take anything between 10 minutes to a few days depending on how complicated the theory part of the paper is.",NA
"1526",1525,30,"It depends very strongly on the paper and on how deeply it is to be understood.

A couple of hours to read and understand at a basic level. A full day is possible too. But to *understand*-- that can take months of reflection and require reading of many, many other papers.",NA
"1527",1526,30,"Fuck, man, I’ve been working on the same paper for 2 weeks. I know I’m slow, but I’d happily take a day.",NA
"1528",1527,30,"Reading different papers requires different levels of understanding. Sometimes you only read abstract and conclusion, sometimes you just need a quick look at the experimental results. Other times you need to deeply understand all the technical details.

The time required to perform these different kinds of reading vary by order of magnitudes (from minutes, to hours, to days).

&#x200B;

If your are reading a paper about a new architecture in a field you are familiar with, it should take at most a couple of hours, and after that time you should be able to comfortably implement their model.

However, if there are novel theoretical results, either in a field you are not familiar with or using mathematical results that you do not know, it can easily take a couple of days.",NA
"1529",1528,30,"For me, this varies between minutes and more than a week (and some times infinity). The deciding factors are (ordered):

1. How close is the problem to things I know really well?
2. How well is the paper written / presented?
3. Everything else, including actual complexity of the problem.

If I work / worked on something very similar, maybe even encountered the exact problem that authors tried to solve, it can understand some papers within minutes by looking at the abstract and experimental results. That is of course pretty rare.

If I read something that is very far from my actual work I sometimes take several days of 2h+ sessions. If the work is also presented poorly, I might have to dig into the code to verify my understanding and sometimes I just don't fully understand some papers, ever.",NA
"1530",1529,30,"Usually less than a half hour. I am not a theorist - but a practitioner of ML. A ML paper that robustly moves the real gains on a problem that you are working on should be very easy to understand - because you should be able to clearly see where they are getting their performance gains from. For me, a paper that is hard to understand, overloaded with mathematic notation is a red flag. For instance, look at the ADAM paper, a widely adopted and effective, many eyes reading the manuscript, but the theory had an error that took years to catch. In the Wasserstein GAN paper, the theory is only tangentially related to how the model works in practice. Look at the paper on neural recommender systems, which showed that the newest models don't outperform well tuned old baselines 

tl;dr: Theory is overrated, ML literature is plagued by unneeded complexity, and reality always violates assumptions of theory. Familiarize yourself with a real problem and its real pain points, reading papers will become a breeze",NA
"1531",1530,30,"Never lol.

Getting a hang of what is going on (main interesting ideas) and actually figuring out what the fuck the authors were thinking are separate things.

I am an author on many papers and even I have no fucking idea what I was thinking when I wrote them. Anyone that claims that they fully understand a paper are fucking liars and don't know what fully understanding means.

Don't worry that much.",NA
"1532",1531,30,"\+1 on the code.

I can read a paper several times, write down the equations there, try to figure out what they are really doing and fail.  But when I see the code and can draw a parallel between what they say in the paper and what they implemented, then it makes a lot more sense.  

A problem is that typically much of the code is just supporting structure (data wrangling and manipulation, scoring, reading / writing network state) around the core idea, so a key process for me is to try to strip that all away and get to the important part.",NA
"1533",1532,30,"WassersteinGAN paper is one of those cases where you read it and you go: "" ah that's cool, I think I get it."" 

Then something seems odd when thinking twice and you read the paper again and think:""fuck I don't understand anything""",NA
"1534",1533,30,"And how many days do you take to review something?",NA
"1535",1534,30,"Sometimes an alternate paradigm can come into focus. 

Without fully recording or others understanding the extent of the paradigm, it’s askew.",NA
"1536",1535,30,"https://i.kym-cdn.com/photos/images/original/000/548/129/538.jpg",NA
"1537",1536,31,"Title:Using a thousand optimization tasks to learn hyperparameter search strategies  

Authors:[Luke Metz](https://arxiv.org/search/cs?searchtype=author&query=Metz%2C+L), [Niru Maheswaranathan](https://arxiv.org/search/cs?searchtype=author&query=Maheswaranathan%2C+N), [Ruoxi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+R), [C. Daniel Freeman](https://arxiv.org/search/cs?searchtype=author&query=Freeman%2C+C+D), [Ben Poole](https://arxiv.org/search/cs?searchtype=author&query=Poole%2C+B), [Jascha Sohl-Dickstein](https://arxiv.org/search/cs?searchtype=author&query =Sohl-Dickstein%2C+J)  

> Abstract: We present TaskSet, a dataset of tasks for use in training and evaluating optimizers. TaskSet is unique in its size and diversity, containing over a thousand tasks ranging from image classification with fully connected or convolutional neural networks, to variational autoencoders, to non-volume preserving flows on a variety of datasets. As an example application of such a dataset we explore meta-learning an ordered list of hyperparameters to try sequentially. By learning this hyperparameter list from data generated using TaskSet we achieve large speedups in sample efficiency over random search. Next we use the diversity of the TaskSet and our method for learning hyperparameter lists to empirically explore the generalization of these lists to new optimization tasks in a variety of settings including ImageNet classification with Resnet50 and LM1B language modeling with transformers. As part of this work we have opensourced code for all tasks, as well as ~29 million training curves for these problems and the corresponding hyperparameters.  

[PDF Link](https://arxiv.org/pdf/2002.11887) | [Landing Page](https://arxiv.org/abs/2002.11887) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2002.11887/)",NA
"1538",1537,31,"Related GitHub repo with learned parameters and datasets: https://github.com/google-research/google-research/tree/master/opt_list",NA
"1539",1538,32,"I've already used the second one for clustering report and it worked pretty well. If you have a large number of document you must also have a good algorithm to find the closest. Here a benchmark of nearest neighbor algorithms [https://github.com/erikbern/ann-benchmarks](https://github.com/erikbern/ann-benchmarks)",NA
"1540",1539,32,"Option 1 works well if you're looking for copy-pasted content. If however you want to catch documents with same meaning but different wording then this approach is mostly useless.",NA
"1541",1540,32,"What similarity scores did you use if you don't mind sharing? Also, no labels, right?",NA
"1542",1541,32,"What approach would you suggest for same meaning different wording? My first thought was BERT but that has the problem of fixed length sequence:(",NA
"1543",1542,32,"For my case ? Just L2 distance. Yes it was unlabeled data and I wanted to create clusters of document",NA
"1544",1543,32,"Uh that really depends on what ""similar"" means in your specific problem domain.

You may get better results with option 1 if you normalize the document before applying them to TF-IDF, so words like

* wanted
* wanting
* await

would all become the same token, which helps a lot.

And that's where my knowledge ends, so if that's still not enough I won't be able to help you further. In that case you'll most likely need an actual AI system to segment each document into topics, which seems a non-trivial thing to do (don't feel discouraged tho!).",NA
"1545",1544,32,"Oh. Thanks!",NA
"1546",1545,32,"Thanks! I am having some success in splitting paragraphs and using BERT. Als, average BERT embeddings seem to be helpful!",NA
"1547",1546,33,"Seems like the related paper has a short ""negative results"" section. Hopefully this will be a trend here onwards.

[https://openreview.net/pdf?id=r1xMH1BtvB](https://openreview.net/pdf?id=r1xMH1BtvB)",NA
"1548",1547,33,"That’s awesome",NA
"1549",1548,33,"Neat",NA
"1550",1549,33,"Tried fine-tuning it on SQuAD 2.0.

On colab, fine-tuning was faster than BERT but inference time is longer than BERT.",NA
"1551",1550,34,"Goodhart's law",NA
"1552",1551,34,"Why are you still alarmed about these numbers in 2020, computer science in 1984 is a lot different than computer science now. The interest from women isn't there, and it's not alarming anymore.",NA
"1553",1552,35,"Reading the title I thought this was gonna be a pun joke lol",NA
"1554",1553,35,"Object tracking?",NA
"1555",1554,35,"What domain is your time-series from? In video this would be called Object Tracking. In audio it could be Audio Event Detection (if the output is onset/offset times), or Source Separation (if the output is the different sounds present).",NA
"1556",1555,35,"I would recommend looking into shapelets or bag of shapelets and start going down that rabbit hole on google scholar.",NA
"1557",1556,35,"I would strongly suggest you look at the Matrix Profile to look for and identify features base don what you are describing.  

 [https://www.cs.ucr.edu/\~eamonn/MatrixProfile.html](https://www.cs.ucr.edu/~eamonn/MatrixProfile.html) 

There are Python and R implementations, and an organization to promote it.  It's used by large companies for anomaly detection.  

Are your time series largely repeating?  If so, this will work really well.  Are your time series nearly random with a few detectable events?  This probably won't work.

If you have questions I have many papers and have used it in both Python and R, but you can find it all via Google.  It's really a unique appraoch.",NA
"1558",1557,35,"If you see similar patterns in the time series whenever the object is detected consider using MASS which can be used to find similar subsequences in a time series very efficiently",NA
"1559",1558,35,"Dynamic time warp",NA
"1560",1559,35,"State space models",NA
"1561",1560,35,"artifact detection is also a possible search key.",NA
"1562",1561,35,"Maybe we can train a dnn to find the punch line...",NA
"1563",1562,35,"Maybe I should have been more precise with the kind of time series we will use. We have sensor data from machines temperature, pressure, etc). But thanks for the tip.",NA
"1564",1563,35,"Thanks for the tip. We want to use it on sensor data from machines.",NA
"1565",1564,35,"Thanks for the tip",NA
"1566",1565,35,"Condition Monitoring and Anomaly Detection are good keywords in that case. What kind of sensor data? Vibration, audio, images, thermal imaging, current are all relatively established.",NA
"1567",1566,35,"I recalled seeing a package posted on this sub awhile ago that performs semantic segmentation of time-series data. I found it [here](https://stumpy.readthedocs.io/en/latest/Tutorial_Semantic_Segmentation.html)",NA
"1568",1567,36,"Won’t make a difference. Just a money grab by Google.",NA
"1569",1568,36,"If I got a certificate for every new TensorFlow version Google has released in the past two years, my chest would be covered with badges like I am a North Korean military general.",NA
"1570",1569,36,"I think it depends entirely on what the test is like. If the test is effectively just another coursera certification then it’s not really going to play a role. If the failure rate is pretty high and you really do need expert knowledge to pass the test then I can see it as useful as a base threshold that lets employers know this person can at least code tensorflow well enough to consider.",NA
"1571",1570,36,"Doubt it, at least not in the long term. All certifications do is set a minimum for people to meet, after that threshold they're all seemingly equal (to hiring managers) . Look at project management-- the PMP is basically worthless now. 

Do you want to do the minimum? :)",NA
"1572",1571,36,"I feel like the field is still wide open for ML frameworks. Researchers seem to have largely switched away from TensorFlow and PyTorch seems to be edging into the industry segment as well. What's more, all of these frameworks are still making fairly major changes throughout the software stack.

Personally, I wouldn't put too much weight on a certification for an ML framework. It says nothing about their current state of knowledge which may very quickly become out-of-date. And TensorFlow, in particular, seems like a poor choice for certification.",NA
"1573",1572,37,"This is awesome.  The application of ML to ancient languages is a service to humanity.",NA
"1574",1573,37,"Do you want accidental ancient demon summonings? Because this is how you get accidental ancient demon summonings.",NA
"1575",1574,37,"Demon is only mentioned in Bible. There is no demon in Chinese history.",NA
"1576",1575,37,"Yeah because demons beyond a metaphorical notion are mythological - and Chinese culture has them just the same.",NA
"1577",1576,38,"Interested!  
If not selected, I also suggested remaining guys can form another one.",NA
"1578",1577,38,"I am interested! This might be the nudge I need to get into this book",NA
"1579",1578,38,"Also interested.",NA
"1580",1579,38,"I am interested.",NA
"1581",1580,38,"Interested, have been thinking about finally going through the book for far too long.",NA
"1582",1581,38,"Interested! Hit me up!",NA
"1583",1582,38,"I am also very interested. Pm'd you.",NA
"1584",1583,38,"I am interested! Hope for discussions with different people!",NA
"1585",1584,38,"I am very much interested!",NA
"1586",1585,38,"Interested",NA
"1587",1586,38,"I'm definitely interested I think it'd be cool",NA
"1588",1587,38,"Hey if you guys get stuck on any parts feel free to dm me, I’ve been through the book before and am happy to help.",NA
"1589",1588,38,"Interested!",NA
"1590",1589,38,"DMed",NA
"1591",1590,38,"DMed",NA
"1592",1591,38,"DMed",NA
"1593",1592,38,"DMed",NA
"1594",1593,40,"Thanks mate. 
Kinda in the middle of something else but will bookmark this and check it out. 

I commend your initiative👍 you’re a star ⭐️",NA
"1595",1594,40,"It would be good to look at the previous 14 days as well as those should be more indicative of when it was transmitted.",NA
"1596",1595,40,"might want to x-post to r/Coronavirus. People were asking about data over there yesterday. And I think that'll be the first place people look for this.",NA
"1597",1596,40,"Not critiquing but I'm curious about the idea behind correlating the illness with the weather. Incubation aside, close to all cases in Sweden are people that have contracted it in Italy/Iran, or people close to them (family). In other words, the Swedish weather had little to do with it. I guess this will change now with the travel restrictions though.",NA
"1598",1597,40,"UPDATE: finally successful. Will keep you guys updated if I manage to get something meaningful. I am a statistical analyst with machine learning expertise. So let's keep our fingers crossed, and let us all do our best to address this global health issue with all our knowledge and expertise. But equally with generosity, sensibility, empathy, and kindness.
Stat safe guys!",NA
"1599",1598,40,"Very cool. I will be using this together with GISAID sequencing data on a computational biology project.",NA
"1600",1599,40,"thank you. I'll see if i can do something with this",NA
"1601",1600,40,"Could air pollution be a variable? I've heard it suggested the different death rates between men and women might partly be due to the different smoking rates between men and women in China. Could air pollution affect lungs in a way that makes lung disease worse?",NA
"1602",1601,40,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [I've compiled weather\/climate date for the confirmed COVID19 infection sites, if anyone wants it (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fh7yme/ive_compiled_weatherclimate_date_for_the/)

- [/r/datascienceproject] [I've compiled weather\/climate date for the confirmed COVID19 infection sites, if anyone wants it (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fhr9rj/ive_compiled_weatherclimate_date_for_the/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"1603",1602,40,"Thank you",NA
"1604",1603,40,"Great initiative indeed. However, I couldn't download the data fron your github link. Any help please!",NA
"1605",1604,40,"Love it.  Thanks!",NA
"1606",1605,40,"omg i suck at web scraping but have been wanting to do something like this with covid data tysm!!!!!!!",NA
"1607",1606,40,"Hi I’m new to reddit, can someone explain how to access this data?",NA
"1608",1607,40,"Thank you! That is a great iniciative. How did you do it?",NA
"1609",1608,40,"This is great. Thanks . Was searching for this .",NA
"1610",1609,40,"This is great!

One thing to note, that the stats are delayed because of multiple reasons, e.g. the diagnosis takes time, some people wait more (or less) before they go to the doctor, etc.",NA
"1611",1610,40,"I really appreciate the initiative, thank you!",NA
"1612",1611,40,"Yeah, I think if there is such a thing as a data set that I can use to understand novel coronavirus, it's this.",NA
"1613",1612,40,"Thank you! Definitely useful, may cite you just in case.",NA
"1614",1613,40,"Can you do a similar analysis to what this paper did?

 [https://www.medrxiv.org/content/10.1101/2020.02.22.20025791v1](https://www.medrxiv.org/content/10.1101/2020.02.22.20025791v1)",NA
"1615",1614,40,"Also a small curiosity. People use to think that cholera epidemics were related to weather, or how they called, bad air or something. I think the first guy who prove the opposite was a medic named John Snow (that one knew some things) buy collecting weather data and comparing it with the epidemics in London, i think. It was a pretty important step, because people was really into the weather cause where they couldn't do much and change into: its something else, may in the other diseases are the same, we may be able to do something. I think that John Snow also prove it was transmited by water, also gathering data related to water suppliers. Old school data science and machine learning to predict who get sick and what causes.",NA
"1616",1615,40,"Thanks!",NA
"1617",1616,40,"Yeah, the first date in the JHU data is 1/22/20. With your point in mind, that’s why I started with 1/1/20",NA
"1618",1617,40,"Also r/COVID19 is the scientifically-minded sister sub to r/Coronavirus",NA
"1619",1618,40,"There are a number of factors that have been either shown or hypothesized to affect respiratory disease transmissibility in general.

Why do we have a flu season, for example, and why is it always in the winter? The factors I'm aware of that contribute to it are (a) that cold weather causes people to congregate indoors, where they are closer together and together for longer periods of time, and (b) that air humidity affects the disease transmission rate when people are in close quarters.

We can reasonably assume that the first factor holds true for COVID-19, but the second factor is worth investigating. Any data that helps predict transmission patterns and helps reduce the rate of new infections is potentially helpful for slowing the case rate and reducing harm.",NA
"1620",1619,40,"While you might be right. As posted in the reply above there still may be some contributing factors that cannot be seen without critically looking at the trends from the data. 
Not an expert data scientist, but since I'm working in the same domain ( machine learning ) , I'll try to post some visualizations using this by tomorrow which may be useful for other people in verifying trends.
Thanks mate.",NA
"1621",1620,40,"I think that’s a fair point. Maybe it’s something, or maybe it’s nothing. Or maybe it’s something that seems like nothing until someone else finds the something. We won’t know until the research is done, right?",NA
"1622",1621,40,"Nice!",NA
"1623",1622,40,">Could air pollution affect lungs in a way that makes lung disease worse?

I mean, we don't need machine learning for that one...",NA
"1624",1623,40,"I mean, it’s certainly a good thought, and I feel that it merits investigation.",NA
"1625",1624,40,"I’m not sure how to help. What happens when you click the “Clone or download” button?",NA
"1626",1625,40,"Click on the link in OP. Clone the GitHub repository",NA
"1627",1626,40,"The basic story is this:

I googled free weather APIs. I didn’t find anything that was free, and I’m not sure why I expected to find anything free. 

However, in the process, I found dark sky’s API. They’re actually the weather app that I have on my phone. They offer 1,000 free API calls per day, and after that, 10,000 calls for $1. I figure that that’s more than reasonable, but I don’t really know for sure, but it works for me. 

I then fired up a jupyter notebook (python) and tooled around until I figured out what I wanted my approach to be, as well as how to structure the API get and all that. I’m not an expert programmer, so if one were to look at my git commits, I’m sure that a seasoned programmer would at least smirk. 

Once I figured it out, I then ran all the API calls. There are thousands of cells, so that part takes a while. 

That’s basically it. Like I said, my code and data haven’t been audited or validated or anything, so, it is what it is. 

I hear that expert climate modelers use something called ERA5. I glanced at that last night. It was just a glance, but I’d probably be way out of my depth with that, because it seems to use some terminology that sounds fancy.",NA
"1628",1627,40,"Thanks. Your point is why my data start ~3 weeks before JHU’s first date, which is 1/22/20.",NA
"1629",1628,40,"Thanks!",NA
"1630",1629,40,"[deleted]",NA
"1631",1630,40,"thank you. i unsubbed the latter bc it was filling up with hysteria",NA
"1632",1631,40,"I tried, but I guess my account is too new to post there. If anyone else wants to post it, feel free.",NA
"1633",1632,40,"Couples schools of thought on flu vs weather.  The two you mentioned, 1. Indoors 2. Humidity the third is vitamin d is greater with more sun which increases immunity. There are some pretty good studies showing promising information controlling for skin color which determines how much vitamin d you produce.",NA
"1634",1633,40,"> We won’t know until the research is done, right?

Sure, I'm just curious about the validity of any results.

Sweden basically stopped testing people yesterday. I doubt Italy is doing much widespread testing at the moment considering they lack resources to try and save everyone and have to make decisions on who to let die.",NA
"1635",1634,40,"But you might need statistics.
If you can say to a government. Halving parts per million particle scores by x% will reduce hospital days stays by y%, that is a useful metric.",NA
"1636",1635,40,"OP is original post, since you're new",NA
"1637",1636,40,"Impressive. Thank you again for that, i will bookmark it to take a look later, it is not just impressive by the actual situation, but also by the way you can gather interesting data for other situations in the same way.",NA
"1638",1637,40,"info",NA
"1639",1638,40,"If by hysteria you mean personal accounts of disease that the US government refuses to test so our President can have his ‘numbers’, yes it is.",NA
"1640",1639,40,"I don't have strong feelings one way or the other as to whether this data could yield a model with predictive power, but that still wouldn't really confirm a causal relationship between weather or climate data and transmission. It could be that we're finding secondary or tertiary correlations.

That's not nothing- even higher-order correlations can be useful for making accurate predictions, of course- but imo it wouldn't really be prudent to infer causality from any perceived correlative relationships between these data and COVID transmission.",NA
"1641",1640,40,"[deleted]",NA
"1642",1641,40,"no i mean like shit american fearmongering news clips

what you said was weird",NA
"1643",1642,40,"You’re spot on",NA
"1644",1643,40,"Here's a sneak peek of /r/okboomer using the [top posts](https://np.reddit.com/r/okboomer/top/?sort=top&t=all) of all time!

\#1: [The boomer way](https://i.redd.it/7nmvg9y17py31.jpg) | [52 comments](https://np.reddit.com/r/okboomer/comments/dwdwll/the_boomer_way/)  
\#2: [Forget OkCupid...](https://i.redd.it/25nqdp37t1z31.jpg) | [27 comments](https://np.reddit.com/r/okboomer/comments/dx6rf3/forget_okcupid/)  
\#3: [ok boomer](https://i.redd.it/ct2qn81borw31.png) | [99 comments](https://np.reddit.com/r/okboomer/comments/drrij3/ok_boomer/)

----
^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/ciakte/blacklist_vi/)",NA
"1645",1644,40,"Feel free to downvote this too. You obviously aren’t paying attention to patient care provider accounts of how difficult it is to get tests from the respective states’ health departments.

There’s a reason it’s gone person-to-person.

There’s a reason we’re going to spike as bad as any country but Iran.",NA
"1646",1645,40,"[removed]",NA
"1647",1646,40,"Agreed, sadly.",NA
"1648",1647,40,"[removed]",NA
"1649",1648,40,"If you can’t test a person, they can’t become a confirmed case.  So we have been keeping our numbers low by refusing to test.  It’s not psychopathic, it’s quite simple.

I am saying our entire response has been botched because Trump is playing a numbers game.

He’s so used to controlling information to affect reality that he doesn’t realize mother nature doesn’t care.

Think of me what you will.",NA
"1650",1649,40,"Canadian here. OP's response may have been a bit non-sequitir, but it's not crazy.

The data and reports that I've seen over here suggest that OP is on the right track: U.S. policy under Trump seems to have been primarily geared at managing optics and ""keeping the [reported case] numbers down.""

I do not think OP is a psychopath.",NA
"1651",1650,41,"Very nice. I liked how you separated NT Xent into its own module as opposed to some of the other implementations. I noticed your mask_correlated_samples didn't return anything though so I submitted a PR.",NA
"1652",1651,41,"Is this python package style of implementation usually preferred by the community over something like a jupyter notebook?

I'm asking because I implemented the same thing in a notebook which to me is a lot more appealing. Less code, no hierarchical project structure I need to understand first, top to bottom reading. Am I alone with this?

https://github.com/pietz/simclr",NA
"1653",1652,41,"Can SimCLR be used for text data ?",NA
"1654",1653,41,"Wtf am I reading?",NA
"1655",1654,41,"[removed]",NA
"1656",1655,41,"Thanks! And another thank you for the pull request, that's quite an essential part of the model :)",NA
"1657",1656,41,"It's better in some instances, worse in others.

If it's code that needs to be reused and not just copy pasted, a module format is definitely preferred. In this particular case, your implementation looks simpler than the OPs, so I'm not sure.

No hierarchical project structure is great, until you have enough code that you need structure. For many projects, the ideal is a module format + a jupyter notebook for demos.",NA
"1658",1657,41,"To be complete frank I never understood the purpose of notebooks. We organize code in hierarchies not out of necessity but because it is actually a very efficient way to organize code. For someone like me who prefers to understand code by reading it, you essentially destroy its organization and its flow by flattening it and then interspersing it with text I have to skip.

But I think it is more probable that it is rather my opinion which constitutes the minority :)",NA
"1659",1658,41,"I have added a Google Colab notebook link on the repository, which should run both pre-training and the linear evaluation on a GPU node: [https://colab.research.google.com/drive/1ObAYvVKQjMG5nd2wIno7j2y\_X91E9IrX#scrollTo=jF8ZoVrwt0n0](https://colab.research.google.com/drive/1ObAYvVKQjMG5nd2wIno7j2y_X91E9IrX#scrollTo=jF8ZoVrwt0n0)

The reason I personally favor modules over notebooks, is that modules make my workspace more clean, certainly for bigger models, and they can be easily re-used in different projects (e.g. the LARS optimizer, NT-Xent loss in this case). 

&#x200B;

Cheers!",NA
"1660",1659,41,"technically yes, but not efficiently: 

SimCLR uses a very broad concept to pretrain a neural network: Similar objects behave similarly, different objects differently. This is very easy to implement for images: take one image with random augmentation (e.g. colorshift, image dropout, center crop) A and the same image with augmentation B. Both of those should be more similar than every other image from the batch.

This kind of augmentation is way harder to do for Natural language, as there are no canonical transformations that don't change semantics easily available.

You could replace words with perfect synonyms but those would have to be hand-crafted and perfect synonyms without any change in meaning are very rare.

The closest thing you can get for an analog would be the recently released [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB) that has a simple generator with the job of swapping some of the words in a sentence with different ones. The objective of the main network is to predict whether or not a specific token has been swapped. This has the additional benefit of training the discriminator not only on the swapped words but also on the words that were kept the same. This leads to electra having better accuracy than BERT/RoBERTA.",NA
"1661",1660,41,"There was a cool paper written, you should read it: [arxiv.org/abs/2002.05709](arxiv.org/abs/2002.05709)

This guy (a hero) took the paper (which contained some pseudo-code for tensorflow and pytorch, but no actual code) and wrote the code. He did a great thing.",NA
"1662",1661,41,"I think it completely depends on the complexity of the code. I'm as biased as I could possibly be here but I find it much easier to skim my notebook implementation than OPs code.

IMHO, understanding the structure of code is always an overhead that is only justified if the project crosses a certain level of complexity.",NA
"1663",1662,41,"Can you point me to some of the semi supervised/unsupervised methods in deep learning for NLP ?",NA
"1664",1663,41,"gal?",NA
"1665",1664,41,"That might indeed be true, it is just that for everything below that level of complexity reading the paper is enough for me. Either I want to work with code or understand an idea - there just tends to never be a middle ground for me.",NA
"1666",1665,41,"pretty much everything that was released in the last \~5 years:

GPT, GPT2, BERT, RoBERTa, XL-NET, Reformer,...

NLP relied on unsupervised learning for a long time as labeling is a non-trivial task and there's an easy objective to (pre-)train a language model: mask out a number of words and let the network predict those.",NA
"1667",1666,41,"Sorry, I used guy as a gender-neutral there, this person is a hero.",NA
"1668",1667,44,"Thanks Andriy.",NA
"1669",1668,44,"Thanks so much! The content looks great. Can't wait to dig in",NA
"1670",1669,44,"!RemindMe 48h",NA
"1671",1670,44,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [Chapter 8 Model Serving and Monitoring (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fhr9r9/chapter_8_model_serving_and_monitoring/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"1672",1671,44,"many thanks",NA
"1673",1672,44,"I will be messaging you in 1 day on [**2020-03-14 19:27:04 UTC**](http://www.wolframalpha.com/input/?i=2020-03-14%2019:27:04%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/fhaaci/p_chapter_8_model_serving_and_monitoring/fkbwi9u/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Ffhaaci%2Fp_chapter_8_model_serving_and_monitoring%2Ffkbwi9u%2F%5D%0A%0ARemindMe%21%202020-03-14%2019%3A27%3A04%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20fhaaci)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",NA
"1674",1673,46,"This is a perfectly valid question! The main objective of training a variational autoencoder is to maximize the likelihood of the following generative model: p(x|theta)=int p(x|z, theta) p(z) dz. In order to sample from such a generative model, one first needs to sample the latent code z\~p(z) and then sample the object x\~p(x|z, theta). This distribution p(z) is commonly referred to as a prior distribution in the VAE model, although this naming may be a bit misleading. So why do you need to sample from the prior? Because you have specifically defined you generative model this way!

How do we find the parameters theta? We would like to use maximum likelihood: log p(x|theta) -> max, but we cannot compute this marginal log-probability. This is where the variational lower bound (ELBO) and the encoder come in. Instead of maximizing the marginal likelihood, we maximize a lower bound that is convenient to compute. In a way, this lower bound and the encoder have nothing to do with the definition of the model but are just tools that allow us to find the parameters theta of the generative model.

The aggregated posterior is the optimal ""prior"" distribution under the evidence lower bound objective when the encoder is fixed. It usually provides you with a better value of the ELBO objective and may result in a better marginal likelihood, at least for the training data. Intuitively, when sampling new objects using the aggregated posterior, you only use the latent codes that the decoder has seen during training. This would typically result in more realistic images, but would provide you with less diversity: all generated images may be similar to the images in the training dataset. This is expected since you overfit your ""prior"" distribution on your training data. Whether or not this would be a problem depends on your particular dataset. One way to mitigate it is to use a crude approximation to the aggregated posterior such as a [VampPrior](https://arxiv.org/abs/1705.07120), or learn a prior distribution in another way, e.g. using normalizing flows.

By the way, sampling from the aggregated posterior is quite simple: you take a random training object, pass it through the encoder, sample the corresponding latent code and then use it to reconstruct the object. Given the size of the encoder roughly equal to the size of the decoder, you have 2x computations and additional memory needed to store the training data.

I hope this clears the misconceptions in this comment section!",NA
"1675",1674,46,"You might wanna check ""Enhancing and diagnosing VAE"". It says that sampling from the prior may not be ideal because the learned distribution q_z in practice may be far from p_z, so it proposes a way to sample q_z using another VAE",NA
"1676",1675,46,"I might be blanking out but what is an aggregated posterior? 

It works for the original VAE because to do VI with ELBO you only need to calculate the KL of the joint probability and the variational posterior, hence sampling from the prior is enough.",NA
"1677",1676,46,"Because in the training process, you map p(z|x) onto p(z) using your encoder. When the model is trained, p(z|x) is p(z) (or at least an approximation of it).

Edit: I guess for a more complete answer, the loss function minimizes the KL-divergence between p(z|x) and p(z), so if the model is trained, you can sample from the distribution that you know, which is p(z) (which is part of the motivation for variational methods).",NA
"1678",1677,46,"VAEs actually do sample from the approximate posterior. The sampling from a unit Gaussian is simply so that we can add it as differentiable noise to the posterior mean and standard deviation and get a differentiable sample from the posterior (i.e. the reparameterization trick). Also, depending on how you formulate ELBO, you would also be explicitly minimizing the KL divergence between the posterior and the prior for regularization (e.g. b-VAE), but you can do this without sampling.",NA
"1679",1678,46,"Hmm 

Assuming we use elbo 

Elbo = E_q[ log p(x | z)] - E_q[ log p(x) / q(z)] 

= E_q[log p(x | z)] + KL ( q || p) 

The first term is called the reconstruction error, if we assume this to be a normal distribution with known variance it is proportional to minimizing mean square error so we can use that.

The second term ”kl” is in the middle layer. It is an expectation of q, this means we will sample from q to estimate it. 

It is important to use what is known as the reparametrization trick when doing this sampling so we can take the gradient through it. 

So to answer the question, we dont sample from the prior p(x) we sample from q(z). q(z) approximates p(z | x)",NA
"1680",1679,46,"I think this is the first comment that has understood my original question. Thanks for the paper suggestion - I might come back to you after I've read it.

Re: sampling from aggregated posterior - my thinking of generating this was the following:

You would encode lots of Zs (i.e. mus and sigmas) from your entire training dataset and then average the mus and average the sigmas. The averaged mus and sigmas can then be used as the parameters for a normal distribtuion - and sampling from this normal distribution would be like sampling from your aggregated posterior. Thoughts? This would also prevent directly overfitting your dataset.",NA
"1681",1680,46,"> This distribution p(z) is commonly referred to as a prior distribution in the VAE model, although this naming may be a bit misleading.

Why do you call it ""misleading""? It indeed represents your prior knowledge about the latent in this model.",NA
"1682",1681,46,"This is a good way of looking at things (esp. VampPrior)

One thing I might add is that you could also 'fit' the aggregate posterior `q(z)` (using kde or some such) and use that to

 1. directly sample from it using the estimated inverse-cdf
 2. rejection sample `z_i ~ p(z)` s.t `q(z_i) > \tau`
 3. importance weight `z_i ~ p(z)` by `q(z_i)` and take the top k samples.

But in general, I agree that you don't really want to be sampling from just `q(z)` since it will largely return samples from the training data.",NA
"1683",1682,46,"I'm confused by this response. The OP indicates that the prior of interest that's sampled from is zero mean, unit standard Gaussian. This is the case for the *prior* of the approximate posterior (i.e. in the regularizing term for b-VAE), but not necessarily for the approximate posterior itself (the ""prior"" q(z) s.t. KL(q(z) | p(z | x)) is minimized).

Edit: Oh I see. q(z) approaches a unit Gaussian (since the true prior p(z) is a unit Gaussian) so you'd just feed in a sample from a unit Gaussian prior to the decoder. Apologies for the confusion.",NA
"1684",1683,46,">Enhancing and diagnosing VAE

wow that's really interesting.",NA
"1685",1684,46,"Your learnt latent distribution isn't going to match your latent prior unless your KL loss =0.",NA
"1686",1685,46,"But during testing, (after you've trained), it's common to see data samples generated through sampling the prior not the aggregated posterior.",NA
"1687",1686,46,"The true aggregated posterior is a mixture of N Gaussian distributions, N being the size of the dataset. What you propose is basically using something like moment-matching (although not exactly, since the variance of the mixture is not equal to the average variance of the components) to approximate that mixture with a single Gaussian. Whether or not it is a good fit depends on a particular problem. Your particular approximation seems to severely underestimate the variance of the prior, but precise moment matching may provide something more reasonable. I suspect that precise moment matching would yield something that resembles a unit Gaussian, but it never hurts to try. More flexible approximations may provide a better result though (but not too flexible, you still risk to overfit!)",NA
"1688",1687,46,"I've seen the variational autoencoder taught at the most basic DL courses. However, the VAE carries strong language: prior, posterior, evidence, amortized inference, reparameterization trick, etc.. It provides a perfectly precise description of the model and the learning technique to the confident practitioner, but it can be hard to approach if you are just learning. I am not saying that calling the prior a prior is wrong. However, from my experience teaching and talking with other folks (and reading the comments here), using this sort of language might be a source of misunderstanding.

From my experience, it is easy for people to treat global latent variables (e.g., the weights of a linear model /  neural network) in a Bayesian way, but much harder to use the same mindset with the local variables (e.g., the latent codes in a VAE). The thing is, these two cases seem very different, but use the same language.

When you deal with global latent variables (parameters), you design your prior distribution to incorporate some kind of prior knowledge about the task at hand, you might reuse the knowledge distilled from other tasks, or you might enforce some properties on your model (e.g., sparsity). You use the training data to perform the Bayesian update and refine your prior beliefs about the parameters. You then use the resulting posterior distribution to reason about uncertainty, obtain the posterior predictive distribution (that is hopefully more robust and accurate than a maximum likelihood / MAP estimate), and possibly translate this information to other tasks. The whole world revolves around the prior, the likelihood, and the posterior.

When you work with local variables (e.g., the latent codes), your goal is usually to marginalize them out during training and then forget about them. By approximating the posterior, you can obtain all sorts of lower and upper bounds that allow you to optimize the originally intractable objective approximately. As for the prior in a variational autoencoder, you can just slap whatever you want, and it wouldn't matter that much if all you want is a generative model. In this case, you don't really need to reason about the local latent variables, since the central part of the model is the generative part.

Of course, if you do something fancier than a plain VAE, say, want to erase some sensitive information from the latent codes, impose some kind of structure onto the latent space, want to employ data with missing features, you need to be fairly comfortable with graphical models in general. However, if this is your first introduction to the variational autoencoder, and you did not have a proper introduction to Bayesian statistics and probabilistic graphical models, it all may be a bit overwhelming.

TL;DR You are not wrong, and I'm not arguing for changing the traditional notation. What I meant in my original comment is that it might be easier to understand the VAE model if you don't overthink the meaning of the words like ""prior"" and ""posterior"".",NA
"1689",1688,46,"thanks - what's kde? What does it stand for - so I can google it :)",NA
"1690",1689,46,"More specifically, it trains a VAE to sample what you call the aggregate posterior (q_z) by using samples generated from the encoder for x~data",NA
"1691",1690,46,"I don't think they should match? It could be beneficial though, see beta vae if you are interested",NA
"1692",1691,46,"I'm not sure what you mean, but I'd guess it's also actually a case where you're really sampling the posterior by adding random noise to the approximate posterior parameters.",NA
"1693",1692,46,"thanks!",NA
"1694",1693,46,"Kernel Density Estimator.

(At a high-level) To sample from the KDE estimate you could just pick a point from your training set at random, find its mean encoding using your encoder, and then sample a gaussian centered at that point with a small variance (i.e. bandwidth) to get the latent code you feed into your decoder.",NA
"1695",1694,46,"kernel density estimator

At test time, just using a [gaussian kde](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html) should be fine, as long as your latent space is not too large (on the order of 5 or 10 should be fine..)

Moreover, if your posterior `q(z|x)` is Gaussian, `q(z)` is effectively a mixture of Gaussians (MoG), which you can (very roughly) approximate with a Gaussian itself in [closed form](https://stats.stackexchange.com/questions/16608/what-is-the-variance-of-the-weighted-mixture-of-two-gaussians).",NA
"1696",1695,46,"B-VAE is something completely different where you increase the KL term. Not really relevant to my question.",NA
"1697",1696,46,"I haven't seen it done. Usually prior is sampled to generate samples after the model has been trained. The question is shouldn't we be using aggregated posterior to sample from.",NA
"1698",1697,46,"I think your question might be misled. After training, we do sample from the prior to generate a posterior sample, possibly using the reparametrization trick. But we don't use that prior directly to generate a reconstruction.

For example, we sample esp ~ N(0, 1), then sample the posterior using z = mu + std * eps, where mu and std are the approximate posterior parameters.",NA
"1699",1698,47,"Management buy in that we actually need data to do ML. And that no, we can’t delete all copies of said data after we deploy the first version of a model if we want to keep supporting it (or to ever have future versions).",NA
"1700",1699,47,"Yea this is something I've heard a lot. Management often has an unrealistic view of ML/Deep Learning",NA
"1701",1700,48,"There has been lots of research mapping words to related words and definitions. Check out ConceptNet if you haven't already -- http://conceptnet.io/

Most of the work that I've seen has been trained on corpora, but there's no reason I can think of why a model couldn't be trained the way you're suggesting, it would just be slower -- though it would also be a lot less noisy than a corpus, so it has that going for it.",NA
"1702",1701,48,"Can you teach it to say something that it knew it had to say when it didn’t have a choice to say it ?",NA
"1703",1702,48,"I'm still suggesting to use a corpus to create the bots initial ability to communicate but then use my idea to create customised user defined way to iterate the corpora to overcome a bots limits. For example if you trained your bot last year it wouldn't know what the coronavirus is, but you could use the method to then let the user train the bots understanding of what  it is as new things become available for refinement.",NA
"1704",1703,48,"Can you explain that a bit more? 

I suppose choice isn't really an element, its based on probability. So you ask the bot something and it predicts the most likely response based on the data its been trained on. 

I suppose you could have a low probability prediction  that it would respond with anyway but a chatbot never really has choice unless you allow it to.",NA
"1705",1704,48,"I'm not a chatbot expert but it sounds doable to me!",NA
"1706",1705,49,"Related blog post: https://greydanus.github.io/2020/03/10/lagrangian-nns/",NA
"1707",1706,49,"Title:Lagrangian Neural Networks  

Authors:[Miles Cranmer](https://arxiv.org/search/cs?searchtype=author&query=Cranmer%2C+M), [Sam Greydanus](https://arxiv.org/search/cs?searchtype=author&query=Greydanus%2C+S), [Stephan Hoyer](https://arxiv.org/search/cs?searchtype=author&query=Hoyer%2C+S), [Peter Battaglia](https://arxiv.org/search/cs?searchtype=author&query=Battaglia%2C+P), [David Spergel](https://arxiv.org/search/cs?searchtype=author&query=Spergel%2C+D), [Shirley Ho](https://arxiv.org/search/cs?searchtype=author&query=Ho%2C+S)  

> Abstract: Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.  

[PDF Link](https://arxiv.org/pdf/2003.04630) | [Landing Page](https://arxiv.org/abs/2003.04630) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.04630/)",NA
"1708",1707,49,"Yeah, Sam's amazing! Really hope he's joining us soon!",NA
"1709",1708,51,"I'm not sure if this is 100% relevant, but there is a saying in Korean academia that goes something along the following (rough translation):

1. Bachelor's: ""I've studied so hard and now I know so much!""
2. Master's: ""Actually it feels like I don't know anything at all...""
3. PhD: ""At least I'm not the only one who's clueless.""
4. Professor: ""Would you look at that, I can still teach even though I have no idea what I'm doing!""

So I guess that feeling of ""I have no idea what I'm doing"" is pretty universal and spans across all levels of education. :)",NA
"1710",1709,51,"That's not a problem. The problem is then you have perfect idea of what you are doing and absolutely sure it will work. But it does not.",NA
"1711",1710,51,"I have a PhD from Harvard and 15 years of experience. My current GitHub personal access token is “iHaveNoIdeaWhatImDoing”.",NA
"1712",1711,51,"Always but I've found that as time went on my ideas were more and more recent. ML is hard because its cross disciplinary. I'm always lacking knowledge in optimization, math, statistics, heck even programming sometimes. It feels overwhelming at times.",NA
"1713",1712,51,"That’s actually the best part of this job. Not a boring moment!",NA
"1714",1713,51,"All day, every day.",NA
"1715",1714,51,"You mean like life in general?",NA
"1716",1715,51,"I’ve got an ... ok idea? After 5 years? I think the most important thing is the line between “yeah that works like this” and “I have absolutely no idea what you’re talking about.” Once you know what you don’t know it gets a little bit easier.",NA
"1717",1716,51,"Here is a relevant quote from Jeremy Howard who - among other achievements - teaches the deep learning courses on [fast.ai](https://fast.ai) and was at one point ranked number 1 on Kaggle.  He's talking about how mixing batch norm and weight decay doesn't really make sense, but no one - not even the best practitioners - realized it for quite a while.

[https://www.youtube.com/watch?v=hPQKzsjTyyQ&feature=youtu.be&t=5427](https://www.youtube.com/watch?v=hPQKzsjTyyQ&feature=youtu.be&t=5427) 

""We have no idea what we're doing.  There's this thing that every model ever always has, and it totally doesn't work.  At least it doesn't work in the way we thought it did.  So.  That should make you feel better about 'can I contribute to deep learning'?  Obviously you can because NONE of us have any idea what we're doing.""",NA
"1718",1717,51,"Yes.",NA
"1719",1718,51,"Always. Fucking black box models.",NA
"1720",1719,51,"I'm in graduate school studying ML at a top national university. I'm just finishing up an ML course that is by far one of the hardest classes I've ever taken in my life - spent at least 60 hours a week studying for that class. I went in thinking I had a decent understanding of ML, and figured this class would help me understand the math better, maybe some implementation techniques. Oh how naive (Bayes) I was. 

I'm currently experiencing some serious imposter syndrome. Even after this class I feel I'm barely scratching the surface and I seriously question if I have the cognitive chops for doing ML in any meaningful way. I'm average at best, even after spending 60+ hours a week busting my ass in trying to learn this stuff.

So yeah, do I feel like I have no idea what I'm doing? No, I know for a fact I have no idea what I'm doing.",NA
"1721",1720,51,"[Insert cheesy quote by ancient philosopher here.](https://i.pinimg.com/originals/04/d5/82/04d5825abb79ebedd92c473e4e07d6cd.jpg)",NA
"1722",1721,51,"Definitely, that's how it should be. After all, modern machine learning is PAC, i.e. ""probably approximately correct"". You can *kinda* know what you (or the algorithm) is doing, but in the end it's all guess work ;)",NA
"1723",1722,51,"My biggest problem in life right now, besides Covid-19, is the number of data scientists in my surroundings who don't have any idea what they're doing, but don't seem to be aware of that fact. 

If you're actually aware of this fact, you're already waay ahead of them.",NA
"1724",1723,51,"I'm changing my position based on the feedback from u/sieisteinmodel (thanks!).

Not knowing what one is doing is symptom of something that's not quite functioning as it should be.

That was the constant feeling that accompanied my PhD. And I utterly despised it. Now, it's no longer the case.

Hence, the final remarks being the same, talk to someone who has been around longer than you have.


--- Older reply ---

It depends.

If you know what you're doing, then you're doing engineering.
If you don't, then you're doing research.
It takes some practice and patience to succeed in the latter.

Anyhow, if feeling puzzled, then have a talk with some of the people who've been around longer than you have. It is reassuring to acquire awareness about their perspective.

I hope it helps.",NA
"1725",1724,51,"I have some idea, although it's usually of what knowledge I lack, or just outright wrong. Does that count?",NA
"1726",1725,51,"I am a beginner and I m doing all that Andrew Ng course and some online book , but after 6 weeks , I learnt that what I was doing was just basics and there's long road after 11 week course and I'm still confused to use matlab for ml .",NA
"1727",1726,51,"Yup. I'm involved with both industry and academia, and I genuinely think there's just too much that goes into understanding ML for it to ever be reasonable to know exactly what you're doing at any given time, especially in industry. Honestly I am of the opinion that knowing how to properly evaluate model performance is by far the most important skill to have in this field. It's pretty easy to get a grasp on that stuff. Then you can have no idea what you're doing and still know if you're doing it well. I barely know how XGBoost works but I can tell you it works well.",NA
"1728",1727,51,"Yep.  I'm in my fourth year of a CS Ph.D (and have worked on models in industry as well) and basically the only time I make confident statements with respect to modeling outcomes is when I have good reason to believe they *won't* work.",NA
"1729",1728,51,"Prototype on toy problems. The numbers won't lie. This is the beauty of applied mathematics. You can literally hit a button and find out if you're crazy or not",NA
"1730",1729,51,"I totally understand what I'm doing : stacking black boxes one on the top of another, waiting for something to converge. If it does not, putting one more black box or changing the thing I want to converge.",NA
"1731",1730,51,"isn't that the alternative definition of ""research"", less eloquently stated though, here is what I was taught:

if you know what you are doing, it's not research",NA
"1732",1731,51,"> does anyone else ...

No, you're 100% unique in this across all of humanity.",NA
"1733",1732,51,"It's a common anecdote now that there are probably only 1000 people (maybe a little more now) who can actually truly be innovative in AI, everyone else is just riding the wave and following along.",NA
"1734",1733,51,"Haha fits really well to Dunning-Kruger effect plot;  

Realising your ignorance is a beginning of wisdom (Socrates)",NA
"1735",1734,51,"I think that regardless of your field or expertise, it is important to have a certain sense of humility - ""the more you know, the more you know you don't know."" I am always most skeptical of people who act like they have all the answers or solutions in a field. Aside from being difficult to work with - even if their math and reasoning is sound, the lack of awareness of how their underlying assumptions can profoundly affect outcomes can be pretty dangerous. People who are forthright with the limitations of their methods and/or understanding are a lot more credible in my eyes.",NA
"1736",1735,51,"Worst feeling ever :D",NA
"1737",1736,51,"HahahahA love this",NA
"1738",1737,51,"If you're spending 60 hours a week studying for a class (highly unlikely) you're either ineffectual in studying or have been done serious disservice by foundational classes.",NA
"1739",1738,51,"Well, I had a pretty long break in between undergrad and graduate (about 10 years), and my jobs in between were not very technical - most technical thing was writing some white papers that were more like basic tutorials on how to use some tools and techniques, practically zero math and coding. So having to relearn linear algebra, calculus and prob/stats was a significant handicap, especially considering how math intensive it is. Having rusty at best coding also didn't help, since a good chunk of the class was coding, oh and we didn't go over any coding in the lectures. Maybe my studying sucks or I'm too stupid for this class, but right now I'm incredibly unhealthy from eating just fast food and insane amounts of caffeine, haven't done any socializing, don't remember the last time I did anything fun (games, movies, etc.), my dreams are about math proofs and python code, and now thanks to coronavirus I don't even leave the house to go to my lectures. As a full time student, I don't see how spending 8-10 hours a day on school is unlikely (I'm fully funded and my spouse works so I don't need a part time job or TA position). My other classes are easy, and one is a C/NC class mostly based on attendance so I don't spend much time on them.

We covered a large range of ML topics - regression methods (linear, logistics), time series analysis, regularization, feature selection, PCA, LDA, LSH, generative models, Naive Bayes, k-NN, decision trees and ensemble methods (RF, GBDT), clustering (GMM, spectral, k-means), kernel methods (SVM), touched on NNs (didn't go too ""deep"" into those). The class was a deep dive into the nuts and bolt math behind all of it, then coding it basically from scratch, with a few ""hints"" on what to do. We used Python exclusively and were not allowed to use any high level APIs, so no SKLearn, no Keras, etc. just Numpy and basic libraries.",NA
"1740",1739,51,"\> If you know what you're doing, then you're doing engineering. If you  don't, then you're doing research. It takes some practice and patience  to succeed in the latter.

No, that's called tinkering. Research is a) the formulation of hypotheses and the design of experiments to test it or (science) or b) looking into the literature of previous attempts (scholarship).",NA
"1741",1740,51,"HahahahA This! Is great",NA
"1742",1741,51,"And the douche has appeared 😂",NA
"1743",1742,51,"Berkshire Hathaway's Charlie Munger had a brilliant quote, something like:

""Never underestimate the man that overestimates himself"".

Almost like the antidote to Dunning-Kruger effect is to try to cultivate an overconfidence.

Ian Goodfellow said on the AI podcast that if he had thought of GANs at lunch while not drinking he probably would have thought it wouldn't work.

I can't imagine how many brilliant ideas are lost to Dunning-Kruger effect.

""That won't work, I am not <insert genius from the past>""

So in that spirit I am going to say Socrates is wrong. That quote just traps you in self doubt.",NA
"1744",1743,51,"Fair.",NA
"1745",1744,51,"You have a point, agree overconfidence has its place. 

just think that, as usual with words, its easy to conflate more concepts together, so with the Socrates quote, i think we can agree that its useful not to be overconfident in some situations, like assesing ciritcism, other pepoles ideas etc as it can easily create blindspots and closed mind perspective.",NA
"1746",1745,53,"I wrote one that detect anomalies in user's IPs when they log into a system.

It's FLOSS, the code is [on Github](https://github.com/nextcloud/suspicious_login). I also [blogged](https://blog.wuc.me/2019/04/25/nextcloud-suspicious-login-detection.html) about the implementation a bit.",NA
"1747",1746,53,"Thanks. Though I was looking for more generic anomaly detection tool.

&#x200B;

Checked the Github. Just curious, why AGPL-3?",NA
"1748",1747,53,"Nextcloud is licensed AGPLv3. So the apps (plugins run with the program) have to be either AGPLv3 or MIT to be compatible. And since I do not plan any other use of the code but for this open source purpose I went with AGPLv3 to protect it.",NA
"1749",1748,54,"Hi, I'm new to this research problem and I found your paper very useful to catch up with SOTA models in this area. The code looks neat so I suppose it's straightforward to reproduce your results, which are numerically impressive btw. Thanks for sharing.",NA
"1750",1749,55,"Will it be postponed or canceled due to COVID-19?",NA
"1751",1750,55,"wow thanks for posting it here, I had no idea. 

I know a couple of ppl that could be very interested in this",NA
"1752",1751,55,"At present, we have no plans of cancelling ProbAI. It's still too early for making the decision. Please read the post as it is addressing your question.",NA
"1753",1752,55,"Thanks for the update!",NA
"1754",1753,56,"Title:How to train your neural ODE  

Authors:[Chris Finlay](https://arxiv.org/search/stat?searchtype=author&query=Finlay%2C+C), [Jörn-Henrik Jacobsen](https://arxiv.org/search/stat?searchtype=author&query=Jacobsen%2C+J), [Levon Nurbekyan](https://arxiv.org/search/stat?searchtype=author&query=Nurbekyan%2C+L), [Adam M Oberman](https://arxiv.org/search/stat?searchtype=author&query=Oberman%2C+A+M)  

> Abstract: Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE based generative models to the same performance as the unregularized dynamics in just over a day on one GPU, whereas unregularized dynamics can take up to 4-6 days of training time on multiple GPUs. This brings neural ODEs significantly closer to practical relevance in large-scale applications.  

[PDF Link](https://arxiv.org/pdf/2002.02798) | [Landing Page](https://arxiv.org/abs/2002.02798) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2002.02798/)",NA
"1755",1754,56,"Neural ODE are amazing if this interests you, also take a look at Hamiltonian NN paper. Applies this concept to make learning physics more accurate",NA
"1756",1755,57,"Don't policy gradients use pathwise derivatives? And isn't Q-learning an expanded policy gradient learning anyway?",NA
"1757",1756,57,"I would also look at this paper which explicitly connects GANs and IRL to energy based models: https://arxiv.org/abs/1611.03852",NA
"1758",1757,57,"PW gradient and SF gradient are doing the same job. One of the guys introduced the pathwise gradient into ML under the name reparametrization trick recently did a survey on this topic, [MC gradient estimation in ML](https://arxiv.org/pdf/1906.10652.pdf)",NA
"1759",1758,57,"I'm not sure I fully understand what you're asking, but there's been a few papers linking Q-Learning and actor-critic methods through entropy regularization:

* [Reinforcement Learning with Deep Energy-Based Policies](https://arxiv.org/abs/1702.08165)
* [Bridging the Gap Between Value and Policy Based Reinforcement Learning](http://papers.nips.cc/paper/6870-bridging-the-gap-between-value-and-policy-based-reinforcement-learning)
* [Equivalence Between Policy Gradients and Soft Q-Learning](https://arxiv.org/abs/1704.06440)",NA
"1760",1759,57,"Pathwise derivatives are where you push the gradient back through a differentiable Q-function fitted to data (usually saved in an experience replay buffer). It's a type of policy gradient, but it's distinct from score function methods (i.e. REINFORCE-based algorithms) that use the score function estimator. Methods using the score function are A2C, PPO, and TRPO -- they rollout a set of trajectories, empirically estimate the value of the policy, and then take an update that maximizes the value. Pathwise derivative methods are algorithms like DDPG, TD3, and SAC, where every time step, the agent samples a batch of transitions stored in a buffer, and then trains a value function off-policy from this data. From there, they feed an action from the policy through the value function to estimate the value of the policy. The derivative of this value with respect to the policy parameters is then pushed back (since neural nets are differentiable) and an update step that maximizes this *estimated* value is taken.
 
Q-learning isn't typically considered to be a policy gradient method -- it's a value-based method.",NA
"1761",1760,57,"Yeah, they're equivalent and the result of the change of variables formula -- I make that point in the pdf. If the SF estimator is reducing cross-entropy, then the PW derivative will as well. I think the interesting part is that the SF estimator can be thought of as the gradient of the cross-entropy between two distributions; given that the PW derivative and SF estimator are equivalent, this implies that all RL algorithms can be thought of as reducing cross-entropy (or KL-divergence) between the policy and an energy-based model, where the energy is given by the reward function. It also suggests there are other forms of the policy gradient theorem that can be used -- for example, using the SF estimator to minimize KL-divergence instead of cross-entropy, or using an MSE loss function to train the policy (this lets you recover a policy gradient from the Q-learning update, for example).

I've read that paper a few times, it's a mainstay in my reading list :)",NA
"1762",1761,57,"No worries! I guess for questions:

1. Deep energy-based models seem to have been developed through MaxEnt IRL and the MaxEnt RL framework. Why doesn't it (the energy-based model perspective) apply to all (deep) RL algorithms? It's seems pretty straightforward to show that the policy gradient reduces the cross-entropy between the policy and a Boltzmann distribution, where the return function is the energy. Is it just something that people take for granted? If we can apply this perspective to standard RL algorithms, doesn't this provide a reasonably straightforward way of unifying value-based methods like Q-learning with policy-based methods like A2C as being different algorithmic choices of the same problem?

2. If (at least some of) the above is true, why do IRL? If you have access to a dataset, you can fit a basic policy function, which would be proportional to exp(-c(s,a)). Why go to the trouble of finding c(s,a) and the normalization constant if you already have a policy distribution that matches the Boltzmann distribution? (Or have I misinterpreted something here?)

3. What does this say about the reward functions that we use, and the policy parametrizations that we train? For example, if we are using independent Gaussians for the policy, would the log-probability of a Gaussian make a better reward function than some other alternative?

Edit: another question -- in [this](https://arxiv.org/pdf/1805.00909.pdf) paper, an RL-as-inference setup is used to derive the max ent objective (see the derivation up to and immediately before equation 9). But doesn't this just follow from the definition of KL-divergence? I don't understand why we need an RL-as-inference framework to motivate this, or introduce an additional optimality parameter. It seems to me that you end up with the same result if you 1) treat the score function estimator as being the gradient of the cross-entropy between the policy and a Boltzmann distribution that uses the return as the energy, 2) derive the policy gradient using standard identities and the definition of cross-entropy, and then 3) use the relationship between KL(p||q) and H(p,q) to derive a KL-divergence based policy gradient. By definition, this maximizes both the return and the policy entropy, and should apply equally to online and offline methods (e.g. you could use it with TRPO as well as SAC -- SAC would become SVG(0) with a KL-divergence loss instead of a cross-entropy loss). What am I missing here? Why is the additional machinery required?",NA
"1763",1762,57,"Q-learning is kinda both though. Policy and value.
Also, with experience playback it also samples from a buffer.",NA
"1764",1763,57,"Sorry, no time to look at / write down equations on my end. But yeah with entropy regularization both actor-critic methods and Q-Learning-like ones can be shown to try and do the same thing (as discussed in the papers I linked). Regarding #2 I'm not sure I understand your question, but if it is why not just do supervised behavior cloning, there are a couple reasons at least: (1) hopefully better generalization to new situations, and (2) potentially ability to learn behavior on a different action space than the demonstrator's.

I don't understand what you mean in #3.",NA
"1765",1764,57,"It's usually considered a separate class of algorithm. The point I'm trying to argue is that it isn't, and that Q-learning, pathwise derivatives, and score function methods are all minimizing the same loss function, with the primary distinction between them being the choice of online, off-policy updates, and offline, on-policy updates.",NA
"1766",1765,57,"> But yeah with entropy regularization both actor-critic methods and Q-Learning-like ones can be shown to try and do the same thing (as discussed in the papers I linked)

Sure, but my point is: why only Q-learning-like algorithms? Q-learning itself (as in bog-standard DQN) can be shown to be doing the same thing as, say, A2C under this framework. The first paper (and the paper I linked) make the connection between SQL and Q-learning, but if you use an energy-based framework, the Q-learning update is equivalent to grad log pi(a|s) (1 - Q * (s,a)) which is an entropy regularized policy gradient (i.e. it's the gradient of the KL-divergence rather than the entropy). I used Q * (s,a) to denote the estimate of the optimal Q-function, given by r + max(a') Q * (s',a'). Also, it seems to me that showing this isn't nearly as complicated as it is in these papers.

For 3), if we take the view that the energy-based model framework applies to all RL algorithms, and that pi(a|s) \propto exp(-f(s,a)), are there reward functions that are more suited to certain policy distributions? For example, if we assume a gaussian policy pi(a|s) \propto exp(Q(s,a) - V(s)), where Q(s,a) = sum gamma^t r_t , would it be better to use a reward function of the form -k(s - s_g )^2 for some goal state s_g, which is itself the log of a gaussian distribution? Versus, say, some other reward function like a scalar reward at the end (which would be, what, a delta function)?

In my last question: say we have two distributions, p(x) and q(x), where p(x) is parameterized by theta, and q(x) is fixed. We want to minimize the cross entropy between them, so we use the definition of the CE function and take the gradient with respect to theta. Reddit isn't great for subscripts, but bear with me:

H(p,q) = -int_x p(x) log(q(x)) dx

Using standard identities from the policy gradient theorem, we get:

grad H(p,q) = -E_(x ~ p) [grad log(p(x)) log(q(x))]

Which implies that for a policy gradient, pi(a|s) \propto exp(Q(s,a)-V(s,a)), which is a Boltzmann distribution. So far so good, but there's no reason we couldn't have used KL-divergence instead, which would have given us:

grad KL(p||q) = E_(x ~ p) [grad log(p(x)) - grad log(p(x)) log(q(x))]

I.e. an entropy-regularized objective that maximizes both entropy and reward (which follows directly from the relationship between KL(p||q) and H(p,q)). Why do we need the additional machinery of an optimality variable in there (see the Levine paper I linked)? It seems that it's unnecessary to reach the same conclusion. From what I can see, the stuff on maximizing the ELBO should also hold true without it?

It's also straightforward to see that the entropy regularized objective is the same as grad (log(p(x)) - log(q(x)))^2 , which, if we take the view that pi(a|s) \propto exp(Q(s,a)) lets us show that Q-learning is *also* a policy gradient that maximizes entropy, which also squares with the fact that the Q-function in Q-learning isn't mapping states and actions to a value; rather, it implies that it's mapping states to log probabilities in a categorical distribution over actions.

I.e. these algorithms are all different algorithmic twists on the exact same process -- fitting a policy to a Boltzmann distribution given by the reward. Q-learning becomes an online PG that uses the score function estimator and minimizes KL-divergence between the policy and a heuristic estimate of the optimal Q-function. PPO is an offline method that minimizes the CE. etc. etc.

I apologize for the essay, and I understand if I'm sounding like a crazy person, or like I don't quite know what I'm talking about. That's why I'm trying to understand. I really appreciate your time.",NA
"1767",1766,59,"This looks really great.  Do you know if it would be possible to build the Docker image on Docker hub and deploy it that way?  In my case, I can't run Docker on my local system without a Windows upgrade, and am wondering how to approach this.  Also, any chance of a small tutorial walking through a Fargate deployment all the way through?

Great work, very amazing for sharing!",NA
"1768",1767,60,"I have had the same issue with automation. To the layman, automation means replacing manual effort and reducing costs and time to completion. I actually had to include a short segment at the end of a 2h long video tutorial that I wrote, which basically summarized how to talk to your manager about automation. Since ML is largely an automation tool, I think some of the things you need to consider are parallel to that. The guidelines were, from personal experience,

1. Automation has inertia. You need significant organizational effort and buy-in, both to implement, and to consume and utilize the outputs of that process. A single person can't move a freight train, but you can't deny a freight train is useful (and efficient). 

2. Automation degrades over time. It requires continual maintenance.

3. Due to 1 and 2, you should not consider automation a cost cutting endeavor in the short or long term. Instead you should use it as

- A means to supplement work done and maximize downtime productivity (hedge current downtime against future crunch)
- To reduce risk in testing and production by gating work product
- To reduce human error
- To lower the minimum time (but not the peak time) to complete certain tasks
- To take advantage of underutilized resources in the off hours (ie, between 5pm and 8am) in otherwise highly manual workloads

A lot of things can derail automation efforts. Some examples pertaining to the above,

-  If you don't have downtime or new effort to implement automation, the train won't move and you will be stuck with a disintegrating half-assed tool
- If you only value peak (worst case) time to complete, then you can't rely on automation in case it fails (usually due to changing requirements in the product under test)
- If your system and work output is entirely online 24/7 due to multi-regional operations, then you can't increase the duty cycle by using automation to utilize resources during the other 2/3 of the day.

Basically, automation (and by extension, ML) is not a cure-all, it's not a magic potion, you need to work at it at an organizational, comprehensive level.",NA
"1769",1768,60,"[deleted]",NA
"1770",1769,60,"The parameters they are setting for you work like an api, they won't understand a diffrent way of doing things. Bussines culture...

These numbers and concepts tunnel to the top this way. It only exist to simplify the thinking for the business end. Like a real bad designed class. 

It also depends on how you are seen in the company, if you are a tool to be used: ""here eat this projects and put some good numbers out..."" or are you seen as an advicer, coaching the projects to find optimisations?

If you think you are seen as tool, run! There will be only competition, ideology, ignorence and sadness.

If you are seen as coach, hold a meeting, explain what you do in 20 min, impotent is that you convey your ""success parameters"" and maybe find a way how these could effect their numbers. And then explain why it is the managers job to pick the projects not yours... To get them to give you a priority list which dataset to check first and so on.

This way both parties have defined a communication protocol, so you can work your way and they get their numbers.

Good luck",NA
"1771",1770,60,"Eh I think your idea of rooting things in data doesn’t make sense. There are plenty of useless models and there is plenty of useless data. It can’t all be exploratory. You need a hypothesis for applications. 

Also, consider that even non machine learning work requires considerable exploration and is non deterministic. Often after two weeks one might realize half of their 3 month plan doesn’t make sense. That’s why going through the thought excercise is useful. 

Honestly, I don’t think there is anything that wrong with the traditional corporate roadmap. In particular when you are not doing research, and the work seems applied. Research of course can be useful in a model improvement phase, but that’s not thrust. 

Come up with metrics for improving things, and applications. Note that those metrics are a goal. make any caveats with those metrics clear. 

you should be able to detail a data cleaning process and a modeling process (ie which models you expect to be successful). You can highlight what you will do to collect baselines, and what avenues you may explore for researching improvements. 

You can then talk a bit about what engineering work will be required to actually make this useful and used by human beings (serving, versioning, data pipeline for training etc). 

Lastly you may want to touch on how you will track online metrics or whatever it may be, and release updates to models where necessary. 

There’s likely some stuff I’m missing but this is generally what I do when writing this stuff up, and I find the excercise pretty effective at making my own thoughts more clear.",NA
"1772",1771,60,"If I'm being totally honest, reading this I think the issue may be a misalignment of goals on your end.

If your company is expecting you to do projects with clear and targeted efficiency gains, and what you're working on is so far removed from efficiency gains that you can't even put a rough number on things, maybe you need to rethink your priorities. It sounds like you're doing long term research/moonshot development. Companies don't usually ask for you to put a number on that sort of work because it's obviously a heavy tailed distribution. Are you sure that you're doing what they want you to be doing?

I say this as a DS at a major tech company. I too like to work on moonshot ideas and find it hard to manage that desire. I've had a few moonshots ""workout"", meaning they provided exciting new insights into problems we're solving. But generally I did this on my own time and brought a working prototype to my manager before investing company time into it. This has worked really well for me as a way to balance my own curiosity with the bottom line mentality that drives my performance evaluation.",NA
"1773",1772,60," Remember, making processes efficient for a corporate is just a means not the end. Since you are on the corporate roadmap, talk business value. You have to ask yourself, what is the key performance indicators (KPI) of business does your corporate have? Then, talk the impact of your project on those KPIs.

You will have to make reasonable assumptions and mention these upfront. For example, our process currently takes X hours but with my ML system it could take a 10m and the reduction in time will help us get product faster on the market and this will make us more money (rough ballpark business value). In my example, getting a product faster on the market was one of the KPI. That, will get people listening. You can during the course of the project outline that this endeavor is experimental and hypothesis driven. You can clearly state that there's no guarantee of success and the only way to know is try it out.",NA
"1774",1773,60,"Bambi?",NA
"1775",1774,60,"You need to negotiate for time and material instead of a fixed price for a fixed deliverable. They'll ask for clauses regarding budget and progress reporting and you'll need to report (and warn) the client at the moment where 80% of the budget is up (or whatever you agreed upon but do the 80% reporting in any case and do it well documented).",NA
"1776",1775,60,"Orient the milestones around learning goals and not performance goals. Focus on what you will know by then (and wrt to what is still unknown/to be uncovered later). For example early milestones should be about setting up baselines, find out human level performance, identify SOTA, reproducing existing results etc, validating the dataset (and data collection process), et.c.. The overall process of the project is going from mostly-unknown-impacts, to very well known and understood problem, solutions and impacts.

Of course it is important to realize that there are things you can know up front. For example one can sketch out the maximal impact possible, given a perfectly executed ML project and ML solution. If this impact is not significant enough, then the project is doomed to be insignificant, no matter how good the ML part is. 

You might want to have pre-defined go or no-go points and criteria, to ensure that one does not spend a lot of resources on something one found wasn't very effective. If at milestone 4 one cannot reach X, the rest of the milestones are canceled, and switch over to another project.

If management is not happy with a whole project set up around discovery, then design a shorter project as a ""pre-project"" - whos only purpose is to uncover and clear away as many of the risks as possible - such that for the later ""project"" part one can have clear performance goals and a",NA
"1777",1776,60,"Very much depends on the objective you have. If the end result is a product or a service then it is possible to  have a similar roadmap (at the end of the day an AI product is still software). Of course you cannot follow the exact same procedure and need to adapt it to take into account the iterative nature of AI projects.",NA
"1778",1777,60,"Great points.  I'm definitely seen as both-- the latter more so (or I wouldn't have signed up) but also wrapped up into that, the former.  I am in the position to help define how the ML narrative is to be written, which is great-- I just don't want to screw it up.  Your post is great and spot on.  

I mentioned in another response that my plan is to describe it in terms of a probability distribution-- the reasoning being that ML outputs are indeed simply distributions and probabilities, so it makes the most sense to describe possible results this way.  

For example: if we increase output/predictive accuracy by X, I think it will have Y% improvement (based on this this and that.). So essentially an output curve/chart.  Caveat being that this is all highly hypothetical and should be viewed as equal parts R&D and dev.",NA
"1779",1778,60,"True that.  Part of the issue is that 

a) there's not a ton of real world examples I can point out with concrete results (barring a few poorly written Medium posts, haha)

b) I don't want to promise something and then underdeliver, though I should just suck it up and explain that may be a possibility

And I for sure have a ton of support-- I think that I gave off the impression that I don't, which isn't the case.  I am simply trying to be as relatable as possible, to your point, so that nobody gets the impression that I'm blowing sunshine up their ass, especially in comparison to normal agile development cycles which the company operates on 2-3 week sprint intervals.  Read my other response about how I plan on building result parameters into a distribution.",NA
"1780",1779,60,"I agree completely. They are actually very open to a new way of reporting/roadmapping-- my problem is defining those ""success parameters.""  My plan is to describe it in terms of a distribution-- the reasoning being that ML outputs are indeed simply distributions and probabilities, so it makes the most sense to describe possible results this way.",NA
"1781",1780,60,"Yes, it's turning out to be incredibly effective actually.  I shouldn't knock the process, just myself for not being able to immediately translate my goals in to it.  I am finding that doing it isn't actually as awful as I thought, as long as-- and you hit the nail on the head-- there are caveats are up front that these productivity metrics are simply goals.

That's actually how I did it originally and I am trying to massage it a little so it's not so strictly goal oriented.  

""There are plenty of useless models""  <---- too true.  And thank you for correctly pointing out that ML work isn't completely in a bubble around non deterministic non-ML work.  I think the point I was trying to make is that the ML work I am doing is a fairly big contrast to most of the other work being done at this company.

Thanks for your thoughtful response.",NA
"1782",1781,60,"I appreciate your comment, and I think I probably misrepresented myself. There are very clear high level KPIs in place (that will depend on a number of factors) that I helped come up with, and very clear directions to go with the data available. It’s not in this case as simple as optimizing for false positives/negatives or a model accuracy issue.

The problem is attributing direct improvement percentages to workflows in the business that will use the output of my modeling alongside other things including, at least initially, human discretion. 

I know I can automate, optimize and improve x y and z but it’s less clear what exact effect this will have on certain business KPIs other than they will definitely improve and I’ve chosen to present the improvements as a distribution accounting for model outputs at different thresholds as well as for human involvement at various levels, and that seems to have made them happy. 

This isn’t moonshot stuff. It’s a lot of human assist stuff (with the eventual goal to automate certain things completely.) 

Sounds like you have a fun job. Cheers!",NA
"1783",1782,60,"I think you are on the right track, but I am not sure what benefit means in your case. 

Let's say you have a manual process and an automated version of it. The automated version should strive to eliminate false positives. If you do that, then the function of benefit is the proportion of positive results times the labor input to that thing (or rather than labor, you can apply that to lost parts or whatever). B = P * L. Then you could also factor in time saved using partial results to guide future analysis or hotspots. 

If you can't guarantee no false positives or negatives, then it's much harder to define. You would probably need to do a study which compared the effects of guidance from the data to the output metric, whatever that means to you. For example if you had ML that tried to guess a customer's most likely purchase from a survey they completed, then there is no positive or negative result, so you would need to measure sales against a control group which did not use the algorithm compared to one that did.",NA
"1784",1783,61,"Title:Reinforcement Learning with Convolutional Reservoir Computing  

Authors:[Hanten Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+H), [Katsuya Futagami](https://arxiv.org/search/cs?searchtype=author&query=Futagami%2C+K)  

> Abstract: Recently, reinforcement learning models have achieved great success, mastering complex tasks such as Go and other games with higher scores than human players. Many of these models store considerable data on the tasks and achieve high performance by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks, respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using the stored data. In this study, we propose a novel practical approach called reinforcement learning with convolutional reservoir computing (RCRC) model. The RCRC model uses a fixed random-weight CNN and a reservoir computing model to extract visual and time-series features. Using these extracted features, it decides actions with an evolution strategy method. Thereby, the RCRC model has several desirable features: (1) there is no need to train the feature extractor, (2) there is no need to store training data, (3) it can take a wide range of actions, and (4) there is only a single task-dependent weight parameter to be trained. Furthermore, we show the RCRC model can solve multiple reinforcement learning tasks with a completely identical feature extractor.  

[PDF Link](https://arxiv.org/pdf/1912.04161) | [Landing Page](https://arxiv.org/abs/1912.04161) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/1912.04161/)",NA
"1785",1784,61,"I tried something quite similar last year, with a randomly weighted CNN, using evolution to set the parameters for the final layer (https://github.com/anthony-dipofi/RandomNeuronES). I was actually inspired to try it by your work /u/hardmaru , and by this blog where they found the RNN didn't need to be trained for world models: https://ctallec.github.io/world-models/ . I've been working on something else similar too, where you just evolve a mask over the activations for a randomly weighted network, this is really cool because it lets you calculate the behavior for the whole population in a single batched forward pass of the network.",NA
"1786",1785,61,"Does anyone know of fixed-weight/reservoir computing papers that are specifically mutli-task? I was thinking something along the lines of using ES to find a set of fixed weights that work well across multiple tasks (preferably a wide variety of tasks).

So far my searches haven't found anything.",NA
"1787",1786,61,"Why use ES for the final layer instead of backprop?

edit: never mind it's probably just the same as choice of ES vs policy gradients.  i had been wondering why not take advantage of the fact that the network is differentiable, but it's still RL so the world dynamics are not assumed differentiable, so backprop only works up to the reward.",NA
"1788",1787,62,"k8s is not simple.

This is more webdev/it related than ML",NA
"1789",1788,62,"To avoid scaling issues, you will have to map the network weights to a shared memory and initialize a multi-process multi-stream service, but I don't know if it is feasible in pure python (Keras models probably aren't fork-safe).

The other path would be using a C/C++ API, but the one time I tried deploying a LSTM using XLAs' AoT compiler, I was baffled on how it needed fixed-length features for my sequence inputs. I don't know if this still stands or whether there are better APIs for nnet inference in shared memory. What I do know is that in PyTorch's C++ API it seems much more feasible with torchscript conversion and C++ inference.",NA
"1790",1789,62,"I contribute to a project called [Cortex](https://www.cortex.dev/) that sounds like it does what you're looking for. Basically, it automates the K8s infra work you need to do to make this work—autoscaling your model, handling pod failure, rolling updates, etc.",NA
"1791",1790,62,"The thing is I need to scale with each call and corresponding calls need to go to its own pod. Would that be possible?",NA
"1792",1791,62,"If I'm understanding your project correctly, it would require some custom work—but if you want to ask in Cortex's [Gitter](https://gitter.im/cortexlabs/cortex), there are some users who have done fairly involved audio/video work that may have a solution.",NA
"1793",1792,63,"in active learning this is often referred to as unlabeled dataset or unlabeled pool",NA
"1794",1793,63,"I would call this ""live data"" or ""production data"". At least in my cases (sensors/IoT) it tends to come in all the time, so to call it a data\*set\* feels wrong since it is unbounded (not finite).

The unlabeled part here does not seem to be the key aspect btw. More that it happens after the model has been developed and deployed.",NA
"1795",1794,63,">\*set\* feels wrong since it is unbounded

I think I feel a similar dissonance in using the word ""dataset"" that way.  But to be pedantic, sets can be any size you like, including infinite!

In fact, our ""standard"" hierarchy of infinities is *defined* in terms of set sizes.  Specifically, the [transfinite cardinals](https://en.wikipedia.org/wiki/Cardinal_number) are defined in terms of the cardinalities of infinite sets, via existence proofs (or nonexistence proofs) of [bijective, surjective, or injective maps](https://en.wikipedia.org/wiki/Bijection,_injection_and_surjection) between those sets.

One of the crown jewels of that line of thinking is [Cantor's theorem](https://en.wikipedia.org/wiki/Cantor%27s_theorem). It's often described as saying ""there is no largest infinity,"" but that's technically just a corollary.  The theorem is stated in terms of set sizes:

>there is no surjective map from a set to its power set.  

This means that a set is always smaller than its power set, even if both are infinite.  And the corollary is:

>there is no largest set, even among infinite sets

because we can just inductively take power sets of power sets.",NA
"1796",1795,64,"Wonder where the name came from, hmmmmmm",NA
"1797",1796,64,"Title:ReZero is All You Need: Fast Convergence at Large Depth  

Authors:[Thomas Bachlechner](https://arxiv.org/search/cs?searchtype=author&query=Bachlechner%2C+T), [Bodhisattwa Prasad Majumder](https://arxiv.org/search/cs?searchtype=author&query=Majumder%2C+B+P), [Huanru Henry Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao%2C+H+H), [Garrison W. Cottrell](https://arxiv.org/search/cs?searchtype=author&query=Cottrell%2C+G+W), [Julian McAuley](https://arxiv.org/search/cs?searchtype=author&query=McAuley%2C+J)  

> Abstract: Deep networks have enabled significant performance gains across domains, but they often suffer from vanishing/exploding gradients. This is especially true for Transformer architectures where depth beyond 12 layers is difficult to train without large datasets and computational budgets. In general, we find that inefficient signal propagation impedes learning in deep networks. In Transformers, multi-head self-attention is the main cause of this poor signal propagation. To facilitate deep signal propagation, we propose ReZero, a simple change to the architecture that initializes an arbitrary layer as the identity map, using a single additional learned parameter per layer. We apply this technique to language modeling and find that we can easily train ReZero-Transformer networks over a hundred layers. When applied to 12 layer Transformers, ReZero converges 56% faster on enwiki8. ReZero applies beyond Transformers to other residual networks, enabling 1,500% faster convergence for deep fully connected networks and 32% faster convergence for a ResNet-56 trained on CIFAR 10.  

[PDF Link](https://arxiv.org/pdf/2003.04887) | [Landing Page](https://arxiv.org/abs/2003.04887) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.04887/)",NA
"1798",1797,64,"Implemented [ReZero here](https://github.com/fabio-deep/Distributed-Pytorch-Boilerplate/blob/master/src/resnet.py) on a ResNet20 and CIFAR10 (based on my understanding) if anyone is interested. Trained a couple of nets for comparison, using identical hyperparams and early stopping on validation accuracy plateau schedule.

\-Results with the proposed alpha:

Training time: 48m 41s, 268 epochs

Best \[Valid\] | epoch: 208 - loss: 0.3313 - acc: 0.9197

\[Test\] loss 0.3387 - acc: 0.9153 - acc\_top2: 0.9727

\-Results without:

Training time: 88m 44s, 398 epochs

Best \[Valid\] | epoch: 338 - loss: 0.3026 - acc: 0.9237

\[Test\] loss 0.3055 - acc: 0.9202 - acc\_top2: 0.9742",NA
"1799",1798,64,"They're painting it like an algorithm to speed up training for transformer architectures, but it seems to me like the real story is how much more effective it makes training super-deep FCNNS:

>With increasing depth, the advantages of the ReZero architecture become more apparent. To verify that this architecture ensures trainability to large depths we successfully trained fully connected ReZero networks with up to 10, 000 layers on a laptop with one GPU to overfit the training set.",NA
"1800",1799,64,"I tried this 0-init coefficient-on-function trick on 7-layer encoder 6-layer decoder Lightconv model with Fairseq -- It converges almost 2x fast.

Also the author should change the abstract, just tell the method inside abstract because it is too simple",NA
"1801",1800,64,"So: Fixup? What's the difference?",NA
"1802",1801,64,"why not use it for training BERT and evaluate on GLUE",NA
"1803",1802,64,"The idea is simple but you need to remove LayerNorm to apply this idea in Transformers.",NA
"1804",1803,64,"Interesting. I also tried FixUp and SkipInit/ReZero in my language modeling experiments to replace layer norm and both times the convergence rate was slower than the vanilla transformer. Maybe some more tuning is needed but the difference should not be that drastic.

Also there are deeper Trafo (language) models than 78 layers, for example [Librispeech](https://arxiv.org/abs/1905.04226) with 112 layers successfully trained or one of the google multilingual mt systems that scaled up to 1500  [layer](https://ai.googleblog.com/2019/10/exploring-massively-multilingual.html) just FYI. Of course the amount of data was larger but it sounds like  training trafos beyond 100 layer is a novelty.",NA
"1805",1804,64,"I am sorry but the XXXzero nomenclature is already reserved by DeepMind, and this paper does not fit into the high impact series of AlphaGoZero, AlphaZero, and MuZero.

Please consider to change the name of your approach.",NA
"1806",1805,64,"So Authors 1:Reddit 1?",NA
"1807",1806,64,"It does show significant training speed boost.

I think the authors also meant to justify ReZero with performance from a deeeeep network , did you happen to test how it fares with say double the layer count?",NA
"1808",1807,64,"I'm not sure it's all that amazing, aren't there initializations and early stuff like highway networks which could train almost arbitrarily deep FC networks? But no one uses them, AFAIK. So, a more interesting question would be - what do you *do* with those FC networks if you could train them? Back when resnets came out, it was easy to see how training 100-layer resnet CNNs would be useful immediately, and they have been, but I've never seen anyone use FC networks more than like 8 layers deep in practice (StyleGAN is a notable exception). What would you ever do with a 10,000 layer FC NN? Do you need these for... recommenders? Or what?",NA
"1809",1808,64,"No, it makes sense. In [_Re:Zero_](https://en.wikipedia.org/wiki/Re:Zero_%E2%88%92_Starting_Life_in_Another_World), the protagonist has a time-loop power, where he repeatedly redoes scenarios, getting a little deeper into the scenario each time. Which makes sense for an arch which, like highway networks, progressively unlocks deeper layers as training proceeds...",NA
"1810",1809,64,"""I am sorry, you can't name your model like some character from sesame street because reasons.""",NA
"1811",1810,64,"What the hell is that argument? Deepmind doesn't own the word zero, because they used it for their approaches a few times",NA
"1812",1811,64,"**Re:Zero − Starting Life in Another World**

Re:Zero − Starting Life in Another World (Japanese: Re：ゼロから始める異世界生活, Hepburn: Ri:Zero kara Hajimeru Isekai Seikatsu, lit. ""Re: Starting life in a different world from zero"") is a Japanese light novel series written by Tappei Nagatsuki and illustrated by Shinichirou Otsuka. The story centres on Subaru Natsuki, a hikikomori who suddenly finds himself transported to another world on his way home from the convenience store. The series was initially serialized on the website Shōsetsuka ni Narō from 2012 onwards.

***

^([ )[^(PM)](https://www.reddit.com/message/compose?to=kittens_from_space)^( | )[^(Exclude me)](https://reddit.com/message/compose?to=WikiTextBot&message=Excludeme&subject=Excludeme)^( | )[^(Exclude from subreddit)](https://np.reddit.com/r/MachineLearning/about/banned)^( | )[^(FAQ / Information)](https://np.reddit.com/r/WikiTextBot/wiki/index)^( | )[^(Source)](https://github.com/kittenswolf/WikiTextBot)^( ] Downvote to remove | v0.28)",NA
"1813",1812,65,"LabelStudio all the way: [https://labelstud.io](https://labelstud.io)

Pigeon for something much much much more lightweight [https://github.com/sugi-chan/multi\_label\_pigeon](https://github.com/sugi-chan/multi_label_pigeon)",NA
"1814",1813,65,"We've been using cvat, its quite easy to get running using docker-compose

[https://github.com/opencv/cvat](https://github.com/opencv/cvat)",NA
"1815",1814,65,"I recommend [brat](https://brat.nlplab.org) if you're doing any natural language annotation task that requires annotating relations between entity/labels.",NA
"1816",1815,65,"I use Doccano right now. I'm quite pleased with it, however I had to write a Python script to turn the annotation output into an appropriate IOB format since I'm doing named entity recognition. https://github.com/doccano/doccano",NA
"1817",1816,65,"If you are working with images I suggest you to go with picsell.ia, it support all types of vector annotations + it has optimized tools to help you annotate complex shapes",NA
"1818",1817,65,"I've been using GIMP, csv files, and matplotlib's interaction tools.  Probably not the most efficient route..",NA
"1819",1818,65,"We have used LabelMe, VGG, and CVAT.  They are all free.  LabelMe is pretty old at this point, and people have complained about speed.   CVAT is kind of complicated.   Try VGG ([https://gitlab.com/vgg/via](https://gitlab.com/vgg/via)).

I haven't used Prodigy, but what these tools don't do is what I think that Prodigy is trying to sell: a closed-loop process, where it can use a detection tool to label and then have a human update / verify, rather than starting with a completely unlabelled image.  Then, when you correct the detection tool or add additional labels, then it re-trains and updates.  This is nice because labeling is so time consuming and verifying is so much faster than drawing boxes or polygons.  

Prodigy is also supposed to be able to produce a packaged, versioned output data set so it handles some of the data mgmt pain and has an API.   These are nice, but of course you pay for it.  It would be good if the open source tools were to incorporate more of these features.  I imagine that it will come, but slowly.",NA
"1820",1819,65,"I will second cvat. Nice tool and very actively developed. We've been using it internally at my company after trying out a few alternatives like VOTT, labelme, etc.",NA
"1821",1820,66,"Can people register for the virtual conference now? Or are they putting limitations on the number of people?",NA
"1822",1821,66,"Does anyone know what the original registration fee was?",NA
"1823",1822,66,"Actually, this makes me start to think about the benefits of hosting an offline conference other than networking.",NA
"1824",1823,66,"Can we get the presentation videos after all, if not being able to register?",NA
"1825",1824,66,"Any idea if ICASSP will go the same route? It's scheduled in the first week of May in Barcelona",NA
"1826",1825,66,"Why make a distinction between poster and talk in virtual format?  Time and space are no longer constraints.",NA
"1827",1826,66,"Man, this will suck for networking :/",NA
"1828",1827,66,"I wonder if ICPR will follow the same path, it's in Italy after all. But it's not until September so who knows, by then maybe everyone will be immune over there.",NA
"1829",1828,66,"Excellent decision by the organizers. Hosting a conference in Africa due to political reasons was a silly decision anyways. I hope they learned their lesson.",NA
"1830",1829,66,"What would be the benefit of registration? I'm guessing all the presentation videos would be available online anyways",NA
"1831",1830,66,"IMO hosting ICLR in Africa was one the best decisions made by a major scientific conference in recent years. Gutted that it turned out like this, and hopefully Ethiopia will still get a chance to host.",NA
"1832",1831,66,"If it's only online, why isn't it free to watch? Open Science anyone?",NA
"1833",1832,66,"They're probably too late to cancel, but how wise of them that they finally did.",NA
"1834",1833,66,"Usually ML registration fees are ~ 500-800 USD.",NA
"1835",1834,66,"Colleague (non-student) said their reg was 550",NA
"1836",1835,66,"That's the main benefit. Also having a one on one discussion with an author presenting their poster can be nicer than a generic talk.",NA
"1837",1836,66,"Don't know about ICASSP but Interspeech has already changed to in-person or online.",NA
"1838",1837,66,"Space might not be, time is very much still constrained",NA
"1839",1838,66,"Quality.",NA
"1840",1839,66,"We will have to use artificial networking....I’ll see myself out",NA
"1841",1840,66,"[deleted]",NA
"1842",1841,66,"Keeping everyone safe is the right move.",NA
"1843",1842,66,"How about using VR? 🙂",NA
"1844",1843,66,"Where does the money to organize everything come from?",NA
"1845",1844,66,"Some of us are illegal in Ethiopia and easily face imprisonment for at least one year. So, no thank you, Ethiopia should fix itself before I'm going to risk my freedom.

Nevertheless, Africa is large, and welcomed me when I helped open a new Master in Machine Intelligence.",NA
"1846",1845,66,"I hope so too. Would love to see Africa get another chance soon at hosting a top ML conference.",NA
"1847",1846,66,"Please work for free on my research ideas.

Also while you're at it, do organize a conference but buy a computer and a camera on your own budget.

What? An event organizer? To make the conference go smoothly? Please handle it, it's online, it should be free. Developers cost what? $1000+ per hour? Well, it's your budget, after all it's online it's free.
Sponsors are unhappy? Why do we need sponsors, it's online it's free.

Eating? You don't need to eat, it's online.",NA
"1848",1847,66,"I think i paid 450 as a student",NA
"1849",1848,66,">Don't know about ICASSP but Interspeech has already changed to in-person or online.

Where did you get this from? The [official website](http://www.interspeech2020.org/) just says it has been delayed.",NA
"1850",1849,66,"That's only true if everything is livestream, I don't think it is.  If I want to watch a full talk for a paper accepted as a poster, why shouldn't I be able to?  What's the rationale for treating the papers differently?",NA
"1851",1850,66,"Quality is in the eye of the beholder.  If I see a spotlight that I like and I want to see more, why should the answer be ""no"" for some papers but not others?  What's the rationale for treating one type of paper differently from another?",NA
"1852",1851,66,"Lol do you mean Neural Networking?...I'll be on my way as well",NA
"1853",1852,66,"I'm excited for the chance to try out the concept. If we are being honest, traditional networking was never the best method anyway and exclusionary to anyone not fitting the mold. Now we can try networking based on research interests instead of alcohol preferences.",NA
"1854",1853,66,"As a nice side effect we'll save tons of CO2 emissions.",NA
"1855",1854,66,"[deleted]",NA
"1856",1855,66,"as someone who has no clue, why are you illegal there?",NA
"1857",1856,66,"What are you talking about mate? Your paper will be published anyways, so people will have access to your knowledge. Maybe you should question yourself, what purpose research has?

It's a **virtual** conference. I don't get the ""if you don't like it, found your own"" argument. That's bs. In general research conferences as a business model are questionable anyway. You could easily stream the conference on youtube **without** developer costs.

But I get it, you are a fan of closed science, cause ""you gotta pay the costs"".",NA
"1858",1857,66,"I don't get it. Since when is ICLR paying the researcher for their work. 

From my understanding, the fees are for paying the venue and other stuff. I would assume that the remaining of the fees is for paying the staff and perhaps setting up the infrastructure. But, definitely not for paying developers or researcher.",NA
"1859",1858,66,"your link has the info

> We look forward to running IS2020 and related events in Shanghai as a well-attended conference, and will augment with remote participation by a fraction of the attendees.  Details related remote participation will be announced in due course.  

https://isca-speech.org/iscaweb/index.php/newsroom/293-cvirus-2021-2

> Authors who have a paper accepted to Interspeech 2020 will have the option either to attend physically, or to attend remotely. Remote attendance would also be associated with a reduced registration fee. More information will be provided soon.",NA
"1860",1859,66,"And recurrent neural networking is when you constantly forget that you already met this person and do the whole ""ah yes I remember!"" dance.",NA
"1861",1860,66,"[deleted]",NA
"1862",1861,66,"The main reasons being two: unawareness, it doesn't affect me.",NA
"1863",1862,66,"Because I am a boy who's romantically interested in other boys.",NA
"1864",1863,66,"I’m transgender.",NA
"1865",1864,66,"Organizing an event has lots of costs besides accomodation, venue and transportation even though they make the bulk of it:

logistics, planning, video stream, recording video + slides, cameras, post-production, chasing-up people, handling sponsors.

I'm not fan of closed science far from it.
I however would prefer that leading research is given the best stage to present and not a poor stream fully pixelated with spotty sound.

Also your ad hominem is off-topic, but I guess you're one of those that prefer researchers and PhD students to starve because everything they do should be free.",NA
"1866",1865,66,"My point is that just because something is online doesn't make it cheap.

And staffing and infrastructure definitely are not free. 

Researcher work (or developer work or data scientist work) is valuable and supporting research also means putting money where your mouth is. That means paying enough so that all that organizational work is done professionally, especially with only a couple weeks away from the conference.

People worked hard all year and having a poor organization would be disrespectful for their hard work. I think asking for $50 is quite reasonable for a live video stream so that the team has funding to do a great conference.

Also do note that full online conference are:
1. A jump in the unknown for both organizers and attendees
2. Probably all companies capable of organizing are working at full capacity due to all the cancellations

That said, I also agree that Science is a common good and should be open. For example it strikes me very strange that public funding enable a lot of research worldwide and then it gets gated by journals for a ransom-like price.",NA
"1867",1866,66,"I’ll see your RNN networking & raise you.

Try general adversarial networking.(GAN)

GAN is where you beat each other up until you are both so black & blue you look the same and then become friends.",NA
"1868",1867,66,"I never attend these conferences because I can't afford the fees + cost of travel/accommodation. Also, I'm deaf, and I'm not guaranteed accessibility in most countries, and I can't afford to take an interpreter to the social networking events.

I've been pushing for more online/virtual conferences for years, and have gotten the most bullshit excuses for why it won't/can't happen.

Interesting to see it suddenly *can* happen after all.",NA
"1869",1868,66,"Sorry but that still doesn't add up. Just because the access to research and its presentation are free, they do not need to end up being a *""poor stream fully pixelated with spotty sound""*. And make no mistake, the presentation stage is a closed one. The knowledge and the ability to network are presented to a small audience, willing to pay for it. That's elitism and an example of closed science at its finest.

Your ad hominem argument is off-topic as well ;-) Well if you work for peanuts, it's your choice. There are postdocs and Ph.D. students starving with 50k/year.",NA
"1870",1869,66,"To be fair, the cost of the online conference must be so much lower than a live one that sponsors only could probably cover it entirely. With a broader audience, their reach would be even stronger.",NA
"1871",1870,66,"At NeurIPS I was able to request sign language interpreting.",NA
"1872",1871,66,"If you want quality video and audio, you need to invest.

It either takes time so that you can do multiple take/teach people about how to present online or it takes money/sponsoring (you could for example rent local venues or even say Google/Nvidia/Facebook offices + their presentation/video experts).
And organizing that takes time and skills which should be remunerated fairly and I think their price of $50 is reasonable.

And that's not even talking about the post-production for all the content that will be produced.",NA
"1873",1872,66,"I would expect sponsors to reduce the sponsoring. Also it's possible that instead of a dollar amount some sponsors choose ""I'll sponsor the venue"", ""I'll sponsor the visual/audio"".",NA
"1874",1873,66,"And what country was this in? Bet it was one that supports accessibility. Not all do.",NA
"1875",1874,66,"NeurIPS is always US or Canada and rarely Spain.",NA
"1876",1875,66,"That's good. US/Canada are good countries for accessibility.",NA
"1877",1876,66,"u/tuanomsok yes it was in Canada. I requested sign language interpreting for NeurIPS 2019, the most recent one. They were able to fund it.",NA
"1878",1877,67,"It's pretty common to mechanical-turk this kind of stuff. I know of several large streaming companies that have doe it. I also know of datasets for sale that contain timecode data.",NA
"1879",1878,67,"It looks like [Apple has licensed this from a company called musixmatch](https://about.musixmatch.com/business/customer-stories/apple-music), which has focused on this problem for 10+ years. Their solution seems to be a [combination of manual and algorithmic](https://www.quora.com/What-algorithm-does-Musixmatch-follow-to-find-songs). The beauty of that is they now have a proprietary data set of millions of manually verified annotations, which their ML setup can use to not only improve accuracy, but also estimate its confidence, which can then be used to decide where to use manual verification/correction.",NA
"1880",1879,67,"They used timecoded lyrics. Like in subtitle srt files.",NA
"1881",1880,67,"There have been companies that provided time-coded lyric databases for a long time. They probably use a mix of humans, interactive tools and semi-automated tools to do this.",NA
"1882",1881,67,"So someone somewhere had to manually match the timing of the lyrics for all the songs?",NA
"1883",1882,67,"I am not sure whether Apple Music did it alone. Maybe they bought rights. Or Mechanical Turk, as said by other comment-or. 

But whatever it is, the end product is really mind blowing. I always use this on AM and is the only feature which keeps me away from other streaming services. Spotify + Genius trivia behind lyrics was good before. But this is revolutionary. And the UI even. The color splash tweaking itself from the album art.",NA
"1884",1883,68,"Yes this is correct. MLFlow is designed for someone with Statistics, Data Science background.  kubeflow if ideal for a team with data engineer / software engineer with some know-how of kubernetes.",NA
"1885",1884,68,"Thanks for your response! Could you provide some specifics on what advantage Kubeflow provides? I would assume that going that route, which requires more skill/knowledge, would provide some non-negligible benefit. MLflow can also be deployed on Kubernetes, so what makes Kubeflow advantageous?",NA
"1886",1885,68,">MLFlow is designed for someone with Statistics, Data Science background

What exactly does MLFlow have that is specifically designed for people with a background in statistics/data science and not for data/software engineers?",NA
"1887",1886,69,"No link?",NA
"1888",1887,69," [https://www.youtube.com/watch?v=HlBGYxO8RaU&feature=youtu.be](https://www.youtube.com/watch?v=HlBGYxO8RaU&feature=youtu.be)",NA
"1889",1888,69,"Live chat was cool, got a lot of questions answered from the engineering team",NA
"1890",1889,69,"Thanks I can't believe I missed that haha",NA
"1891",1890,69,"What questions did you have? I liked that they talked about tensorflow.JS and react Native integrations.",NA
"1892",1891,70,"It's not outdated, but it's really about artificial intelligence in general, ML is only a small part of it. It also covers formal logic, game theory, agents, robotics... It's very broad.

It's definitely worth reading, at least the parts you care about. But if you want to get into ML, you may need something more specific.",NA
"1893",1892,70,"You should hold out for the 4th edition. [Its webpage](https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html) is up. Prices have been decided. Amazon is even taking [pre-orders](https://www.amazon.com/Artificial-Intelligence-A-Modern-Approach/dp/0134610997). But it's not released yet.

I think we're in the ""any minute now"" stage (probably in the next few weeks).",NA
"1894",1893,70,"It depends on what your goals are:   
if you want to go more into the engineering side, then this book probably won't be for you.   
If you want to know about theory (not only ML but the wider field of ai) then this is still the book to get.  
I own the global version which is significantly cheaper (different publisher and two non-essential chapters about philosophical implications of ML are missing, chapters are ordered differently). 

If you want to you could get the international version to save a lot of money.",NA
"1895",1894,70,"In addition to the other answers about ML, I'd point out that probabilistic approaches have become a lot more popular than the deterministic approaches emphasized in the book.",NA
"1896",1895,70,"It gives a good understanding of all the parts of AI that is outside of what we now tend to call ""Machine Learning"". Numerical optimization, constraint solving, logic, graph traversal.  Which are very useful techniques, and for some task more appropriate than ""ML"". So if you want to have a broad your perspective and a wide range of tools in your toolbox, it is good. However if you want to understand what people refer to as ""ML"", it is not the right book.",NA
"1897",1896,70,"Honestly, ML is moving so fast, specifically neural networks/transformer learning. Any books out there will be too slow to keep up, and the fundamentals are less relevant than would initially appear. If you wanted textbooks to study, things like linear algebra, optimization theory, etc might be a better place to gain a solid foothold. the rest can be found in recent seminal papers",NA
"1898",1897,70,"**Question**: How up to date is ""Artificial Intelligence - A modern approach""?

**Answer**: Yes",NA
"1899",1898,70,"Agreed: it gives a very good overview of the whole field of AI, in my opinion, and that's important background information. But it's not a specific guide to particular methods, generally speaking",NA
"1900",1899,70,"amazon says it's releasing April 11th 2020 in the product details section",NA
"1901",1900,70,"Peter was on Lex Fridman’s podcast and briefly mentioned the changes coming in the new book, mostly about recognizing the impact of deep learning, if i remember correctly.",NA
"1902",1901,70,"... which makes it not that useful to read IMO, although I agree it is very interesting.",NA
"1903",1902,71,"Looks quite useful, however it seems that, unlike TensorFlow lite, your framework does not support quantized networks. Most embedded/IoT devices do not have a floating-point unit, or at least not an efficient one.",NA
"1904",1903,71,"Are there any benchmarks comparing it's memory + latency performance to PyTorch/TensorFlow?",NA
"1905",1904,71,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [Pico-CNN v1.0 - Deploy your Networks on Embedded Systems and IoT Devices (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fh7ymp/picocnn_v10_deploy_your_networks_on_embedded/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"1906",1905,71,"Nice, great to see work in this area! I considered adding CNNs to emlearn, but with this out there I will probably just recommend to use this instead.",NA
"1907",1906,71,"Good, point. We actually looked into fixed point calculation already and will probably include this in a later release",NA
"1908",1907,71,"Not yet, we are working on optimized code currently and will benchmark it on a couple devices then.",NA
"1909",1908,72,"Title:Contrastive Representation Distillation  

Authors:[Yonglong Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian%2C+Y), [Dilip Krishnan](https://arxiv.org/search/cs?searchtype=author&query=Krishnan%2C+D), [Phillip Isola](https://arxiv.org/search/cs?searchtype=author&query=Isola%2C+P)  

> Abstract: Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: [this http URL](http://github.com/HobbitLong/RepDistiller).  

[PDF Link](https://arxiv.org/pdf/1910.10699) | [Landing Page](https://arxiv.org/abs/1910.10699) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/1910.10699/)",NA
"1910",1909,72,"This is such a simple yet effective idea. I recently used Contrastive Multiview Coding (by the same author) to learn representations in a self-supervised manner and it worked out really well. 

It's pretty clever how they applied it to knowledge distillation. Plus they provide a sound theoretical foundation to their work.",NA
"1911",1910,72,"Could you expand upon how you used CMC in a self supervised manner?",NA
"1912",1911,72,"Sure! It is basically an implementation of [this](https://arxiv.org/abs/1906.05849) paper.

Overall idea is: 

You take two views of an image. A ""view"" can be one or more channels of RGB or LAB format, segmented or pixel depth output of an image. In my case I used L as one view and AB as another view.

Now if you get a representation of views 1 and 2 of the same image, they should be similar since they represent the same information. In contrast, if you compare representations of view 1 of one image and view 2 of another image, these representations should be different.

Thus, given a representation of view 1 of an image and representations of view 2 of a bunch of images (including the image used for view 1), a model should be able to detect which of the view 2 representations belongs to the image used in view 1 representation. 

The loss used by the classifier is termed contrastive loss and is used to learn good representations.

EDIT: if you want to know details at a more technical or detailed level, I can share some notes I took on it.",NA
"1913",1912,73,"Unfortunately the algorithm is well documented and has received a ton of interest in the last year, so it was inevitable that it'll get implemented in a bunch of ML projects and packages, and they don't own Winograd fast matrix multiplication algorithms.

I am not a lawyer, but the issue seems to be that an ex-employee shared trade secrets, which Facebook would have otherwise been fine to develop independently or reverse-engineer. No patents or copyright violation claimed.

edit - see the last paragraph of the introduction on the filing https://www.universalhub.com/files/neuralmagic-complaint.pdf",NA
"1914",1913,73,"Too bad if you build your business model on just a matrix multiplication algorithm",NA
"1915",1914,73,"IMO, from reading this lawsuit, this seems like trumped up claims.

> Notes accompanying Mr. Park’s LinkedIn post credited Zlateski for his contributions “to sparse kernels""

Wow, he wrote sparse code for matrix multiplies and convolutions.

> The GitHub modification history for the Sparse GEMM JIT shows that the initial submitter is the lead of the FBGEMM software package at Facebook, Jongsoo Park, and Zlateski is listed as the first “reviewer” of the code.

He wasn't even writing the code...

TBH, the biggest takeaway I've gotten from this lawsuit is that Neural Magic's ""secret sauce"" just seems to be sparse kernels run on CPU - ie: the same thing 20 other startups are doing.",NA
"1916",1915,73,"Saw these people at NeurIPS, and they seemed like a decent bunch, too bad!",NA
"1917",1916,73,"Never thought I'd support Facebook in a copyright lawsuit",NA
"1918",1917,73,"Were they really banking on a secret algorithm business model, with said algorithm being well documented?",NA
"1919",1918,73,"Code patents are a joke.

Unless you ripoff actual source code, reaching similar solutions is inevitable.

It is the nature of being Turing-Complete.",NA
"1920",1919,73,"Facebook not paying people for what they create is kind of the business model.",NA
"1921",1920,73,"The difference in responses here vs if this was Google holding a patent is hilarious.",NA
"1922",1921,73,"Good luck winning _""non-competition agreement""_ in California!

https://www.huffpost.com/entry/understanding-californias-ban-on-non-compete-agreements_b_58af1626e4b0e5fdf6196f04",NA
"1923",1922,73,"Why do they sue Facebook and not the employee? Is it legal to move to a competitor and share trade secrets?",NA
"1924",1923,73,"I will fork it before they take it down",NA
"1925",1924,73,"I hope facebook pays, its not like they're lacking them monies
And the authors probably would do more research with that",NA
"1926",1925,73,"https://fortune.com/2018/02/05/waymo-v-uber-what-you-need-to-know-about-the-high-stakes-self-driving-tech-trial/",NA
"1927",1926,73,">Unfortunately the algorithm is well documented and has received a ton of interest in the last year

I am curious to learn more about the algorithm, could you share some resources about it?",NA
"1928",1927,73,">Too bad if you build your business model on just a matrix multiplication algorithm

This same sub complains that researchers don't make enough money...",NA
"1929",1928,73,"No no no, you dont understand. Our matrix multiplication algorithm multiplies matrices SO GOOD.",NA
"1930",1929,73,"or, you know, theft of trade secrets",NA
"1931",1930,73,"If an employee learns math on the job, that knowledge doesn't become the property of the employer, sorry Neural Magic",NA
"1932",1931,73,"What other startups are doing the same thing?",NA
"1933",1932,73,"> TBH, the biggest takeaway I've gotten from this lawsuit is that Neural Magic's ""secret sauce"" just seems to be sparse kernels run on CPU - ie: the same thing 20 other startups are doing.

But that is about as novel as applying deep learning to health analytics and we would have a majority here defending google in that case

https://healthitanalytics.com/news/google-tries-to-patent-healthcare-deep-learning-ehr-analytics",NA
"1934",1933,73,"> Neural Magic's ""secret sauce"" just seems to be sparse kernels run on CPU - ie: the same thing 20 other startups are doing.

so don't hire an employee from NM and get him to reimplement their proprietary stuff",NA
"1935",1934,73,"Which is also why patents don't serve the economic purpose they originally intended.

They used to be to protect the upstarts against incumbents, now they're used by incumbents to widen their moat.",NA
"1936",1935,73,"[deleted]",NA
"1937",1936,73,"They don't have a patent though.",NA
"1938",1937,73,"This isn't about patents, it's about an ex-employee sharing trade secrets instead of Facebook engineering it themselves.

>Zlateski breached the non-disclosure and non-competition agreement he signed with Neural Magic as Technology Director. Moreover, Zlateski and his new employer Facebook engaged in acts and conduct in the Commonwealth of Massachusetts that violate Massachusetts trade secrets laws, Chapter 93A, and the Defend Trade Secrets Act of 2016. Facebook has refused repeated requests to cease these acts and remove misappropriated material from its own code. Neural Magic has therefore been forced to bring these claims to protect its intellectual property.

[https://www.universalhub.com/files/neuralmagic-complaint.pdf](https://www.universalhub.com/files/neuralmagic-complaint.pdf)",NA
"1939",1938,73,"> Why do they sue Facebook and not the employee? 

Because only Facebook would be able to pay real damages.",NA
"1940",1939,73,"> Is it legal to move to a competitor and share trade secrets?

It depends on the country laws and the contract agreements. Usually, you get a plus in exchange of the prohibition to work on similar projects in other companies after the contract ends.",NA
"1941",1940,73,"Where is the code for it?",NA
"1942",1941,73,"Yeah, cause fuck ethics, right?",NA
"1943",1942,73,"Me too, don't get the negativity here towards that. The guys are legit, had a NDA and \*possibly\* were scooped by a former employee.",NA
"1944",1943,73,"There was another suit against Facebook/Oculus for this behavior as well right? Not open sourcing but trade secret poaching?",NA
"1945",1944,73,"If you wanted to make money, you'd go become an engineer or even better get an MBA and become an executive.

You do research because ramen noodles are your favorite food.",NA
"1946",1945,73,"It's not about the math, it's about the internal expertise and specifics of their implementation strategy that is protected. They could have hired a bunch of math geniuses and do it themselves, no problem. What's wrong is hiring the competitor's employees and having them share their secrets.

- Several companies can do the same thing
- One company might do it better
- A big company can't hire a high level employee from the better startup and just steal their implementation by asking them. All their work and effort in engineering the best solution to the problem in the best way, is now looted by the competitor. That's what the laws are there to prevent.

The justification is the same as for copyright and patent protections, the idea being that it prevents the big players from exploiting their financial leverage to just copy and beat competition. Same goes for a company that has a special implementation of some algorithm that nobody can beat, and hiring an employee in order to steal their secret sauce fromula.

FWIW, I think the idea of matrix multiplication or other specific speedups (e.g., a fast convolution algorithm on cpu), is not a good basis for a company, unless you can prove it won't become ubiquitous and implemented everywhere in a few years anyway and make it redundant. This is what I would expect from their algorithm, so I wouldn't have invested in it, unless there was proof that they had a special way of implementing it that nobody would be able to reproduce or beat.",NA
"1947",1946,73,"The definition of a strawman argument.",NA
"1948",1947,73,"Not all necessarily startups, but the point is that there's a lot of parties interested in doing sparse computation on non-GPU chips. 

[SLIDE](https://insidehpc.com/2020/03/slide-algorithm-for-training-deep-neural-nets-faster-on-cpus-than-gpus/): ""You don’t need to train all the neurons on every case ... If we only want to pick the neurons that are relevant, then it’s a search problem.""

[Cerebras](https://www.cerebras.net/): They get a lot of their advantage from sparsity - their pitch is ""Design extraordinarily sparse networks""

[Block sparse kernels](https://openai.com/blog/block-sparse-gpu-kernels/)

[Myrtle.ai](https://myrtle.ai/): ""The unrivalled efficiency achieved by Myrtle.ai results from exploiting unstructured sparsity and optimal quantization.""

Essentially, nearly every single startup working in the ""I want to make neural networks fast"" is doing something with sparsity. The 2 big obvious speedups in deep learning are quantization and pruning. Quantization requires specialized kernels as well (which everybody is also writing). Pruning results in sparse networks, which requires sparse kernels. 

There's a lot of work in getting this stuff to work to well in practice - maybe Neural Magic even has the best compiler to do so. But it's not revolutionary, and I'd be surprised if the guy named was absolutely crucial for what Facebook has now.",NA
"1949",1948,73,"He didn't reimplement anything (based off their lawsuit). He was listed as ""first reviewer"" - ie: somebody else asked him for help.

His main work had nothing to do with the stuff listed in the lawsuit.",NA
"1950",1949,73,"It's because their research is all very good. Half of google's research is very good, the other half is ""look what good results we get using 100x data and 1000x compute compared to others"".",NA
"1951",1950,73,"Is fairness now solely defined by legal code. In that case the fairest length of a copyright is however long is necessary to keep Mickey Mouse out of public domain.",NA
"1952",1951,73,"When you're an expert, you're paid for your expertise. Is he supposed to unlearn the things he learned at his previous company?  


Filing a patent requires public disclosure in exchange for legal protection. Why do we want to give legal protection to secrets as well?",NA
"1953",1952,73,"Ethics would demand you have more information before claiming the ability to pass judgement. Most comments on here are pro Facebook already. Ideally, the legal process should be the same as any scientific hypothesis test. Acquisition of evidence to try and distinguish between different hypothesis. Unless you have hard evidence that Facebook was within their legal rights in how they conducted that research, the ethical thing to do is withhold judgement entirely. The fact that the former employee had peer review logs in the GitHub repo for this algorithm is very problematic, and implies Facebook may well have been in the wrong.

On the other hand, if you're just saying 'Facebook should lose because they have more money' is a terrible argument, then I agree with you completely, haha.",NA
"1954",1953,73,"Yeah the comments all rush to a conclusion where Facebook has done nothing wrong and the plaintif has no chance, arguing that ""you can't patent maths"".

Welp, you can't patent maths, but this sure as hell doesn't mean you can't protect code with licensing laws.

If it is proven Facebook stole the code by having someone reimplelent it as is, it definitely could be judged as a breach of contract.",NA
"1955",1954,73,"Looks like you have to protect your talent by compensating them well (bonuses, large salary etc) and providing an amazing work environment.",NA
"1956",1955,73,"Yeah except the most likely thing is not what you describe. Most likely their solution is largely equivalent and not differentiated over other solutions. They like everyone else read the same set of papers and are following the same line of research. They hire someone and say here read these papers and learn about this topic. And then what? Is the employee's knowledge of the literature now the property of the employer? If the employee leaves the firm, does any future work that may build upon the knowledge gleaned in those papers intellectual property theft",NA
"1957",1956,73,"Ah I see, I'm aware of those ones, I was interested in other sparse CPU software startups. Cerebras is a little bit different, since a lot of the hardware players are using sparsity- it is a lot easier to use sparsity in hardware than software generally speaking.",NA
"1958",1957,73,"> Quantization requires specialized kernels as well

Hello.  By quantization do you mean shit like reducing representation granularity to get faster math and parallel math, like tpu?",NA
"1959",1958,73,"now they just have to prove that they didn't pick his brain unofficially",NA
"1960",1959,73,"I mean, also, they are the underdog... compared to Google.

Google's ML branches are like 10x the size of Facebook's.",NA
"1961",1960,73,"I don't understand your comment, so it's possible that you have responded to the wrong one.",NA
"1962",1961,73,"Generally speaking, a trade secret confers economic benefit on its holder because the information is not publicly known. It is protected because, otherwise, what's to stop a big company poaching a smaller company's employees to access this information. 

Like, Toyota spends a ton to hire a top employee at Ford in order to learn how they accomplish some very special manufacturing process.",NA
"1963",1962,73,"I feel that a clean room reimplementation should be protected as the code hasn't been copied, just the math.

Of the employee took files or notes etc then that'd be different, but I'm all for employees being able to reuse their knowledge and skills after leaving a company.",NA
"1964",1963,73,"[right...](https://techcrunch.com/2019/03/21/tesla-sues-former-employees-zoox-for-alleged-trade-secret-theft/)",NA
"1965",1964,73,"Most of the people doing sparse CPU software are at the bigger companies I think. I suspect selling software is difficult - especially in comparison to Google/FB.

So the ""block sparse kernels"" from OpenAI, FBGEMM from FB, don't know of one from Google but I'd be surprised if nobody was working on it there.",NA
"1966",1965,73,"Well, not necessarily TPU, and all math in DL is already parallelized, but yeah it's faster.

So instead of using FP32 you use FP16 or FP8.",NA
"1967",1966,73,"I feel so sad for facebook. It must be so hard for them. I’m just full of sympathy.",NA
"1968",1967,73,"Its the assumption of your previous comment about having a patent as if that would end the discussion on ""fairness""",NA
"1969",1968,73,"The whole idea of trade secrets is bullshit anyway. If it's a good idea, patent it. Trade secrets lead to stagnation of technology and wasted effort and should have no legal protection.",NA
"1970",1969,73,">but I'd be surprised if nobody was working on it there.

Working on it there :) [https://arxiv.org/abs/1911.09723](https://arxiv.org/abs/1911.09723)",NA
"1971",1970,73,"yeah i just meant it as a low hanging fruit example

but the general practice of ""fewer bits more zoom""

does this also cover float -> fixed (which can be a big speedup,) or would that go under a different title",NA
"1972",1971,73,"Knew about XNNPack but didn't know they were doing sparse kernels too. Sparse kernels are kind of a ""no duh"" for big companies with the types of workloads Google/FB do. 

They can spare the engineering expense of turning their models into sparse models, they often have the type of sparse models that are amenable to sparse kernels in the first place, and they have the expertise to write these sparse kernels at all :) Neat stuff.",NA
"1973",1972,74,"One thing you can do is just concatenate the image 3 times so the input image would be of size (h,w,3) and plug this into a pre-trained network like VGG. 

Also keep in mind that each of these pre-trained networks have their own protocols for normalizing the inputs. VGG subtracts the mean image from imagenet. Xception normalizes to range (-1,1). You get the idea. Look into this beforehand so you get a proper comparison.",NA
"1974",1973,74,"Stacking the single channel image will work perfectly well. 

Another way is to replace the first layer with a single channel version then just copy the weights from the channel of your choice from the pretrained model.",NA
"1975",1974,74,"Oh awesome, thanks for the advice! I’ll look into that for sure",NA
"1976",1975,75,"Title:Reservoir memory machines  

Authors:[Benjamin Paassen](https://arxiv.org/search/cs?searchtype=author&query=Paassen%2C+B), [Alexander Schulz](https://arxiv.org/search/cs?searchtype=author&query=Schulz%2C+A)  

> Abstract: In recent years, Neural Turing Machines have gathered attention by joining the flexibility of neural networks with the computational capabilities of Turing machines. However, Neural Turing Machines are notoriously hard to train, which limits their applicability. We propose reservoir memory machines, which are still able to solve some of the benchmark tests for Neural Turing Machines, but are much faster to train, requiring only an alignment algorithm and linear regression. Our model can also be seen as an extension of echo state networks with an external memory, enabling arbitrarily long storage without interference.  

[PDF Link](https://arxiv.org/pdf/2003.04793) | [Landing Page](https://arxiv.org/abs/2003.04793) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.04793/)",NA
"1977",1976,76,"My first thought is to use document embeddings and do some kind of [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) on them. No idea how well this would work in practice.",NA
"1978",1977,76,"**Hierarchical clustering**

In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:

Agglomerative: This is a ""bottom-up"" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.

Divisive: This is a ""top-down"" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.

***

^([ )[^(PM)](https://www.reddit.com/message/compose?to=kittens_from_space)^( | )[^(Exclude me)](https://reddit.com/message/compose?to=WikiTextBot&message=Excludeme&subject=Excludeme)^( | )[^(Exclude from subreddit)](https://np.reddit.com/r/MachineLearning/about/banned)^( | )[^(FAQ / Information)](https://np.reddit.com/r/WikiTextBot/wiki/index)^( | )[^(Source)](https://github.com/kittenswolf/WikiTextBot)^( ] Downvote to remove | v0.28)",NA
"1979",1978,76,"I didn't really consider hierarchical clustering, because in the examples I've worked with, hierarchical clustering could give branches with different lengths between the leaves and the roots of the hierarchy, whereas I need the branches to always have a fixed length corresponding to the number of levels in the Hierarchy.",NA
"1980",1979,76,"There are a couple ways around that. One is to simply ignore the branch lengths. Another is to cut the tree at set depths (branch length depths, not tree topology depths) and collapse everything within one depth bucket down into siblings.

Edit to add: the first approach is bad for various reasons. The second approach is what I would do.",NA
"1981",1980,77,"This is certainly a fascinating post.  However, one issue with interpretability approaches like this is it's hard to tell what interpretations are simply cherry-picked.  Humans tend to severely overinterpret things in my experience, often writing off clear violations of their interpretation as irrelevant ""edge cases"" rather than accepting their current interpretation is incomplete/wrong.  One prominent example of this outside of ML is the BCS theory of superconductivity:  [https://arxiv.org/abs/2001.09496](https://arxiv.org/abs/2001.09496)",NA
"1982",1981,77,"Cool read. I like the idea of exploring connections as algorithms. And the visualizations are pretty.",NA
"1983",1982,77,"This paper is actually amazing. It is so epic to get some insights or even guesses at what is being formed in learning networks!",NA
"1984",1983,77,"Wow! That was very informative *and* easy to understand at the same time. Well done.",NA
"1985",1984,78,"A drug candidate for coronavirus was recently found among 10000+ candidates using ES. I think the use cases are few and far between for ES but it's fast for needle-in-hay searches",NA
"1986",1985,78,"We just today open sourced our production learning to rank system that uses NES and is a part of the system that powers Search at Etsy.  Very much not a toy problem, though we did get a paper out of it.",NA
"1987",1986,78,"CMA-ES is pretty much the state of the art for any kind of black-box high dimensional global optimization problem. I think it would be harder to not find places using it.",NA
"1988",1987,78,"Fwiw, we use genetic algorithms to fit a model that doesn't have a straightforward way to be fit. I've been looking into moving from Genetic algos to cma-es",NA
"1989",1988,78,"Newer CAD programs like Autodesk Fusion 360 seem to feature evolutionary algorithms under the buzzword ""generative design"". I'm not a mechanical engineer though, so I have no idea how widespread the use of these tools is.",NA
"1990",1989,78,"Thanks. Yeah, maybe the lack of replies is also an indication to what you said.",NA
"1991",1990,78,"This is the kind of stuff I am looking for. Can you provide a link to the system you open sourced? 

I would also love to know more about what other approaches did you try and how does NES perform compared to other approaches. 

Thanks!",NA
"1992",1991,78,"It should be cautioned that high-dimensional in CMA-ES case is only around 1k to 10k dimensions: doing the matrix inversion to sample from the covariance matrix is O(n^2) in memory and similar complexity for compute, which limits it at high dimensions.

That said, there are variants of CMA-ES which scale better by relaxing the covariance requirements in the matrix, which hinders some types of problems (e.g. hypersphere) but scale linearly in dimensions and make it more suitable for larger dimension problems.",NA
"1993",1992,78,"Not saying you are wrong but you could contribute better to the discussion if you backed up your claim with at least one example. I am well aware that CMA-ES is SOTA and there's a wiki page listing applications of evolutionary algorithms. However as I said, most of them tend to be research or toy problems, not commercial applications.",NA
"1994",1993,78,"Can you provide more details please? What is the nature of the data you are trying to fit?",NA
"1995",1994,78,"Woah that's an area where I didn't expect to see EAs. Interesting!",NA
"1996",1995,78,"Sure, happy [to](https://github.com/etsy/Evokit).  

Our marketplace requires optimizing for multiple, competing objectives for which gradient methods typically have a hard time with since often those objectives are not differentiable.  Other solutions have been proposed for certain classes of objectives (such as the paper out of Amazon last year), but are constrained to both listwise metrics and pointwise evaluation.

By using NES and Canonical ES variants, we can directly train ranking policies that can optimize for the entire marketplace directly, so incorporate more interesting constraints such as fairness via the Gini Index.  Since our policies are also non-differentiable, since they require windowed computations, heuristic aggregators, and stochasticity, black box optimization is basically all that's left in the table.

ES is nice since salimans et Al showed simply tricks to scale the gradient estimation across a cluster and has performed reasonably well for us.",NA
"1997",1996,78,"Look at BBOB benchmarks here: [https://coco.gforge.inria.fr/](https://coco.gforge.inria.fr/)   
The optimization community is the most careful CS research community I've worked with, everything gets evaluated scientifcally.  [/u/Red-Portal/](https://www.reddit.com/user/Red-Portal/) is pretty much correct this is sota.

CMA-ES is very widely used in industry, academia, R&D in many domains, from space rockets/booster control optimization to a lot of physical sciences etc. I've helped integrating it at CERN. 

Disclaimer: I'm the (busy elsewhere) maintainer of the C++ libcmaes OSS lib.",NA
"1998",1997,78,"It's actually been around for some years. For example, antenna for some NASA satellites were designed using evolutionary algorithms

https://en.m.wikipedia.org/wiki/Evolved_antenna",NA
"1999",1998,78,"Thank you. This was useful to understand the landscape of black box algorithms better.",NA
"2000",1999,78,"Desktop link: https://en.wikipedia.org/wiki/Evolved_antenna
***
 ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^297570. [^^Found ^^a ^^bug?](https://reddit.com/message/compose/?to=swim1929&subject=Bug&message=https://reddit.com/r/MachineLearning/comments/fgqul2/discussion_are_there_any_commercial_applications/fk72r1e/)",NA
"2001",2000,78,"**Evolved antenna**

In radio communications, an evolved antenna is an antenna designed fully or substantially by an automatic computer design program that uses an evolutionary algorithm that mimics Darwinian evolution. This procedure has been used in recent years to design a few antennas for mission-critical applications involving stringent, conflicting, or unusual design requirements, such as unusual radiation patterns, for which none of the many existing antenna types are adequate.

***

^([ )[^(PM)](https://www.reddit.com/message/compose?to=kittens_from_space)^( | )[^(Exclude me)](https://reddit.com/message/compose?to=WikiTextBot&message=Excludeme&subject=Excludeme)^( | )[^(Exclude from subreddit)](https://np.reddit.com/r/MachineLearning/about/banned)^( | )[^(FAQ / Information)](https://np.reddit.com/r/WikiTextBot/wiki/index)^( | )[^(Source)](https://github.com/kittenswolf/WikiTextBot)^( ] Downvote to remove | v0.28)",NA
"2002",2001,79,"Thanks!",NA
"2003",2002,79,"looks interesting. thanks.",NA
"2004",2003,80,"Whats the difference from fastspeech?

It would be nice to have image of model architecture in repo.",NA
"2005",2004,80,"Why are there emoticons in the description?",NA
"2006",2005,80,"Snippets sound great, inference is fast.

I wonder if we can use it to condition the TTS that we have trained on external speaker embeddings, in order to manipulate the voice qualities.",NA
"2007",2006,80,"I had a chance to speak with the FastSpeech folks about their architecture at NeurIPS. Their model does have an attention mechanism, it's just a hard attention mechanism extracted from a pre-trained duration predictor.

How does ForwardTacotron avoid all of that?",NA
"2008",2007,80,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datascienceproject] [⏩ForwardTacotron - Generating speech in a single forward pass without any attention! (r\/MachineLearning)](https://www.reddit.com/r/datascienceproject/comments/fgophp/forwardtacotron_generating_speech_in_a_single/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"2009",2008,80,"Link to the paper please",NA
"2010",2009,80,"The main difference is that we use LSTMs instead of Transformers to avoid self-attention. We will do an image of our model architecture soon. Sorry for being lazy ;)",NA
"2011",2010,80,"What loss you have at the end?

Also your len predictor same as fastspeech?

I think the main problem of fastspeech is the necessity of extracted attention alignments.",NA
"2012",2011,80,"Yap len predictor is same as fastspeech. We've updated our model architecture figure. Have a look at our repo again: [https://github.com/as-ideas/ForwardTacotron](https://github.com/as-ideas/ForwardTacotron). For PreNet we're using CBHG which is also used in Tacotron.",NA
"2013",2012,80,"Why do you want to avoid self-attention?",NA
"2014",2013,81,"I would have liked to see them evaluate models on out-of-domain datasets with misleading texture biases (e.g., train on ImageNet, test on stylized ImageNet), I think that would be more convincing then the negative results in 5.6 to show that texture bias really has been 'fixed'.  [Prior](http://papers.nips.cc/paper/9237-learning-robust-global-representations-by-penalizing-local-predictive-power.pdf) [work](https://arxiv.org/abs/1910.02806) has followed that method.",NA
"2015",2014,81,"Im the first author of this paper! Would be happy to answer any questions",NA
"2016",2015,81,"I can see this would work, but from an image processing perspective ""shape"" isn't well-defined. I guess it's the same as ""edges"".

> causes the learned CNN models to be biased towards high-frequency tex- tural information, compared to low-frequency shape information in images.

Edges have low-frequency information but aren't themselves low-frequency. They're a sum of all frequencies up to the Nyquist limit, like a square wave. So you can detect them with a high pass too.

But a CNN should be able to learn a Gaussian blur, so introducing one might not help that much. Have you tried a nonlinear filter to remove texture detail, like NL-means/K-means/median filtering?",NA
"2017",2016,81,"Interesting! That’s a good idea. We added the stylized ImageNet pretty early on, and almost forgot to revisit it — likely why the results are so seemingly contrived (randomly sampling classes). I was actually super excited / surprised about the CUB results, which is why we decided to include them. 

Would be cool to run those experiments.",NA
"2018",2017,81,"To me, I have some questions about your baselines. For example, I could imagine that your method is redundant with data augmentation by adding random amounts of gaussian blur to the image. Is that included in your baseline runs of VGG?

Also, for example, your baseline accuracies for VGG seem lower than what [VGG is capable of](https://towardsdatascience.com/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c).

>  On a single test scale, VGG achieved a top-1 error of 25.5% and a top-5 error of 8.0%.

While you guys seem to report a top-1 error of 37% and top-5 error of 17% on VGG, if I'm reading this correctly. I assume this is because the training process differed somehow. Could you elaborate on how?",NA
"2019",2018,82,"As someone relatively new to the field, I want to make sure I'm thinking about this correct. So are frameworks like pytorch, tf, jax, cafe, etc at their core, just ways of building computational graphs and then creating ways to calculate gradients (autograd) for these graphs? And they differ in abstraction levels, cpu vs. gpu calculations?",NA
"2020",2019,82,"Thanks a lot. I wish that Jax official document can include an explanation like this!",NA
"2021",2020,82,">  JAX on the other hand makes you express your computation as a Python function, and by transforming it with grad() gives you a gradient function that you can evaluate like your computation function

Isn't this similar to TensorFlow 1?  (tf.gradients)",NA
"2022",2021,82,"But Jax is a low-level element of the future TF2 stack so why worry now? Or do I have JAXs meaning completely upside down",NA
"2023",2022,82,"Sort of, I think it depends on how you define a computational graph. If you consider computer code as a computation graph, then sure.

Now days, most frameworks consider ""dynamic"" computational graph and implement this by tracing the computation and implement reverse differentiation by storing the trace (not unlike autograd as you pointed out). Additionally, they'll provide a mechanism for creating a possibly more optimized static graph based on what was traced.

However, a few frameworks provide truly unique solutions and features that most others don't have. Jax, in my opinion, is one of them. The difference between jax and, say, pytorch's jit might feel small, but jax's functional approach allows for some truly powerful features that could not be easily added to pytorch, tf or cafe.

By restricting everything to be pure functions, jax can confidently trace and transform the function as it pleases while never exiting the function's stack frame (excluding jax.jit, of course) until the final return value is computed. If there is a bug in your code, you can always see the exact line that caused it and, provided you're running a debugger, you can jump into the relevant stack frame and figure out what went wrong. This is much more powerful than what most other frameworks do when they report which line of code might have caused an issue (or sometime they can't even).

As a result of only allowing pure functions, jax can offer really cool transforms like `jax.vmap` which vectorizes functions along arbitrary dimensions or forward and backward differentiation all with fairly simple implementations.

(The compiled frameworks, e.g., Swift for Tensorflow, Julia's Zygote/Flux, also deserve to be mentioned as providing truly unique features, some similar to jax).",NA
"2024",2023,82,"Shorter answer: Basically, yes",NA
"2025",2024,82,"Thats one way you can think about it sure.  You can also think of programming in any high level language as a way of building a computational graph (for optimization or whatever).  

I think this is however mostly a hindrance to most programmers when thinking about programming neural networks.   I recommend looking up forward mode automatic differentiation, trying to understand what its drawbacks are, then trying to understand backward mode automatic differentiation, and then most of the time forget about either and just think about differentiation as a higher order function.",NA
"2026",2025,82,"Yes",NA
"2027",2026,82,"It's very different behind the scene. While `tf.gradients` constructed the entire computational graph using TF's structures (well, they didn't even allow you to use your own structures...), Jax constructs the computational graph somewhat lazily, and is much more deeply integrated into Python.

The end result is something that feels more like PyTorch than TensorFlow, imo.",NA
"2028",2027,82,"When I first read the limitations of Jax, it stroked me at how similar they were to the limitations of the old TF!
I liked how the old TF would allow to build a functional-style graph, but everybody else in my team started screaming when they learned that it could not do ""for"" loops and ""if"" statements... That kind of reaction is what lead TF 1 to make eager mode and TF 2.

I hope the team working on Jax has learned from the TF experience and will manage to build a good framework that manage these issues from the start.",NA
"2029",2028,82,"You're describing XLA, jax is more like Numpy + Autograds written on top of XLA, and now there are more high level ML focused libraries written on top of JAX, like Haiku, Flax and Trax",NA
"2030",2029,82,"Jax and TF2 are completely separate.  Unless I've misunderstood what you're asking?",NA
"2031",2030,82,"Ah okay that helps me appreciate it a little more. I'm just worried that by the time I'll learn one framework thoroughly it'll be out of date, and so I want to know the fundamentals to help contextualize each framework in my mind. Thanks.",NA
"2032",2031,82,"Yeah, philosophically Jax is very different.  It relies much more on functional programming ideas and I'd honestly love to see it implemented in Scala.",NA
"2033",2032,82,"Well, they do have a differentiable ""if"" statements somehow, using ""np.where"". For all the rest, they seem to have everything done or planned: [https://jax.readthedocs.io/en/latest/notebooks/Common\_Gotchas\_in\_JAX.html#Structured-control-flow-primitives](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#Structured-control-flow-primitives)",NA
"2034",2033,82,"Lol hard at “everyone else in my team started screaming”... going through that phase as well",NA
"2035",2034,82,"You got that right. I don't see Jax replacing any of the main frameworks as the go-to toolkit seeing how usable it is.",NA
"2036",2035,82,"That's a risk you'll have to live with for the coming time. Nobody knows if we are evolving towards one framework or not.  
If you're writing code, try to separate code touching your DL framework out from the rest of your code, so that adding a new algorithm in torch or JAX won't screw up your other stuff.",NA
"2037",2036,83,"Title:AutoML-Zero: Evolving Machine Learning Algorithms From Scratch  

Authors:[Esteban Real](https://arxiv.org/search/cs?searchtype=author&query=Real%2C+E), [Chen Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang%2C+C), [David R. So](https://arxiv.org/search/cs?searchtype=author&query=So%2C+D+R), [Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&query=Le%2C+Q+V)  

> Abstract: Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.  

[PDF Link](https://arxiv.org/pdf/2003.03384) | [Landing Page](https://arxiv.org/abs/2003.03384) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.03384/)",NA
"2038",2037,83,">Each cycle picks T<P algorithms at random and selects the best performing one as the parent (i.e. tournament selection, \[25\]). This parent is then copied and mutated to produce a child algorithm that is added to the population, while the oldest algorithm in the population is removed.

As usual with ""evolving"" in the title, the actual methods are toyish, not evolutionary. It's 1980s level. In fact, that \[25\] is a review from 1991, which in turn cites the actual source for their method from 1981. LOL",NA
"2039",2038,83,"To me, this would be potentially interesting if it managed to solve a problem where traditional AS fails to takes a lot of tweaking to get results... or if it were more generic.

As it stands.... It just seems like a very inefficient way to get good results on cifrar, which is pretty boring.",NA
"2040",2039,83,"Seems to be relevant to Jürgen Schmidhuber's phd work and the concept of Genetic Programming",NA
"2041",2040,83,"Agreed. It looks interesting at first, but looking closer at the OP dictionary, basically the only thing it can do is affine transforms and various scalar nonlinearities, which means it will only come up with variations of NNs with potentially exotic activation functions. 

The co-learning of predictor and learner is interesting, but very dumb: the learner should at least have access to the predictor in some sense, either through operations such as grad(), or some more advanced ability to traverse the computational graph.",NA
"2042",2041,83,"> it will only come up with variations of NNs with potentially exotic activation functions

I am not capable of attacking the detailed math, but the above statement covers a significant number of ArXiv papers!",NA
"2043",2042,83,"Right, and nothing wrong with that. I'm just adding some nuance to the insinuation from the paper that it can come up with any algorithm possible.",NA
"2044",2043,84,"Title:StyleGAN2 Distillation for Feed-forward Image Manipulation  

Authors:[Yuri Viazovetskyi](https://arxiv.org/search/cs?searchtype=author&query=Viazovetskyi%2C+Y), [Vladimir Ivashkin](https://arxiv.org/search/cs?searchtype=author&query=Ivashkin%2C+V), [Evgeny Kashin](https://arxiv.org/search/cs?searchtype=author&query=Kashin%2C+E)  

> Abstract: StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces' transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.  

[PDF Link](https://arxiv.org/pdf/2003.03581) | [Landing Page](https://arxiv.org/abs/2003.03581) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.03581/)",NA
"2045",2044,84,"Lots of neat examples but I always wonder when I see these sorts of things: how well does it work on images of men with beards?",NA
"2046",2045,84,"FaceApp: I will take your whole stock.",NA
"2047",2046,85,"For traditional 3D computer vision, take a look at “Multiview geometry in computer vision” by Hartley and Zisserman. It is an excellent textbook. As for lectures, a quick google brought up this lecture series, but I haven’t personally gone through them https://youtu.be/RDkwklFGMfo

As for the deep learning side of things, I tend to read papers to get up to speed on a topic. I don’t know of any lectures covering 3D vision specifically. If you haven’t had any luck searching on your own, your best bet may be searching for specific topics rather than a full course.",NA
"2048",2047,86,"Title:A Survey on The Expressive Power of Graph Neural Networks  

Authors:[Ryoma Sato](https://arxiv.org/search/cs?searchtype=author&query=Sato%2C+R)  

> Abstract: Graph neural networks (GNNs) are effective machine learning models for various graph learning problems. Despite their empirical successes, the theoretical limitations of GNNs have been revealed recently. Consequently, many GNN models have been proposed to overcome these limitations. In this survey, we provide a comprehensive overview of the expressive power of GNNs and provably powerful variants of GNNs.  

[PDF Link](https://arxiv.org/pdf/2003.04078) | [Landing Page](https://arxiv.org/abs/2003.04078) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.04078/)",NA
"2049",2048,87,"There are various ways to handle this, for a short review I recommend taking a look at the r2d2 paper from deepmind where they focus explicitly on handling recurrent state in the experience replay setting.",NA
"2050",2049,87,"I'll check it out thanks. Their MuZero paper was a great read",NA
"2051",2050,87,"From that paper it seems like it IS better to store the recurrent hidden state than init from zero in distributed training. 

I'm looking for if anyone has tried training a memory augmented RNN (where hidden state is thousands of vectors vs. one) with RL or something similar, to see if that works without experience replay, or how they handled it, if you know of anything.",NA
"2052",2051,87,"If you squint at BERT it could be seen as that.",NA
"2053",2052,88,"This is a very interesting question and as someone who works in disentanglement and teaches it to people I find it fascinating. 
And perhaps also shows the limitations of our current interpretation of what disentanglement means. (the definition of disentanglement is somewhat obscure and loose.)

My best way of explaining this is that disentanglement must explain some factors of variations. Here in your examples some factors of variations are in fact entangled with a variable that clearly defines a type of animal (or breed).

Most modern models can't handle this. In fact most models struggle in combinations of continuous and hard discrete factors and this is largely attributed to that (successful vae) models assume reconstruction objectives and these induce a smooth Latent space which can't handle structural breaks. On top of that our networks can't shear easily so the discrete factors must be dealt in different ways.

The paper of locatello points rightly so that in an unsupervised way disentanglement is impossible, and biases and annotations are necessary. 

So how would someone address this problem? 
First look into factor VAE. This model assumes that Latent spaces are conditional and hence can capture relationships. 

Second, one other view is to treat the space as true hidden Latents that can relate to true (observable) factors of variation even if entangled (dog breed, style, fur color etc) 

Lastly, from a pure data point of view you enter these dilemmas because data bias (nature) necessitates that certain combinations of factors are impossible. Hence data bias comes into play. If all possible combinations were allowed then the Latents can be fully disentangled. I like to call this the data bias bound on disentanglement.",NA
"2054",2053,88,"Thank you for your detailed answer! I will read into factor VAE and locatello. 

>The paper of locatello points rightly so that in an unsupervised way disentanglement is impossible, and biases and annotations are necessary.

Do you think that different latent space priors, such as the spike and slab, Gaussian processes or others, might be better at creating an interpretable representation of factors of variance that are not smooth and independent? Can these be used to represent the our biases and annotations?

>Second, one other view is to treat the space as true hidden Latents that can relate to true (observable) factors of variation even if entangled (dog breed, style, fur color etc)

I'm not sure I follow this approach. Can you please what do you mean by true hidden latents?",NA
"2055",2054,88,"

>Do you think that different latent space priors, such as the spike and slab, Gaussian processes or others, might be better at creating an interpretable representation of factors of variance that are not smooth and independent? Can these be used to represent the our biases and annotations?

To be honest I am not sure. I think they will help but still a recon objective biases to smoothness and the inherent limitations of neural networks as mapping functions. 

In addition, do not forget that let's say I have 10 true hidden values of a discrete variable. (eg mnist digits). Who says that I can't find two discrete variables, one that only takes 2 values and another one only 5 and thus can still get 10 distinct combinations? 

Thus prior knowledge of cardinality of the discrete space is necessary. 

>I'm not sure I follow this approach. Can you please what do you mean by true hidden latents?

I was too terse here in my response but I meant that you learn a spaze z that let's say is a multi variate gaussian and then you relate this z to known interpretable factors. This is like infogan and the recent stylegan2 based paper from yandex.",NA
"2056",2055,89,"You could use GPT-2 or Transformer-XL (something generative) to generate some text, and then fine-tune BERT in a similar task as NSP (next sentence prediction) but instead, you would optimize for paraphrasing.",NA
"2057",2056,89,"You can take any model that is able to do translation and use that. As opposed to translation, where you are translating from language X to language Y (e.g. English to German), in paraphrasing you are translating from language X to language X (e.g. English to English).

Like the other comment said, something based on transformers would probably be a good start.",NA
"2058",2057,90,"torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements

https://pytorch.org/docs/stable/optim.html",NA
"2059",2058,90,"[The same, but Keras](https://keras.io/callbacks/)",NA
"2060",2059,90,"Thanks a bunch! I completely forget about this 🤦",NA
"2061",2060,91,"Isn't this...kinda obvious?",NA
"2062",2061,92,"[deleted]",NA
"2063",2062,92,"Serious question: So what?

Someone please educate me why this is an important line of research. I feel like there is a disconnect between the people who really follow this stuff, and the non-quantum-enlightened ML peasants.

Is it about more efficient optimization of ML models?

Simulating quantum systems with a data-driven spin?

Some context given along with such posts would increase my appreciation by a tenfold. Otherwise it just looks like another corporate plug.",NA
"2064",2063,92,"So, I'll be honest, I still don't know what quantum data is or why I should care about it.",NA
"2065",2064,92,"Isn't quantum supremacy supposedly achieved by Google still highly contested? Surprised that it's taken as a fact in their README.",NA
"2066",2065,92,"Excuse the ignorant question. What is the application of this for ML?",NA
"2067",2066,92,"Quantum computing is most likely dead on arrival because no one has a good solution to quantum error correction. It's possible that there are no good solutions for error correction of quantum computers, in which case, quantum computing may be not feasible ever.",NA
"2068",2067,92,"looking forward to see how it will be applied",NA
"2069",2068,92,"So how does one make use of this framework? We need access to quantum computer right?
Is that doable via cloud computing?",NA
"2070",2069,92,"don't talk to me unless it's got VR too",NA
"2071",2070,92,"nah, it's distributed Byzantine fault tolerance.",NA
"2072",2071,92,"Can I deploy it on edge devices? I need this for my IoT pipeline.",NA
"2073",2072,92,"If you like it then you better put a blockchain on it. - pauli's principle",NA
"2074",2073,92,"Oh boy /r/ml is approaching /r/selfawarewolves right now.",NA
"2075",2074,92,"There are a lot of things that are very interesting about quantum computation theory (e.g., [polynomial time factorization](https://en.wikipedia.org/wiki/Shor%27s_algorithm)) and, more broadly, quantum information theory (e.g., super compression, quantum teleportation).

This library seems to be aimed at researchers that want to develop and understand quantum computing algorithms. While this library only simulates a quantum computer with a classical computer (making everything as slow or even slower than classical computing), the goal is to allow people to validate ideas or theories. Given how quantum computing can easily solve some classically hard problems (e.g., factorization), there is an open question regarding what else could be easy using quantum computers.

It's very possible that some forms of ML-like optimization are easy to solve using a quantum computing model. A library like this one allows people to investigate this more quickly since (presumably) a significant amount of work implementing the low-level details of simulating a quantum computer has already been done.

This is especially well suited to try out ideas that are not necessarily universally good/appropriate but well suited for real-world applications, e.g., an optimization algorithm with bad worst-case guarantees but empirically good performance similarly to something like the simplex algorithm.

To my knowledge, nothing seems to suggest that quantum computing is fundamentally impossible which means that practical quantum computers are a very real possibility once the many engineering obstacles are solved (which could be tomorrow or in hundreds of years, I don't know enough to about this aspect to even guess how long). Understanding what we can and cannot do with them is very important both for knowing what is practical (e.g., theoretically, you could build a quantum computer that is really slow or otherwise limited at doing the types of ops we want), and, what parts of the design should be prioritized (e.g., no point in exploring how to cram more of 'X' if what we need in our quantum computers is more 'Y's).",NA
"2076",2075,92,"Read the introduction of their paper: [https://arxiv.org/pdf/2003.02989.pdf](https://arxiv.org/pdf/2003.02989.pdf)   It gives a decent overview. You can find out a lot more by reading the references cited there.

I also recommend just trying out their tutorials.

ETA I'm just going to quote Scott Aaronson here ([https://www.scottaaronson.com/blog/?p=3848](https://www.scottaaronson.com/blog/?p=3848)):

&#x200B;

>**Optimization and machine learning.**  These are obviously huge application areas for industry, defense, and pretty much anything else.  The main issue is just that we [don’t](https://www.scottaaronson.com/papers/qml.pdf) [know](https://www.csm.ornl.gov/workshops/ascrqcs2015/documents/jordan.pdf) how to get as large a speedup from a quantum computer as we’d like for these applications.  A quantum computer, we think, will often be able to solve optimization and machine learning problems in something like the *square root* of the number of steps that would be needed classically, using variants of what’s called [Grover’s algorithm](https://en.wikipedia.org/wiki/Grover%27s_algorithm).  So, that’s significant, but it’s not the *exponential* speedup and complete game-changer that we’d have for quantum simulation or for breaking public-key cryptography.  Most likely, a quantum computer will be able to achieve exponential speedups for these sorts of problems only in special cases, and no one knows yet how important those special cases will be in practice.  This is a still-developing research area—there might be further theoretical breakthroughs (in inventing new quantum algorithms, analyzing old algorithms, matching the performance of the quantum algorithms by classical algorithms, etc.), but it’s also possible that we won’t *really* understand the potential of quantum computers for these sorts of problems until we have the actual devices and can test them out.",NA
"2077",2076,92,"I don't understand much in this but I got excited because I vaguely know that Quantum operators are unique and not necessarily intuitive so to be able to create a learning algorithm that is based on Quantum computations could result in some really interesting stuff. Like there are all these concepts on the internet that with just a few Quantum logic gates you can break rsa encryption or grand stuff like that so just imagine what you could potentially achieve if you made a learning algorithm that uses quantum operations. This could be complete nonesense, if you're someone who understands the field better call me out if that's the case. Just sharing my pov.",NA
"2078",2077,92,"Read this it's pretty great: [https://www.scottaaronson.com/blog/?p=3848](https://www.scottaaronson.com/blog/?p=3848)",NA
"2079",2078,92,"4 things",NA
"2080",2079,92,"Google solved a very contrived problem (outputs of random quantum circuits) with a quantum computer astronomically faster (seconds) than their classical algorithm could do on classical supercomputers (hundreds of years). IBM responded by saying they had a better classical algorithm, which would run in days instead on a supercomputer. If Google added just one more qubit, it would be classically intractable, even by IBMs algorithm. The question at this point is not whether quantum supremacy has been achieved, but rather whether or not the quantum computer is useful. It so noisy, it gets the answer wrong 999 times out of a thousand for many quantum algorithms. However, some hybrid models and algorithms, which are what this project aims at, could be successful at solving certain problems.

ETA: read Scott Aaronson's blog post for a great take: [https://www.scottaaronson.com/blog/?p=4317](https://www.scottaaronson.com/blog/?p=4317)",NA
"2081",2080,92,"> quantum supremacy

For those who don't know, this term means less than it sounds like.  It means ""we proved that this is actually a quantum computer, by solving a problem a classical computer can't solve"".  It does not mean ""quantum computers are supreme"".

The reason it's relevant is because a lot of us computer scientists are sitting around saying ""how do I know you actually built that thing, rather than just offloading the compute to a big datacenter somewhere?"".  So ""quantum supremacy"" means ""yes, I actually built it"", not ""it's so much better"".

Now, one would expect quantum computers to be ""so much better"" at some set of tasks, someday.  But the ""I actually built it"" step is probably a decade or more before the ""it's so much better"" step.  It's necessary but not sufficient.",NA
"2082",2081,92,"This is one area where the rigorous definition of quantum supremacy and those quibbling about it are more misleading than the original headline

Its more about wanting to be the one to achieve some specific goal than anything about the state of the industry.

The original argument was that storing the whole 2^n state vector would *only* take 9 petabytes and there's one supercomputer with 40 so it could theoretically be programmed to do the task in a few days. The Quantum computer did it in 400 clock cycles.

56 has already been achieved which is beyond any supercomputer, if you get to 70 then you'd be at the total storage of the whole world.

the only way the counterarguments would be meaningful would be if storage grew faster. But if # of qubits grows exponentially like most other computational technologies, you'd need storage to grow double exponentially (which it doesn't). Anything above linear growth of cubits would outpace current storage growth.",NA
"2083",2082,92,"[https://github.com/tensorflow/quantum/blob/master/docs/tutorials/mnist.ipynb](https://github.com/tensorflow/quantum/blob/master/docs/tutorials/mnist.ipynb)  


You may find this as a good starting point. mnist using Quantum Neural Network.",NA
"2084",2083,92,"Topological quantum computing is the way. Or one-way quantum computing using photons",NA
"2085",2084,92,"I think it is only a simulation of how it would work on a quantum computer, so that you can run the code without your own quantum computer.",NA
"2086",2085,92,"It's ~~AR~~ ~~MR~~ XR now",NA
"2087",2086,92,"I want it to be streamed live",NA
"2088",2087,92,"Thank you very much for your answer.",NA
"2089",2088,92,"Fantastic answer.",NA
"2090",2089,92,"Basic question: how do qubits relate to the wave functions of QM?",NA
"2091",2090,92,">(which could be tomorrow or in hundreds of years, I don't know enough to about this aspect to even guess how long)

in which universe could all the engineering challenges be solved by ""tomorrow""? Even if you know next to nothing, it would be obvious that it needs several decades at minimum.",NA
"2092",2091,92,"""The Talk""

http://imgur.com/t/The_More_You_Know/bPslN",NA
"2093",2092,92,"Thanks a lot, gonna check this right now. Prior to my question I didn't find anything satisfying regarding this subject",NA
"2094",2093,92,"That's not entirely true. 

Google used a QC emulator as their test comparison rather than a native classical computer program. If they had done so then their test would have failed. 

Their rebuttal was just changing the goalposts.",NA
"2095",2094,92,"[deleted]",NA
"2096",2095,92,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/tensorflow/quantum/blob/master/docs/tutorials/mnist.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/tensorflow/quantum/master?filepath=docs%2Ftutorials%2Fmnist.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",NA
"2097",2096,92,"XARGmented reality?",NA
"2098",2097,92,"From a SpaceX mission to Mars",NA
"2099",2098,92,"I wish I knew the physics part better but unfortunately my understanding of quantum mechanics is superficial at best. I've based my previous answer off of an advanced class in quantum information theory I've taken and a few talks I've attended on quantum computation.

I'll regurgitate what I remember and hopefully it will be clear enough for you to piece everything together. But, really, my objective is to provide enough intuition that the wikipedia article makes some amount of sense and to make few enough mistakes that the people that actual know this well don't hunt me down and burn down my house.

In those lectures/talks, I've only seen a fairly abstract definition of a [quantum state](https://en.wikipedia.org/wiki/Quantum_state), e.g., qubit. Once accustomed to the [bra-ket](https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation) notation, it's mostly just linear algebra:

* You represent a pure quantum state as a linear combination (with possibly complex coefficients) of eigenvectors (or as they call them, eigenstates) which very informally represent the different things you can observe (e.g., momentum and position).
* You can think of a (pure) quantum state as a ""normalized"" vector whose entries are the coefficients.
* An observable defines a set of eigenvectors each representing different observation (e.g., spin up, spin down).
* The probability of making a certain observation (which can be thought of as a matrix) of some observable is the norm of the vector (i.e., quantum state) projected on that matrix, e.g., think something like x^T A x where A is a matrix and x is a vector.
* After being measured, the quantum state changes and can be represented by the projection of the vector on the matrix representing the observation, and the resulting new quantum state is an eigenvector of that matrix (many assumptions have been omitted from this reply).

This last point is why the concept of eigenvector is so fundamentally important. Since the new quantum state is an eigenvectors of that observation, measuring the same observable will not change the quantum state (e.g., if I look twice at the same thing in quick succession, nothing just magically changes).

If the quantum state cannot be an eigenvector of every observable (e.g., position and momentum), then it is impossible for me to observe one then the other without the second measurement also changing the quantum state. This effectively models the uncertainty principal and my guess is that this is probably also related to the QM wave function in some way.

For a better explanation, you probably can just check back a bit later once [Cunningham's Law](https://en.wikipedia.org/wiki/Ward_Cunningham#Cunningham's_Law) kicks in... ([relevant kxcd](https://xkcd.com/386/))",NA
"2100",2099,92,"A qubit is a two dimensional hilbert space. A basis for this space are solutions to the Schrödinger's equation, and because this equation is linear, then the linear combinations and by extension the elements of the hilbert space of are also solutions.",NA
"2101",2100,92,"The qubit is the physical system (for example a trapped atom with only two relevant internal energy levels) and the wave function of the qubit describes the state of the system.

The wave-function of a single qubit is a normalized vector in C\^2 - i.e. |psi> = (c0, c1)\^T with |c0|\^2 + |c1|\^2 = 1 .  Specifically the |0> state of the qubit is (1,0) (e.g. atom is in the ground state, lower energy level) and the |1> state of the qubit is (0,1) (e.g. atom is in the excited state, higher energy level).    


A possible measurement of the qubit is described by an hermitian 2X2 matrix O where the average result of the measurement (when repeated many times with the same wave-function) is given by <psi | O | psi> = (c0\^\*,c1\^\*) O (c0,c1)\^T. For example considering O = \[\[1,0\],\[0,-1\]\] you will get <O> = |c0|\^2 - |c1|\^2 .

The wave-function of n-qubits is a similar object but it is now living in the tensor product Hilbert space C\^2 X C\^2 ... X C\^2 which is of dimension 2\^n , hence the wave-function is now specified by 2\^n complex coefficients c\_{i\_1,..,i\_n} with i\_1,..,i\_n \\in {0,1}. For example the wave-function of two qubits will be described by |psi> = (c00,c01,c10,c11) and possible two-qubit measurements are described by 4X4 hermitian matrices.

p.s : For a nice introduction to quantum mechanics which assumes only familiarity with linear algebra (I think) take a look at [chapter 2](http://www.theory.caltech.edu/people/preskill/ph229/notes/chap2.pdf) of John Preskill's lecture notes on quantum computation.",NA
"2102",2101,92,"I honestly wouldn't be able to tell you what the relevant research groups are working on, and, if I could, I wouldn't be able gander a guess as whether it could lead to a breakthrough or not. From my perspective, reading about a major breakthrough tomorrow doesn't sound flat out impossible.

If you tell me that it is, then I'm content taking your word for it, but I don't know if I would consider it such a deeply entrenched common knowledge that ""even if you know next to nothing, it would be obvious that it needs several decades at minimum.""

Though, my catty tone aside, I am genuinely curious about what aspects are likely to hold us back for next several decades and I'd love to have better understanding as to why it would be difficult to make progress. I'd love it if you could give me a bit of insight or give me some pointers!",NA
"2103",2102,92,"The magazines in the last few frames are hilarious.

>Guy with website has invented gigaqubit processor.  
>  
>Will the next iPhone be... Quantum?  
>  
>Quantum computing and consciousness are both weird and therefore equivalent.",NA
"2104",2103,92,"holy shit that was amazing",NA
"2105",2104,92,"I'll just say that I'm not 100% convinced that all NISQ devices are purely classical and provide zero speedup. I just haven't seen a mathematical proof.  Of course, the more the noise, the more the entanglement with the environment, and the more classical the system is. However, the claim that less noisy quantum devices can simulate themselves better than classical computers shouldn't be very shocking. We're far from reaching fundamental noise limits, so whether this is achieved now or later is a moot point to me. What matters more is reaching the level of noise when we can make useful quantum simulations or run error correction codes. That's not that far away.",NA
"2106",2105,92,"So the wave functions are elements in the same 2D Hilbert space as qubits? Then would they be the *measured* wave functions (Dirac delta) or the *real* theoretical wave functions?",NA
"2107",2106,92,"I would put it like this: each qubit has a wave function in this Hilbert space. You could concatenate all the wave functions of all the qubits together to get one big vector-valued joint wave function. This is really the wave function of interest for simulating an n-body quantum system. 

A quantum computer couples together all the qubits through Hermitian matrices—a quantum logic gate is generally coupling/entangling two qubits at a time, so O(n^2 ) logic gates would then be required to create an arbitrary quantum system with a dense Hermitian matrix. You would then run your initial conditions vector through a sequence of matrices, and at the end you measure an ensemble of runs to get a probability distribution, which is (in expectation) the amplitude squared of the joint wave-function of all qubits, just like in quantum mechanics.

Edit: I should have said Unitary, not Hermitian. And I the O(n^2 ) number does not appear to hold up—but I think the explanation is still qualitatively correct.",NA
"2108",2107,92,">so O(n\^2) logic gates would then be required to create an arbitrary quantum system with a dense Hermitian matrix.

This is incorrect as an arbitrary Hermitian operator acting on an n-qubit system is a 2\^n X 2\^n Hermitian matrixIf this would have been true  it would mean that we could encode all quantum states in the Hilbert space with a polynomial number of coefficients which cannot be true for an Hilbert space of size 2\^n .   

 (Edit: by the way a quantum circuit is typically implementing a unitary transformation on the initial state, so it would be described by a unitary matrix, at least when ignoring effects of coupling to the environment). 
>just like in quantum mechanics.

It is not ""like"" in quantum mechanics, it **is** quantum mechanics :)",NA
"2109",2108,92,"I think that is correct for a classical computer, but shouldn’t it be like n^2 to implement in quantum hardware? I’m thinking that the quantum logic gates are encoding coefficients of a continuous-time a state transition matrix, which is just quadratic in the number of qubits.",NA
"2110",2109,92,"> It is not “like” in QM, it **is** QM

Pretty sure he’s using “like” to mean “as in”. “Like” isn’t exclusively used for similes.",NA
"2111",2110,92,"If you are considering only two-qubit gates then O(n^2) gates would be described by O(n^2) coefficients while an arbitrary unitary transformation of the initial state would be described by a 2^n x 2^n unitary matrix with an exponential number of coefficients . Note that the state of the quantim system at each time is described by 2^n coefficients and not O(n) coefficients.",NA
"2112",2111,92,"ok, I'm not a native English speaker so I guess that this interpretation of the sentence escaped me. Anyway it is not the important point of my comment.",NA
"2113",2112,92,"Ok but does that mean you’re going to need 2^n x 2^n 2-qubit gates to solve an arbitrary problem? If that were the case, the complexity of just fabricating a quantum chip would be non-polynomial, which would kind of defeat the purpose, right? Or is the idea that only a polynomial number of 2-qubit gates are required for problems of interest?",NA
"2114",2113,93,"* I believe you meant encoder and decoder. No they need not be symmetrical.

* Depends on the dataset. If you are implementing a paper, you could just use the model architecture they used. In decoder you could use upsampling or fractional/atrous convolution etc 

* Usually the number of filters increases as you go deeper into an encoder. At the same time width and height decreases. This is reversed for a decoder.",NA
"2115",2114,93,"Thanks, yes I meant decoder, I edited the post. 
For the second point, can I use a Conv2DTranspose with strides? What's the difference between that and those you mentioned?",NA
"2116",2115,93,"Here is a great article on using deconvolutions (i.e. transposed convolutions) vs resize-convolutions.

https://distill.pub/2016/deconv-checkerboard/",NA
"2117",2116,93,"Of course you can. There are some architectural differences between these convolutions. You can search them online.",NA
"2118",2117,94,"It's an interesting business case, but I wish they did a much better job of explaining exactly how ML is used.  What is trained on what?  Radar is lower resolution than optical imaging; are they filling in missing pieces by training with radar inputs and photo outputs, so they can fill in photos and remove clouds?  Or is is really just about compute power, and not really ML per-se (i.e. what the GPUs were originally for--image processing!)",NA
"2119",2118,95,"Title:A Generalized Training Approach for Multiagent Learning  

Authors:[Paul Muller](https://arxiv.org/search/cs?searchtype=author&query=Muller%2C+P), [Shayegan Omidshafiei](https://arxiv.org/search/cs?searchtype=author&query=Omidshafiei%2C+S), [Mark Rowland](https://arxiv.org/search/cs?searchtype=author&query=Rowland%2C+M), [Karl Tuyls](https://arxiv.org/search/cs?searchtype=author&query=Tuyls%2C+K), [Julien Perolat](https://arxiv.org/search/cs?searchtype=author&query=Perolat%2C+J), [Siqi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+S), [Daniel Hennes](https://arxiv.org/search/cs?searchtype=author&query=Hennes%2C+D), [Luke Marris](https://arxiv.org/search/cs?searchtype=author&query=Marris%2C+L), [Marc Lanctot](https://arxiv.org/search/cs?searchtype=author&query=Lanctot%2C+M), [Edward Hughes](https://arxiv.org/search/cs?searchtype=author&query=Hughes%2C+E), [Zhe Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z), [Guy Lever](https://arxiv.org/search/cs?searchtype=author&query=Lever%2C+G), [Nicolas Heess](https://arxiv.org/search/cs?searchtype=author&query=Heess%2C+N), [Thore Graepel](https://arxiv.org/search/cs?searchtype=author&query=Graepel%2C+T), [Remi Munos](https://arxiv.org/search/cs?searchtype=author&query=Munos%2C+R)  

> Abstract: This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime wherein Nash equilibria are tractably computable. In moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, $\alpha$-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and applies readily to general-sum, many-player settings. We establish convergence guarantees in several games classes, and identify links between Nash equilibria and $\alpha$-Rank. We demonstrate the competitive performance of $\alpha$-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player Kuhn and Leduc Poker. We then go beyond the reach of prior PSRO applications by considering 3- to 5-player poker games, yielding instances where $\alpha$-Rank achieves faster convergence than approximate Nash solvers, thus establishing it as a favorable general games solver. We also carry out an initial empirical validation in MuJoCo soccer, illustrating the feasibility of the proposed approach in another complex domain.  

[PDF Link](https://arxiv.org/pdf/1909.12823) | [Landing Page](https://arxiv.org/abs/1909.12823) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/1909.12823/)",NA
"2120",2119,96,"Wow this actually seems really powerful. I've been waiting for algebraic topology to step into its own with respect to deep learning, and this seems like it.",NA
"2121",2120,96,"Let me see if I got the gist:

- you propose complexity as a generalisation measurement: less compexity -> less specialisation -> less ""overfitting"" -> greater generalisation

- persistence is a measurement of how often a strong connection forms during training, hinting that it may have general importance over the training corpus, meaning that drop out should be performed on other connections to maintain these important ones

- you want to apply the same topological analysis of complexity and persistence to graph representations of narratives to identify sudden drops in complexity that would indicate plotholes.

Did I get it right so far?",NA
"2122",2121,96,"Interesting paper... this seems promising, I will go through in details and post my findings in regards to recommendation engine",NA
"2123",2122,96,"Thanks for the kind words!",NA
"2124",2123,96,"Complexity indicates an entry point for where an AI should begin looking for plot holes. 


Essentially, plot hole detection is outlined in more detail in the other entries but complexity acts as an importance distribution of where the AI needs to sample. Once it enters at a point in the story it’ll attempt to perform multihop inference up to k steps away to see if the point it selected is actually a plot hole.

This is the subject of my (soon to be) PhD thesis, I’ve barely made a dent here. Check back in five or so years :p


Edit:
No, drop out is performed on the crucial ones so that the network learns to recover from these massive errors.",NA
"2125",2124,96,"Note: I replied seconds after waking up so I’ll edit my response if it turns out I was wrong due to being sleepy.",NA
"2126",2125,96,"I don’t think you can get it to scale to that size in practice. I’m struggling to get it to scale to ResNet sizes... i presented mostly a theoretical result. The authors only apply it to MLPs, although we will begin work soon to apply it to LSTMs.",NA
"2127",2126,96,"Indeed it will take some good amount of time. For now, just get to understand the math behind it",NA
"2128",2127,98,"Title:The importance of transparency and reproducibility in artificial intelligence research  

Authors:[Benjamin Haibe- Kains](https://arxiv.org/search/stat?searchtype=author&query=Haibe- Kains%2C+B), [George Alexandru Adam](https://arxiv.org/search/stat?searchtype=author&query=Adam%2C+G+A), [Ahmed Hosny](https://arxiv.org/search/stat?searchtype=author&query=Hosny%2C+A), [Farnoosh Khodakarami](https://arxiv.org/search/stat?searchtype=author&query=Khodakarami%2C+F), [MAQC Society Board](https://arxiv.org/search/stat?searchtype=author&query=Board%2C+M+S), [Levi Waldron](https://arxiv.org/search/stat?searchtype=author&query=Waldron%2C+L), [Bo Wang](https://arxiv.org/search/stat?searchtype=author&query=Wang%2C+B), [Chris McIntosh](https://arxiv.org/search/stat?searchtype=author&query=McIntosh%2C+C), [Anshul Kundaje](https://arxiv.org/search/stat?searchtype=author&query=Kundaje%2C+A), [Casey S. Greene](https://arxiv.org/search/stat?searchtype=author&query=Greene%2C+C+S), [Michael M. Hoffman](https://arxiv.org/search/stat?searchtype=author&query=Hoffman%2C+M+M), [Jeffrey T. Leek](https://arxiv.org/search/stat?searchtype=author&query=Leek%2C+J+T), [Wolfgang Huber](https://arxiv.org/search/stat?searchtype=author&query=Huber%2C+W), [Alvis Brazma](https://arxiv.org/search/stat?searchtype=author&query=Brazma%2C+A), [Joelle Pineau](https://arxiv.org/search/stat?searchtype=author&query=Pineau%2C+J), [Robert Tibshirani](https://arxiv.org/search/stat?searchtype=author&query=Tibshirani%2C+R), [Trevor Hastie](https://arxiv.org/search/stat?searchtype=author&query=Hastie%2C+T), [John P.A. Ioannidis](https://arxiv.org/search/stat?searchtype=author&query=Ioannidis%2C+J+P), [John Quackenbush](https://arxiv.org/search/stat?searchtype=author&query=Quackenbush%2C+J), [Hugo J.W.L. Aerts](https://arxiv.org/search/stat?searchtype=author&query=Aerts%2C+H+J)  

> Abstract: In their study, McKinney et al. showed the high potential of artificial intelligence for breast cancer screening. However, the lack of detailed methods and computer code undermines its scientific value. We identify obstacles hindering transparent and reproducible AI research as faced by McKinney et al and provide solutions with implications for the broader field.  

[PDF Link](https://arxiv.org/pdf/2003.00898) | [Landing Page](https://arxiv.org/abs/2003.00898) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2003.00898/)",NA
"2129",2128,98,"Vanity link Is not working",NA
"2130",2129,98,"We didn't upload LaTeX source so Vanity can't do its thing.",NA
"2131",2130,99,"I did my PhD using matlab. So.... No.",NA
"2132",2131,99,"It's entirely what you make of it. You're asking if going to the park will make you better at basketball... Sort of depends what you do and make of your time at the park.",NA
"2133",2132,99,"[deleted]",NA
"2134",2133,99,"If it does it's a side effect. A PhD is about doing research.",NA
"2135",2134,99,"I'm working with a guy who has a PhD. He is a terrible programmer.",NA
"2136",2135,99,"A PhD will definitely improve your coding skills. Software engineering skills? Probably not, unless you work with people who have good practice.

There are quite a few people in my lab that came to get their master's or PhD's after being a SE in industry for a while. They always scream when they see my code... I'm not a PhD but...yeah..... :(",NA
"2137",2136,99,"I'll probably get downvoted into oblivion for this, but as a software engineer working the past six years in ML, optimization, and simulation, I work with quite a few PhD contributors. Saying something looks like ""PhD code"" is not a compliment. It's often hacked together just enough to illustrate a concept. There are exceptions, but PhD folks are people, too, and not all of them want to be craftsmen (or craftswomen) when it comes to code. In fact, very few of them do.

So, no, in my limited experience, people with PhDs have not had their software engineering skills improved by their university training. But they probably did learn to code or code better.",NA
"2138",2137,99,"Yes, a lot. My Prof came from industry, before her tenure, she worked at Google. Thus, she was pretty experienced and had a different view than someone who never went outside universities. However, she approached most problems very differently from how I used to. Prior to the PhD I had quite a bit of work experience but I was caught in what I would now consider a trap of framework upon framework and ""enterprise best practices"". 

During my time we always built fully functional applications to showcase our resarch and we usually did so from scratch so that we would know about all possible sideeffects. I know that many things we did in terms of software engineering on top of the actual research were probably a very backwards and not best practice (the worst example probably being homegrown webservers from socket code in C and XML/JSON response over HTTP building the strings by hand -- not everything was that bad, of course).

In the long run, I think, this helped me tremendously. Back in industry, it was time to adapt again and, of course, it takes a bit to get used to everything. However, knowing both sides and especially knowing a lot more about fundamentals feels very valueable to me.",NA
"2139",2138,99,"You don't enter a PhD program to be a better engineer, you enter to be a better scientist.

Coding is something you get undergrads to do for you =p",NA
"2140",2139,99,"When I completed my PhD (and postdoc), I was convinced I learned a lot about software engineering, and others told me that I wrote nice code. Then I started a role in industry and learned that I knew practically nothing of modern software engineering practices, good testing, design patterns, continuous delivery, agile collaboration, planning / estimating, and all the rest that you will have to deal with as a software engineer (what's a product manager again?).

Long story short - whatever coding you do during your PhD has probably very little in common with ""proper"" software engineering practices, but you'll be able to pick it up quickly. Software engineering involves a lot of stuff, and solving an actual problem with code is just a very small part of it. If you can get that through an internship will depend on the internship - most companies won't let you get close to production code.",NA
"2141",2140,99,"A PhD is of course a doctor of philosophy which values much more than the tools of the field. Programming is a tool that researchers can use should they choose, but isn't strictly necessary. I wouldn't expect all construction workers to be experts with a hammer; many only use the hammer once every few weeks.",NA
"2142",2141,99,"I coded quite a lot during my PhD which in turn increased my overall SW engineering skills too. Though, im obviously not as skilled now as I would have been when pursuing an industry position right after my masters.",NA
"2143",2142,99,"Doing PhD. My fellow research scholars consider me programming wizard. All I do is Google the errors.",NA
"2144",2143,99,"So arguing a bit against the mainstream, most PhD programs have fairly limited access to computational resources and so you learn to be much more judicious with experiments and testing when you don't have 1000 EC2 instances you can just fire up. You also usually get quite good at making algorithms CPU/memory efficient so you can iterate quicker. You won't have the same mentorship and rigorously enforced software engineering culture you would at a company, but you can usually find a few allies. You are also (normally) completely free to work with and contribute to open-source which often isn't the case outside of academia and can massively improve your skills. Numpy, Pandas, Spark, Tensorflow, Dask all have excellent coding practices and ecosystems that rival many tech companies.",NA
"2145",2144,99,"Yes.

But in more of how to plan and execute projects. I already knew Python  even before my phd.

You can always ask  or read if there's something you don't know, leading your own project is the next level shit.",NA
"2146",2145,99,"[deleted]",NA
"2147",2146,99,"Ooff",NA
"2148",2147,99,"No worries, now that you’ve got the PhD they’ll let you use PowerPoint much more often.",NA
"2149",2148,99,"I agree, coding quality is generally not encouraged as much as papers, but it can depend on your field as well.  If people are doing ML in an applied field and cross discipline (e.g. engineering, physics, biology), and their works winds up as tools/platforms for others, then they usually have decent programming skills.",NA
"2150",2149,99,"I knew a few who mostly worked with excel during their PhD",NA
"2151",2150,99,"I'm working with a few guys without PhDs. The quality of their code is universally atrocious.

I don't think the ability to write good code correlates with having or not having a phd.",NA
"2152",2151,99,">software engineer working the past six years in ML, optimization, and simulation

I'm an undergrad and I'm extremely interested in these topics. Do you mind if I pm you a few questions?",NA
"2153",2152,99,"I discovered the ways of Python. So all is good. Except for the poor bastard doing my code review. But thank you for your kind words.",NA
"2154",2153,99,"That's if you're lucky. Most of my time now is Powerpoint, Excel and Outlook.",NA
"2155",2154,99,"I don't know, I would probably still be writing shitty code if I hadn't worked with a real project where quality code was required. Obviously it's not a rule, but I talked to a friend about it and he said all PhD persons he had worked with wrote shitty code.",NA
"2156",2155,99,"Please do.",NA
"2157",2156,99,"Imho, it's more about the willingness to put the effort to write quality code and having the right environment to learn good practices. This is not something that people are born with, you have to put the effort into it.

In a way, this talks more about the company rather than the people. Your colleagues don't write code in a vacuum, they write it in collaboration with other people. If you don't push for quality, the results will always be very underwhelming.

p.s. For example, at the company where I work now, there are veteran coders who write universally incomprehensible and undocumented code... even in high-level languages. The new coders they hire are also equally awful. The catch is that no one at the company ever bothered to establish and enforce good coding practices, so you get what you get.",NA
"2158",2157,0,"I'm interested to read the papers, but they're like place 35 on my list currently. Urgh, gotta read them all.",NA
"2159",2158,0,"haha. Been there.",NA
"2160",2159,1,"Is this like, using some kind of biopsy or cheek swabs? Interfacing a discriminatory network with a digital microscope sound interesting to be sure",NA
"2161",2160,1,"It's using CT scans of the lungs. So much more expensive/interventionist than a cheek swab but interesting nonetheless.

Anecdotally, I've heard that x-rays are almost as useful for human diagnosis (edit: of COVID19 specifically) as CT scans, and unlike a CT, you can wheel a machine into a patient's room and take a picture without contaminating too much; however, if you bring a COVID patient into a CT, you have to wipe down the entire room after. So, CT scans aren't very practical as a diagnostic measure.",NA
"2162",2161,1,"That depends on so many factors I genuinely cannot be bothered to consider them, but sometimes, that's true.
That said, covid 19 causes enough lung damage to be detected by a discriminatory network from a CT scan? Kinda spooky ngl",NA
"2163",2162,3,"I wrote an interactive article about how Linear Discriminant Analysis works, I hope you find this useful!

[https://omarshehata.github.io/lda-explorable/](https://omarshehata.github.io/lda-explorable/)

I was also hoping this would be a useful teaching tool - so you can drag and drop your own data in any of the interactive figures to create your own examples on the fly. There's examples of this here: 

[https://github.com/OmarShehata/lda-explorable#a-teachers-guide-to-a-geometric-intuition-for-linear-discriminant-analysis](https://github.com/OmarShehata/lda-explorable#a-teachers-guide-to-a-geometric-intuition-for-linear-discriminant-analysis)",NA
"2164",2163,3,"That looks awesome. Which tool did you use for those animations?",NA
"2165",2164,3,"OMG I was LITERALLY IN SEARCH FOR THIS! Thank you",NA
"2166",2165,3,"Excellent work and intuitive explanation!",NA
"2167",2166,3,"Woah this is interesting",NA
"2168",2167,3,"Nice. Any tutorial links?",NA
"2169",2168,3,"awesome",NA
"2170",2169,3,"Link is broken.",NA
"2171",2170,3,"So cool. Thank you.",NA
"2172",2171,3,"Is this related to PCA?",NA
"2173",2172,3,"Thank you! This is all just JavaScript on a page:

https://omarshehata.github.io/lda-explorable/

The 3D is threejs and the 2D is just canvas drawing with Twojs. It's all open source here:

https://github.com/OmarShehata/lda-explorable",NA
"2174",2173,3,"So many of the comments I've heard on this are ""wish I had this when I was learning it"" so I'm very glad you found it at the right time!!

I've been collecting like inaccuracies that need to be fixed or things that can be improved here: [https://github.com/OmarShehata/lda-explorable/issues](https://github.com/OmarShehata/lda-explorable/issues)

Feel free to add to that (or it's all open source, so creating your own thing off this/extending is also a great way to learn!)",NA
"2175",2174,3,"Slightly. PCA minimizes projection error and maximizes variance. LDA maximizes distance between classes and minimizes the variance within a class. They are both dimensionality reduction techniques.",NA
"2176",2175,3,"When would you use one vs the other?",NA
"2177",2176,3,"LDA and QDA are generally used for classification while PCA is used for preprocessing and exploratory analysis",NA
"2178",2177,8,"Fun project!  What does your data look like?

How well do you understand how genetic algorithms work?

You will likely want to organize your populations as routes consisting of “sub-routes” or “directions”.  A sub-route might just be encoded as “30th block of Maple St”, while a direction might be encoded as something like “from south bound Maple St, turn right at Oak St”.  That’ll depend on what your data looks like, I’m just offering examples to illustrate.

The inclusion of existing bus routes should help constrain your search space.  I suppose a key question might be whether the objective is to find new sequences of bus stops, or simply better paths from one stop to the next.  

Another factor in your population encoding is that there is an inherent ordinality to the combinations of individuals in a solution.  iow, the solution has to form a contiguous path.  That means that your mutation and recombination functions are going to have to take that constraint into account; you don’t want to waste generations testing for contiguous paths in your fitness function.  Depending on your encoding, this may or may not be trivial.

Your fitness function is pretty simple: time to destination.  Or if you are dealing with multiple routes, then it might be something like aggregate time to destination, and you may want to devise some metric that takes into account mean time to destination.

Hope that’s enough to get you started.",NA
"2179",2178,13,"I am up for it, though I don't have much advanced knowledge in deep learning.",NA
"2180",2179,13,"Hi! I'm in a very similar boat. Let's stay in contact. Do you use discord?",NA
"2181",2180,13,"Im up for it, but Im still working through Professor Ng's old coursera class because life got in the way.

Anyone want to try working together as a study group and then project group who is more at an early level?",NA
"2182",2181,13,"we get a post like this every month and a new discord gets created each time and then dies within a month, why not just use the LML discord?",NA
"2183",2182,13,"Firstly, Kudos in taking the initiative and marking considerable progress. I have immense respect for those who persevere towards their goals.

Seeking a team is a great idea. Helps stay motivated and creates a support system for learning. However, there are a few things to keep in my mind.

1. you want to be create a team that is either highly diverse in expertise (one specializes in unsupervised learning, someone in regression, someone in classification, etc.)
2. Make sure you are not the smartest or second smartest person in the group. This creates a learning opportunity for you and a mentoring opportunity for someone else.

Something that really worked for me is going about solving kaggle competitions post end date. This not only creates an opportunity to work on the problem but also to learn from the best solutions. Kaggle community is very helpful when you are stuck.

I am personally someone who likes to jump into the thick of things and then figure things out along the way. Group learning hasn’t worked for me but people like me are more than happy to advice/mentor based on our experiences. 

If there’s a problem you are stuck on and need help, feel free to message me.",NA
"2184",2183,13,"PM'ed you :)",NA
"2185",2184,13,"I've done a couple of courses in AI and currently doing as well. Do let me know if you guys are collaborating. Would love to be a part of it.",NA
"2186",2185,13,"i am having career in field. recently i want to have aboard friend so pm me if you're interested in.",NA
"2187",2186,13,"Hi, I'm not sure how much of a help I can be since I come from more of a business development background but I'm recently forayed into Machine Learning and more specifically, pre-processing, clustering and validating data sets. I've been pursuing a couple online courses and have been working in the analytics team in my present firm. Would love to get in touch! Cheers.",NA
"2188",2187,13,"I'm interested although I'm still not at the level where I'd do big AI/ML projects. If it's any use, I'm currently doing research internship at an AI startup where I focus on NLP and its application to econ and finance (I'm an econ major). The startup also has interests in computer vision so I'd probably dabble on it later on.",NA
"2189",2188,13,"I'm starting my journey into ML and DL because I have a project that I have to complete (both thesis and implementation) before the first week of July, it's not gonna be a big project, but I have to learn introductory stuff in computer vision and machine learning to implement a facial recognition system using scikit-learn.

So yeah, I'm interested, if it'd discord it would be great.",NA
"2190",2189,13,"Hey, I've just started with my ml course. Would be greatful for any form of guidance",NA
"2191",2190,13,"Hi! I am finishing my master soon (its called Web and Data Science), and starting a career in Machine learning and Data Science. I love the idea of meeting other people in a similar place, and i plan to start working hard on improving my skills starting 2 weeks from now.

Lets connect :)",NA
"2192",2191,13,"Your October will be my May - I’ll be a bit behind you but would definitely love a network to have conversations with and learn from!

Also from a technical background, worked through one introductory book and now going through another for implementation in sklearn and tensorflow. Once May hits I’ll be doing courses and seeing where it goes from there.",NA
"2193",2192,13,"I'm down to join up. I have experience with classification models and such. Not much experience with deep learning though. I recently finished my undergraduate, so I plenty of time to work on projects and learn more.",NA
"2194",2193,13,"I’m in !! How can we organize ?!",NA
"2195",2194,13,"I am in ! 
First year in clg cs major .",NA
"2196",2195,13,"Could be interesting. I've done some NLP at work for bug classification, I have a model running in a serverless setup. Now looking into using Deep Reinforcement Learning - though I'm still new to it and learning it on the side.",NA
"2197",2196,13,"Son of a bitch, I'm in",NA
"2198",2197,13,"Hi. I've started fast.ai and would definitely love to join the discord group/any group where fast.ai is the main topic which is being done by the peers.",NA
"2199",2198,13,"I’m a pretty seasoned software engineer with many tear experience on development, leadership and data management. 

Here’s my profile: linkedin.com/in/exemartinez

Let’s talk! I’ll love to be a part of the team!",NA
"2200",2199,13,"Count me in",NA
"2201",2200,13,"Hey! Can I jump in the discord! Id like to start learning as well",NA
"2202",2201,13,"Can I join too?! I’m very interested in learning more about ML and I’m a novice too",NA
"2203",2202,13,"Hi! I'm interested. I'm currently working on a reinforcement learning research project.",NA
"2204",2203,13,"Hi, I'm very interested, could I get the discord link?",NA
"2205",2204,13,"Im interested as well.",NA
"2206",2205,13,"I have started learning ML from quite a few days now. I would be highly grateful if I get a guide!",NA
"2207",2206,13,"This is a great idea, I will for sure try to form a team with people here",NA
"2208",2207,13,"I a very novice learner. Currently also learning Rust so i can right all ML algos from scratch especially DL ones.",NA
"2209",2208,13,"Im interested too.",NA
"2210",2209,13,"Hello there. I also interested in form a team I still in university and I am subscribed  in some ml/dl courses and will be interesting in joining you. I am interested in image classification and nlp",NA
"2211",2210,13,"I am definitely interested. I actually have an extremely similar background to you as well.",NA
"2212",2211,13,"I saw your post for a machine learning team and I have experience with clustering algorithms and cnns. Could I possibly form a team with some other people that messaged you in reference to your post.",NA
"2213",2212,13,"I've been working in ML for a couple of years and my major in systems engineering was ML and AI. Nevertheless I'm also still learning and would like to hop on the learning train. I might be able to give some advice too.",NA
"2214",2213,13,"I figure the internet is the only place to make this work.  It takes special kinds of people to have a job and stay motivated to do all the learning. I failed running my own co-learning community, kagglekings with the accompanying slack. Resource density is so high out there that, I surmise, many people decide to DIY their learning and give up at some point",NA
"2215",2214,13,"Also interested! I had some programming during my BS in bioengineering but it was only the focus in a couple classes. Been working a year and started some true CS classes (including ML) in my down time. I’m also about to get the last part for my deep learning computer just in time for corona shutdowns, so would love to join! Interests are rational protein engineering and synthetic biology, pls send the invite! Edit: oops just read your edit I’ll PM",NA
"2216",2215,13,"Just PM'ed you!",NA
"2217",2216,13,"Neither I suppose. I started learning recently. What are you doing right now to learn? Courses? Papers? Comps? What field are you digging deep into?",NA
"2218",2217,13,"Will send you a PM with a discord server",NA
"2219",2218,13,"Yeah, I'm down for it",NA
"2220",2219,13,"Yes, same early level studying",NA
"2221",2220,13,"That's the first thing on my list once this quarter is over. I'm just getting into machine learning.",NA
"2222",2221,13,"Lol because I didn't know it existed.",NA
"2223",2222,13,"What courses did you take? If you don't mind sharing",NA
"2224",2223,13,"I'm from a business background too. Actually preceded by physics though. But yeah definitely keen in meeting people like you too. I've taken on one computer vision job and want to do more of it and maybe build a consultancy team. PM me and I'll send you a discord chat.",NA
"2225",2224,13,"Hey. That sounds cool. If you're keen and energetic, feel free to PM me and I'll send you a discord server.",NA
"2226",2225,13,"https://discord.gg/qxzaS3",NA
"2227",2226,13,"Well I was going to only do PMs but screw it https://discord.gg/qxzaS3",NA
"2228",2227,13,"I have done some online courses..I have worked on few datasets and used traditional ml techniques mostly..I have started to dig into deep learning now..With so many resources around, really confused with the learning path. I did the first course of andrew ng deep learning specialization and intro to pytorch course from Udacity. I am also going through the deep learning in python book by francois.
Can you suggest a learning path for DL? I am eager to learn NLP and work on some good case studies related to it.",NA
"2229",2228,13,"What’s the Discord?",NA
"2230",2229,13,"Same!",NA
"2231",2230,13,"And... I didn't mean to start a discord server haha. I just wanted 2 or 3 like minded people t talk to",NA
"2232",2231,13,"You should check out fast.ai.  It's a nice course and accompanying library for practical ml programming.  I feel the exact same way you do about AGI (artificial general Intelligence) and just started a subreddit for that reason.  If you're interested in that too you should check it out and join the discussion.",NA
"2233",2232,13,"NLP is great. I do chatbots and stuff as a ML engineer. Hit me up if you have questions while learning",NA
"2234",2233,13,"Hey that all sounds awesome. Very interested in deep learning myself and haven't yet stared with NLP. I'll PM you a discord channel to join",NA
"2235",2234,13,"Thanks.. can you suggest me some good resources for nlp?",NA
"2236",2235,13,"Interested as well !",NA
"2237",2236,13,"NLP is so fun. That's what I do for work. Let me know of you have questions",NA
"2238",2237,13,"I have been playing with BERT and GPT-2, I would recommend checking those out and reading some papers on what has been done with them. If you need some paper suggestions, I have compiled a list for an English assignment I am working on and I could share some links with you.",NA
"2239",2238,13,"I've been just working on a project based on BERT and GPT-2. It's a medical chat bot with which we wanted to participate in a Facebook Hackathon. We haven't finish though and won't make it till the deadline in 1.5 days, but the project is still running if you would be interested.",NA
"2240",2239,13,"How far along are you? I don't have a ton of experience as I'm still in undergrad but I'd love to check it out",NA
"2241",2240,13,"Not far... I've built some model but we're stuck on the data engineering part. Looking rather for someone more experienced who would give the project a direction.",NA
"2242",2241,14,"This is a project I've been working on for a while now. All code is Python and uses OpenAI retro to interface with the emulator. Full code can be found here: [https://github.com/Chrispresso/SuperMarioBros-AI](https://github.com/Chrispresso/SuperMarioBros-AI)",NA
"2243",2242,15,"This is my understanding.

For example,

You are making a decision to go to a certain place B in least amount of time. Initially, you could possible take any road to go to that place. So, this problem can be thought of as a unconstrained problem as you are free to take up any road without any constraint.

Now, suppose you get to know that one of the roads is not working due to some sort of repair or construction. Now, your possible number of road choices has narrowed down and this problem can be thought of as constraint problem because there is one constraint.

Or Mathematically if you are familiar with calculus and functions,

Usually, An unconstrained problems involves a optimization where we have knowledge of the function and we then use calculus to optimize that function. On the other hand, constrained problem is where we have data not function and we need to approximate the function using that given data or constraints.

I want to maximize a function in x say f(x). I don't have any knowledge about the variable x (data). This is an unconstrained problem.

Now, take same function but this time we know that the possible values of x can be > 0, this is a constraint now and this problem of maximizing f(x) given x>0 is a constrained problem. You have narrowed down your search space.

This has confused a lot, but here is what i know. I hope it helps you.",NA
"2244",2243,16,"You've tried XGBoost?",NA
"2245",2244,16,"No, I haven’t. This is the first I’m hearing about but I did google it and browse up on it. What does it do?",NA
"2246",2245,17,"PCA *is* fully unsupervised. Any human readable labels are given manually after the PCA has been run.",NA
"2247",2246,17,"No.",NA
"2248",2247,18,"I've heard the same thing, and from what I understand, deep learning requires a different subset of feature engineering than other forms of machine learning. Unless your data is already encoded as a set of vectors or tensors, you're going to have to do some work to make it that way. This requires temporal extraction for dates/times, a LOT of work for text extraction, and similar tasks for other nonnumerical data. You also still need to do feature selection, choosing the features which you want to include for your model to train on.

However, you're correct that deep learning requires much less feature engineering than other models. It helps that many deep networks are designed for different data structures or even unstructured data, and a lot of neural architecture automates what used to be pure feature engineering. However, you're still going to have to do some engineering in any task, at least for the foreseeable future.",NA
"2249",2248,18,">I've heard the same thing, and from what I understand, deep learning requires a different subset of feature engineering than other forms of machine learning. Unless your data is already encoded as a set of vectors or tensors, you're going to have to do some work to make it that way. This requires temporal extraction for dates/times, a LOT of work for text extraction, and similar tasks for other nonnumerical data. You also still need to do feature selection, choosing the features which you want to include for your model to train on.

>However, you're correct that deep learning requires much less feature engineering than other models. It helps that many deep networks are designed for different data structures or even unstructured data, and a lot of neural architecture automates what used to be pure feature engineering. However, you're still going to have to do some engineering in any task, at least for the foreseeable future.

If text, such as data, were already encoded as a set of vectors or tensors, would it be easier for the computer to understand?",NA
"2250",2249,18,"Yes! This means if you have a standardized way of transforming data into vectors/tensors, you're pretty set and you really only need neural architecture.",NA
"2251",2250,18,">Yes! This means if you have a standardized way of transforming data into vectors/tensors, you're pretty set and you really only need neural architecture.

>GRAPED

What's that?",NA
"2252",2251,21,"No, normalizing data is not necessary if the scales of features are same. For example, if all of your features are lying between say -10 to 10, then there is no point in bringing these features again in range 0-1 by normalizing. 

It is necessary when one of the features lies between a range which is very different than the range of another feature. Say, one feature lies between 0-1 and another lies between 0-100000, then naturally in models like multivariate regression, the feature with high range will seem to be more important than the other which otherwise may not be the case.

So, usually, it is said that the features should have same scale otherwise our model may give us distorted results.",NA
"2253",2252,22,"shape = (32,6,17)",NA
"2254",2253,22,"Thank you I fixed it.

Honestly that's the same exact answer that I found online and  puzzled for the whole day, it's right that `(32,6,17)` is the size of a mini-batch, but not the size of the training set serving as an input to the model. When creating the first LSTM layer you specify input shape as `(sequence_length, features)` or in my case `(6,17)`. Laster you simply reshape your training set as 3D (Numpy array) where its size is `(total_samples, sequence_length, features)`, it's just an array containing many samples of size `(6,17)`. in my case I had total of more than 20k rows, I reshaped them as `(3438, 6, 17)` (I truncated few last rows so the total number of rows can be divided by 6, reshaping function doesn't work otherwise). The actual batch\_size is specified later when calling `fit()` function of the model.

    #first layer
    model.add(LSTM(units=50, activation='relu', input_shape=(6, 7), return_sequences=True))
    ...
    ...
    model.fit(input, output,batch_size=32, epochs=30, validation_split=.2)

where input is of shape  `(3438, 6, 17)` 

Output shape is `(3438, 6)`",NA
"2255",2254,22,"Good stuff :^)",NA
"2256",2255,23,"Any benefits of using MongoDB with Spark instead of just S3 with metastore?",NA
"2257",2256,24,"Your plot shows a uniform distribution of correct labels for a feature value of zero. It also shows that a label of zero is the most often correct label for the nonzero feature values. It looks like an impossible problem (you'll never get better than 30% standard error). What is your original data? How did you process it to create this plot? What are the other features?

Edit: uniform distribution",NA
"2258",2257,24,"If the axis called “label” is not a label but a feature it’s probably worth separating it into two features: one Boolean feature to capture whether what you call the label is zero or not, and another to measure the magnitude of it if it’s non-zero. The patterns in the data in these two regimes are quite different and making that explicit should help your model.

Even if the “label” axis is actually a label it may be best to train a model to predict whether the label is zero or non-zero, and then a separate mode to predict the non-zero label conditional on knowing its non-zero.

Irrespective of whether your data is a histogram or a scatter plot I think this separation would help capture the first order variation in the data.",NA
"2259",2258,24,"I'm not sure if you are trying to mask your real problem but this example is shit. Is it a continuous variable or a label? Is the x axis a count",NA
"2260",2259,24,"Clearly you have a huge lump where y variable is zero, an isyzero dummy might help. It really depends what you want to do.",NA
"2261",2260,24,"I agree (if I understand the data properly). I think it’s important to note that some problems are “impossible” (will just return the same as flipping a coin). When learning ML too often we expect similar results in real data sets to what we see in class and on “iris” like data. 

I think the important thing to note is, what is the aim? Maybe a perfect prediction model isn’t necessary. For financial predictions, anything better than random maybe enough.",NA
"2262",2261,24,"Thanks! That's right. The thing is I could filter those values (for instance keep only train and test values where the feature < 20). Do you think that would help? I could also filter some random zeros on the feature as well (the test set is ""kind of"" flexible for a particular reason that would take time to explain).",NA
"2263",2262,24,"Thank you, it is indeed the label. So I thought that regression may be easier than a very unbalanced binary problem, that's why I wanted to work with probabilities. Also, do you think filtering some of those zeros could help? Or maybe filling NANs in another way?",NA
"2264",2263,24,"It's looks likes binary variables to me , I would say use log transformation or logistic regression it might look proper after that",NA
"2265",2264,24,"Why so? The feature is a count while the label is continuous. BTW I'm sorry for the example, I uploaded many other images but they didn't appear, doing that again.",NA
"2266",2265,24,"It may be worth adding that this feature ALONE doesn’t predict the label well,  However, this feature combined with another feature may predict the label better than either feature alone.",NA
"2267",2266,24,"No. You absolutely do not want to stratify your data for the train\_test\_split. The purpose of train\_test\_split is to predict your accuracy in the future. You can only do that with a completely random sample of your dataset as your testset. Unless you share more information about your data (more features and their physical meaning) it's impossible to help you avoid all the pitfalls of feature engineering. It's the Anna Karenina problem of data science. There are an infinite number of ways to do it wrong, and usually only a few ways to do it right.",NA
"2268",2267,24,"Try scaling the data to a distribution, maybe fill the nans first with zeros and then with some more mighty trick (mean,std, percentile mean etc)",NA
"2269",2268,24,"Hi, thank you. In this case, the label is not binary. It is indeed values that range from zero to one. Why would you use log transformation, and you mean on the labels?",NA
"2270",2269,24,"I’m curious how you arrive at that conjecture when the values clearly span a continuous range? How can they be binary?",NA
"2271",2270,24,"Look at using a zero inflated model",NA
"2272",2271,24,"That’s true, but that should be quite easy to visualise in a 2d plot or dim. reduction with higher dimensions",NA
"2273",2272,24,"to make your all variables values to normal n if your values are already normal  then  you don't have to do nothing , I might be wrong but see if it works.",NA
"2274",2273,24,"This is an event predictions problem.

The features are a count of other events tha happened. Thus integer numbers. 

The labels are the inverse distance to the target event. For example, if an event happens in the next 7 days, the label is 1 - normalized(7). If the event never happens, the label is 0.",NA
"2275",2274,24,"Yes, exactly. Visualizing each feature against the label doesn’t tell the whole story. It is often necessary to look at covariances, vif, pca etc to get clues about which features should be plotted using multivariate techniques against the label.",NA
"2276",2275,27,"Why not just use legacy TF?",NA
"2277",2276,29,"Khan academy. Really ML uses tensors a lot and most of the tensor based books will assume several courses in linear algebra have been taken so your best path is to drill linear algebra basics.",NA
"2278",2277,29,"Senior Math major here looking to go into DS/Analytics. Linear Algebra is a bit of a pain, but once you’ve got reduction techniques and terminology down, the remainder focuses on space generation. As for proofs, once structure is declared, most actual work takes a little intuition and scratch work. PM me if you want.",NA
"2279",2278,30,"YouTube.",NA
"2280",2279,30,"I was hoping for something a little more specific",NA
"2281",2280,31,"And the Hourglass source code for this article is here:
https://github.com/ethanyanjiali/deep-vision/tree/master/Hourglass/tensorflow

There’re also implementations of many other networks I made before",NA
"2282",2281,34,"First of all, I learned a new word (commination), and then realized you made a typo :p.

Second of all, what a fun little project. Did you end up comparing your RL solution to depth first search, or just stuck with q learning? Also why the convolutional layers for finding the fruit, given the ""fruit"" is known and can just be a matched filter, for the snake to know where its own body is? Were the convolutions vastly over-parameterized, could you prune (maybe lead to faster training)?",NA
"2283",2282,34,"The aim was to not apply a Snake specific algorithm, hence a general convnet and q learning (or rather A2C eventually).

And yes I see the typo now. That's what I get for redditting at midnight.",NA
"2284",2283,37,"Follow the most popular ones that are being mentioned on this sub

**Andrew Ng’s ML course**

Or

**Udemy’s Machine Learning A-Z (100,000+ reviews)**

..... If you don’t like video lectures, there is a popular book called:

**“Hands-On Machine Learning with Scikit-Learn and TensorFlow”**",NA
"2285",2284,37,"You should also read 'Introduction to Statistical learning in R'. It's a really great book IMO. Also, a free pdf is available on their website.",NA
"2286",2285,37,"This is the post that I follow. And it seems to be the best post out here. 

https://www.reddit.com/r/learnmachinelearning/comments/cxrpjz/a_clear_roadmap_for_mldl",NA
"2287",2286,37,"If you are already technical (have done some programming), I'd recommend:

Learning from Data - By Yaser

ISLR - Introduction to statistical learning - by Tibshirani and others

I've also tried to gear a ton of my materials to people that can program and don't know data science: [https://www.youtube.com/data-talks](https://www.youtube.com/data-talks)

I've seen a ton of people just jumping into Sklearn and Tensorflow without understanding the basics in stats and often time they have to memorize and then end up forgetting small but crucial steps (not standardizing the test data with the training's mean and var). I really respect Andrew Ng, but I've seen students really get lost in the derivations and give up. And understanding how SVMs are derived is not too important if you want to be a data scientist)",NA
"2288",2287,37,"3B1B does an amazing but dense summary of neural nets and helped make the math more intuitive for me. https://youtu.be/aircAruvnKk",NA
"2289",2288,37,"I think that in addition to MOOCs and more pragmatic books, it's also valuable to get a solid foundational understanding of the topic. [Deep Learning by Goodfellow et al.](https://deeplearningbook.org) offers good insight into the how and why behind CNNs, GANs, SGD, etc.",NA
"2290",2289,37,"I complied down some of the resources in this repository and blog post. Check them out

 [https://github.com/ahmadmustafaanis/Machine-Learning-Path](https://github.com/ahmadmustafaanis/Machine-Learning-Path) 

 [https://towardsdatascience.com/beginners-learning-path-for-machine-learning-5a7fb90f751a?source=your\_stories\_page---------------------------](https://towardsdatascience.com/beginners-learning-path-for-machine-learning-5a7fb90f751a?source=your_stories_page---------------------------)",NA
"2291",2290,37,"I posted an [article](https://megacog.com/post/ACfrUhFsg0) a couple of days ago on my blog for what I think can help you. It is like a roadmap for becoming a machine learning engineer. You can find it here [https://megacog.com/post/ACfrUhFsg0](https://megacog.com/post/ACfrUhFsg0)",NA
"2292",2291,37,"> What's the proper guide one should follow to get into this field?

What would you say ""into this field"" means to you? Ages ago I wrote the [""Getting Into ML""](https://www.reddit.com/r/learnmachinelearning/wiki/index) guides on this sub's wiki, which are relevant here.

The Hacker/Hobbyist approach to being 'in the field"" is very different to the ""Engineer"" path or the ""Scientist"" path.",NA
"2293",2292,37,"Go look at Kaggle's courses. I tried learning through stack overflow and youtube, bur while doing the kaggle courses i found a bunch of problems i have.",NA
"2294",2293,37,"This compilation maybe of benefit for the beginners. https://github.com/ayonroy2000/100DaysOfMLCode/tree/master/Coursework.md",NA
"2295",2294,37,"Brilliant.org

Mathematics for machine learning is both a coursera mooc and a textbook. Start with the coursera then go to the book. 

If you don’t want the math background edx has an MIT sequence that oh wait, that uses math too. 

Learn math, and you’ll learn machine learning.",NA
"2296",2295,37,"I have done all three recommendations and they are very solid. The  Hands-On ML book is probably my favorite as it is easy to read and understand. Udemy ML A-Z is one of the best courses I have taken and they have template scripts you can use for your projects.",NA
"2297",2296,37,"Thanks for sharing these resources! I have already enrolled into a few of Andrei's courses. Definitely thinking of enrolling into his ML course.

Edit: I just realised that you said Andrew and not Andrei (he released a course on ML recently) 🤦🏻‍♂️",NA
"2298",2297,37,"The book you mentioned is bit advanced for beginners. Using book after the course may result in a smother transition.",NA
"2299",2298,37,"What level of math sophistication do you need for the hands on machine learning book?",NA
"2300",2299,37,"For me personally Ng just droned on. He’s a genius but maybe just not a teacher. But OMG I LOVE that Hands-On Book. It feels less of a textbook and more like a friend explaining it to you. Can’t recommend it enough.",NA
"2301",2300,37,"Can you let me know who the instructor is for the Udemy course?  I see a couple of machine learning a - z but none with that many reviews nor many stars. Thanks.

EDIT:  scratch that. Would help if I spell “learning” correctly. I see it’s a few instructors, Eremenko being one of them...",NA
"2302",2301,37,"Is it important to know little about data science for ML?  
If yes, how important?",NA
"2303",2302,37,"Great! Will check that out",NA
"2304",2303,37,"I would say Hobbyist/Engineer. Is that possible?",NA
"2305",2304,37,"Ok! I have no problem learning the math 😄.
I just needed to know from where. Thanks for sharing the websites. I'll be checking them out soon!!",NA
"2306",2305,37,"Have you been able to use those templates in the 'real world'? I bought the course a while back and got the impression that they do not do a good job of teaching you how to use their teachings for problems you can encounter in the wild. Would love to know if you think otherwise.",NA
"2307",2306,37,"Yeah i believe in that too, Normally i watch video before reading books on Programming",NA
"2308",2307,37,"Linear algebra, calculus, and statistics

You can’t really succeed in ML without knowing all three",NA
"2309",2308,37,"Yup that’s him! It’s him and his SuperDataScience team. They have a pretty good podcast too on all topics Data Science related",NA
"2310",2309,37,"ML is data science, what do you mean? You need to know how to analyze data, use pandas, numpy libraries, etc...

Finding a best-fit straight line through a bunch of data points is considered machine learning too in the traditional sense.It is predictive modeling and learning from all the data points to predict new values from the ""best fitting straight line"".

Now deep learning is a bit different which involves neural networks. It is a sub-category of ML because it can learn at a much ""deeper"" level, hence the name.",NA
"2311",2310,37,"http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf",NA
"2312",2311,37,"Not sure what a mix would entail?",NA
"2313",2312,37,"The downside to the A-Z ML course is that it isn't really a course. There is no homework assignments with solutions to help you see the concepts in different contexts.

The best thing to do is to find already done Kaggle problems with solutions and try to obtain those solutions yourself.",NA
"2314",2313,37,"Yes. I work as a Decision Scientist and when we were shopping for ML products to use, I used the templates to quickly run models to test their platforms and it worked. I even used it to run XGBoost models and it was setup nicely. It is simple to use.",NA
"2315",2314,37,"i tried to go through the courses - particularly Andrew Ng's course about 2 years ago and found it futile without the math - spent the last 2 years basically learning algebra, calculus and stats - it was hard but definitely worth it . Going right back to fundamentals.",NA
"2316",2315,37,"Thank you so much! I'm on my 'starting out' phase, I'm gonna pick up a good data science course now. Could you recommend me some courses? Or books?",NA
"2317",2316,37,"Well, right now I'd say I'm more of a hobbyist",NA
"2318",2317,37,"Gotcha. Thanks",NA
"2319",2318,37,"Gotcha. Thanks man",NA
"2320",2319,37,"Hmm I would find the most highly reviewed data science course on Udemy.

There are lots of options even youtube but Udemy/Coursera/EDX courses are the most thorough. 

You need to do practice problems and not just watch endless tutorial videos to get the skills engrained in your head. So courses are good for that... good luck!",NA
"2321",2320,38,"If you're from India you can do NPTEL Course called as deep learning part 1, and part 2 by iit madras

Hands down it was one of the best courses I've taken for concepts

Try udemy machine learning A-Z and deep learning A-Z",NA
"2322",2321,38,"> It took me almost 1 month 

Only?!",NA
"2323",2322,38,"ISLR is great to learn ML from scratch, you'll learn R as well.",NA
"2324",2323,38,"Thanks",NA
"2325",2324,38,"Yeah, I spent 2-3 hours a day.",NA
"2326",2325,38,"I guess you've excluded naps randomly happening when watching the series?",NA
"2327",2326,38,"I could not do that..lol",NA
"2328",2327,39,"You should include the compensation you're offering for this consult. What's the hourly rate you're providing for their time?",NA
"2329",2328,40,"Danger Zone :D",NA
"2330",2329,41,"fwiw my recommendation is that if you've already finished the whole machine learning specialization on Coursera would be to start working on real problems and not get sucked into the trap of taking course after course after course.",NA
"2331",2330,41,"I certainly appreciate the recommendation. I have tried tackling a couple kaggle datasets but my python skills for machine learning aren’t quite where they need to be. I understand the different models and when they should be used, but implementing it in python is an area where I need to improve. Thank you again for the advice!",NA
"2332",2331,42,"[deleted]",NA
"2333",2332,42,"That's helpful. Regarding gradient descent — how do multiple epochs help find the global minimum (more) accurately?",NA
"2334",2333,42,">That's helpful. Regarding gradient descent — how do multiple epochs help find the global minimum (more) accurately?

Not really, you're working in advertising. I think some sort of call Centre position. It's not beneath me, but it's also not the plan. Let me give you some advice, never trust anyone. When humans have human contact, it creates conflict. Therefore, you should never trust anything that I say. I'm replying out of courtesy, but I'm also blocking you now.",NA
"2335",2334,44,"No. Approximately 4000 daily patient location data points for approximately 200 patients' routes out of the 7000 or so possible patients in the joined table.",NA
"2336",2335,45,"Hey, so your bounding box is the whole image ? Object detection is about localising a specific object inside an image, if you do like that, you probably need a classifier and not a detector.",NA
"2337",2336,45,"Ok, that makes sense - thank you!",NA
"2338",2337,45,"Hey, yes - the bounding box is the whole image. My goal is to detect a certain object from a webcam, crop it, and send it further to a siamese neural network.

Basically same as they have done [here](https://www.hackster.io/166810/ince-intelligent-cat-entrance-73a065)",NA
"2339",2338,45,"So your bounding box labels are like (0,0,1079,1919) (xyxy format) ? Since you’re whole image is inside the bounding box, your bounding box starts at top left corner of your image and ends at the right bottom corner ? You should use uncropped image, and the bounding box coordinates should be inside the image, not the whole image it self.",NA
"2340",2339,45,"Yes, thats correct.

OK, I was afraid of that. Out of curiosity, why? 
And do you have any idea how many images I should annotate?",NA
"2341",2340,45,"Well ... a detector learn to localise object. So if you give him a picture and you tell him “the whole picture is the object I want to localise” then the model will just output what you told him to output, the model learns that “the whole image is the object” and hence it outputs the result that you got.

Now, if you feed it an image of size 1080x1920 (whole image), let’s say the picture contains a cat on a couch, and a cat that is 300 by width, and  by 160 height, is at position 250, 1000 (the part of the picture you want to crop), then your model learns to tell where the cat is and the bounding box is no more the whole image.",NA
"2342",2341,45,"OK - I see, thanks!",NA
"2343",2342,49,"Would it be possible to make a Colab demo for this?",NA
"2344",2343,52,"Does it charge you right away or after a 7-day free trial?",NA
"2345",2344,52,"See what works for you the best",NA
"2346",2345,52,"They charge this amount right away I guess",NA
"2347",2346,52,"So I wonder. Maybe I do the 7 day trial and then go through this link?",NA
"2348",2347,54,"Probably the best option is to not include both as the variance is the std**2. Did you check if how does your metric changes using one or the other?",NA
"2349",2348,54,"I haven’t checked yet because the model I made takes forever to run and I am in a serious pinch for time!",NA
"2350",2349,54,"Ok so I recommend you to use one of them. Are you using mean or median stats too? 

As an advice, try to make some time to check it or let it running in background so when it finishes you only need to see the results :)",NA
"2351",2350,54,"Thank you for the help! I will run it when I go to bed because I’ve put so much work in today already. Yes, I am working with mean stats as well",NA
"2352",2351,54,"Are you working in anomaly detection or something like that? Median might be a better option due to it is not much deviated by extreme values as mean is. In my last AD project I used median and the IQR values as far as I remember.. 

Glad to help you :)",NA
"2353",2352,54,"No, not anomaly detection. To give you a good idea of what I’m working with, my last post in my post history outlines the problem I am solving! I know how to approach it now so the post itself is outdated but it is very good to give you an idea",NA
"2354",2353,54,"Nice problem to work with! Wish you luck to solve it! :)",NA
"2355",2354,54,"Thank you :)",NA
"2356",2355,56,"Aweaome, thanks for these resources. Will be sharing with teammates.",NA
"2357",2356,56,"Happy to help.",NA
"2358",2357,57,"iirc its a stats method for removing cyclical patterns so you can focus on the deviations from the cycles. idk how you do it though. revert to learning more stats",NA
"2359",2358,58,"Boring",NA
"2360",2359,58,"MLOps is super important but what about stuff a little closer to the exploration and innovation phase? What about [DSOps](https://gigantum.com/)?",NA
"2361",2360,58," A subreddit dedicated to ~~learning machine learning~~ advertisements",NA
"2362",2361,58,"I also work at the same company... but u/tylercasablanca pointed this sub out to me, and I realize it's probably a good place for me to engage (including on not-Gigantum things). I used to support data science education at UC Berkeley, and would love to see a less academic side of things.

I'm wondering if folks would be open to help from me (and others) if it came through our DSops platform?",NA
"2363",2362,58,"For clarity, I work at a company that does DSOps.",NA
"2364",2363,60,"Video depicts pose estimation.",NA
"2365",2364,62,"[This blog post](https://medium.com/@5agado/stylegan-v2-notes-on-training-and-latent-space-exploration-e51cf96584b3) is a collections of notes and results collected while training multiple StyleGAN models and exploring the learned latent space. It is meant to provide insight or starting point for discussions for other people out there with similar interests and intents",NA
"2366",2365,62,"Cool, it's a clothing themed DMT trip. I myself wear clothes, therefore I can appreciate the need for this.",NA
"2367",2366,62,"I'm actually surprised there's no AI designing clothing brand yet. At least none I've heard of",NA
"2368",2367,62,"Sorry if I missed, but what were the training times/hardware used?",NA
"2369",2368,62,"Amazon is working on one",NA
"2370",2369,62,"[Glitch](https://www.vice.com/en_us/article/vb9pgm/this-clothing-line-was-designed-by-ai) it's a well known example.

There have been other minor cases in the news, and more common ""collaborations"", see for example [Acne Studios](https://hypebeast.com/2020/1/acne-studios-fall-winter-2020-collection-runway-paris-fashion-week), and generative fashion initiative, see [Zalando Generative Fashion Design](https://research.zalando.com/welcome/mission/research-projects/generative-fashion-design/) and [TailorGAN](https://arxiv.org/pdf/2001.06427.pdf).",NA
"2371",2370,62,"~14 days on 2 NVIDIA GeForce GTX 1080 Ti, for each dataset",NA
"2372",2371,62,"In terms of recommending existing products based on your preferences or designing entirely new clothes?",NA
"2373",2372,62,"Cool thank you dude",NA
"2374",2373,62,"I see, thanks.",NA
"2375",2374,62,"You give the system an image and it’ll style your body based on your dimensions and style type you’re looking for",NA
"2376",2375,63,"As a tip: Don't start a project with the tool.

When a construction worker begins work, he/she doesn't say ""Here's a hammer. What can I do with it?"" They say ""What do I need to accomplish? Does a hammer help me to accomplish that?""

It's the same for software. Start with the idea THEN figure out what tool you need to accomplish the task. Don't get caught up in ML hype. As cool as it is, ML is still just a tool.

That being said, what problem are you trying to solve? If you don't know, talk to the person you're helping about what problems they're having, then decide if you can help with any of them using software. Once you identify the problem, THEN you can decide if ML is the correct tool to solve the problem (spoiler: it probably isn't).

On the other hand, if you're just learning ML and you're using this as an opportunity to have some real world project that you can experiment with, go for it! Maybe something like efficiently finding routes for the vehicles to take (ML is *not* the correct tool to solve this problem, but you can use ML to solve it as a learning exercise).",NA
"2377",2376,63,"Yeah I always look to ensure I’m solving a business problem as opposed to solutioning. I figure their fleet problems are the same as other company’s fleet problems who may have already implemented some ML which I could look to. 

There has to be opportunity for preventative maintenance but on the other hand, I don’t even know what data they have! So a model might be impossible.",NA
"2378",2377,66,"Sounds very promising, gj!",NA
"2379",2378,66,"I also wrote a [blog post about using bash and jq to search notebooks](https://www.optowealth.com/blog/better-way-to-search-your-notebooks) if anyone is interested.",NA
"2380",2379,67,"Playing games from raw pixel data seems like a decent sized project, depending on how well you want your agent to play. Deep Q Networks and OpenAi’s gym could be a useful place to start if you’re interested. DM me if that’s up your alley and I can send you some resources :)",NA
"2381",2380,67,"Well, I read somewhere that Renaissance funds uses hidden Markov models to predict prices of stocks. Additionally you can build a recommender system using rl.",NA
"2382",2381,67,"Hey last year I did the same as part of a personal project.

I bought the game SuperHexagon on steam. It's very cheap.

https://store.steampowered.com/app/221640/Super_Hexagon/

I used pillow, OpenCv and tensorflow for the project. My agent learned to play this game.",NA
"2383",2382,67,"Related to stock trading: I've been reading about real time bidding in online advertising lately and there are some really interesting reinforcement learning approaches being used.

You could implement a recent paper or try some idea from your course. If you open-source your code, I might even be able to use it. ;)

Here is a good list of literature: https://github.com/wnzhang/rtb-papers",NA
"2384",2383,67,">reinforcement learning for computer vision tasks (object detection, segmentation, tracking, etc.)

What's that?",NA
"2385",2384,67,"I don't know about the difficulty you are looking for, but there is a recent Kaggle competition created by the creator of Keras that is trying to create AI that can solve visual IQ problems. Might be interesting to work on.",NA
"2386",2385,67,"Agree with this, I’ve done a reinforcement learning project with similar length constraint before, and OpenAI gym is a great place to start!",NA
"2387",2386,67,"Do you remember where you read that?",NA
"2388",2387,67,"Thank you for the link! Yeah I’d like to ideally do something related to recent literature.",NA
"2389",2388,67,"No idea but Baum used to work under him and he published a Baum Welch algorithm using HMMs. 
The actual algorithm is explained in Silvers's course.

Edit: found some of it here 
https://quant.stackexchange.com/questions/30056/james-simons-renaissance-technologies-corp-and-his-model

If you want to work on it let me know. I would love to collaborate as well.",NA
"2390",2389,67,"Good luck! Let me know if you end up doing something with this.",NA
"2391",2390,69,"If you’re truly new to the field, research papers are probably not the right place to start. Instead go look for intro courses or in your case, specific DL and RL for robotics. Also, if you’re used to reading research papers, even if you have the background it’s pretty hard to read. Reading research papers effectively is a skill itself.",NA
"2392",2391,69,"Thank you! 
Actually I am doing cs231n by Stanford. But I don't feel confident enough to carry out anything. What would you suggest?",NA
"2393",2392,69,"if you want to feel more confident, the best thing you can do is get your hands dirty and try building some models even if it’s just toy models",NA
"2394",2393,69,"Okay, thank you! I will definitely try this.",NA
"2395",2394,70,"
I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/bscottnz/Projects/blob/master/PokeNet.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/bscottnz/Projects/master?filepath=PokeNet.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",NA
"2396",2395,70,"Really enjoyed this, thanks for posting. I didn't know about split_folders and tried it out and it works beautifully!",NA
"2397",2396,71,"I'm super curious about how RNNs work in text and music generation now.",NA
"2398",2397,71,"I’d rather ask what they have in common",NA
"2399",2398,71,"Look up Seq2Seq for text generation. Essentially, you have two RNNs: one that takes input text word by word to create a text encoding, and another that produces text from the encoding, word by word.",NA
"2400",2399,72,"If by ""cut and paste programming"" you mean using others' code extensively, then I don't see a problem with that if you are a beginner, because that's one way to learn how people do stuff to get a grasp.",NA
"2401",2400,72,"Not particularly specific to ML but writing out the code you're copying line by line and trying to understand it to the best of your abilities is a good way to go. Slowly you'll rely on copied code less and less. Edit: Spelling",NA
"2402",2401,72,"You will very quickly begin to see solutions you can cut and paste but which you choose to modify because you see improvements, simplifications, or stylistic choices you don’t like. At that point you have nothing to worry about. You are only using what you need for speed, and aren’t reliant on code you don’t fully grasp.

No one is expected to do everything from first principles and the kind of coders who write everything themselves end up ignored, or working on a little app because that’s all they can scale to (see “Overcast”).",NA
"2403",2402,72,">praticularly

What's that?",NA
"2404",2403,72,">You will very quickly begin to see solutions you can cut and paste but which you choose to modify because you see improvements, simplifications, or stylistic choices you don’t like. At that point you have nothing to worry about. You are only using what you need for speed, and aren’t reliant on code you don’t fully grasp.

>No one is expected to do everything from first principles and the kind of coders who write everything themselves end up ignored, or working on a little app because that’s all they can scale to (see “Overcast”).

Already made the training complete in Google colab by increasing the batch size from 1 to 10, and decreasing epoch number from 10000 to 1,000.",NA
"2405",2404,72,"Poorly spelled particularly.",NA
"2406",2405,72,">Poorly spelled particularly.

Okay, okay, I suggest using ASR if you're on a mobile device, and using grammark otherwise.",NA
"2407",2406,75,"You are adding layers to the sequential model defined in the `model` variable.",NA
"2408",2407,75,">You are adding layers to the sequential model defined in the model variable.

Is someone making one layer the input of the other, or is it some sort of ensemble learning, where a machine learns to correct for the errors of another?",NA
"2409",2408,75,"The first one, you are combining multiple layers in a sequential manner, i.e. the outputs of one layer are the inputs for the next layer, to create a full model. See also the tensorflow documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Sequential",NA
"2410",2409,75,">The first one, you are combining multiple layers in a sequential manner, i.e. the outputs of one layer are the inputs for the next layer, to create a full model. See also the tensorflow documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Sequential

Okay, that must be why it's called sequential then. I want to ask you a question.",NA
"2411",2410,76,"We just hired an intern like you and are essentially putting them on an e2e ML project. They're finishing up CS undergrad and moving to a master's next year. A demonstrated interest in the subject is why we hired them, and it should really get them started.",NA
"2412",2411,78,"Please Google this before asking questions. You need to learn how to research really simple stuff on your own.",NA
"2413",2412,78,"I'm here after googling. I read the documentation on sklearn but couldn't understand.",NA
"2414",2413,78,"Then you need to go deeper. Use the information there and query again.",NA
"2415",2414,79,"Try using [optuna](https://optuna.org/).
They have great documentation and tutorials.",NA
"2416",2415,81,"This... Makes me very happy. I’m going to check this out when I’m at my computer.",NA
"2417",2416,81,"Having some depth perception by scale comparison would be nice.",NA
"2418",2417,81,"Okay, this is hilarious.  Awesome job!",NA
"2419",2418,81,"Thanks all! Last night I added the ability to continue if you want to continue using it without it pausing every time.",NA
"2420",2419,82,"This video explains from scratch what a neural network is, how to write a neural network to recognize handwritten digit and train it in Tensorflow 2 using Google Collab GPU runtime. The dataset used here is MNIST: http://yann.lecun.com/exdb/mnist/   Code:https://colab.research.google.com/github/ankitjamuar/mnist\_neural\_network/blob/master/MNIST\_ANN.ipynb 

Github: https://github.com/ankitjamuar/mnist\_neural\_network  

Blog: https://www.meshcookie.com/writing-neural-network-using-tensorflow-2-to-recognize-handwritten-digits-on-google-colab/ 

Facebook: https://facebook.com/meshcookie/ 

Twitter: https://twitter.com/mesh\_cookie 

Github: https://github.com/ankitjamuar/ 

Linkedin: https://www.linkedin.com/company/meshcookie/ 

Instagram: https://www.instagram.com/meshcookie/",NA
"2421",2420,84,"If I understand correctly you just have an unsplit dataset right?

In that case you should split into Training, Testing initially and never touch the Testing set from that point onwards (e.g.  via setting a random seed first or saving it into an external file).


Then you use your full training set into GridSearchCV.

This will split in into training and validation for each cross validation fold, and choose the parameters which perform best based on the average accuracy across all folds.

Finally after you have decided the best parameters to use you can train it again on both Train+Validation and report your metrics on Testing.",NA
"2422",2421,84,"Okayy thank you so much :)",NA
"2423",2422,86,"The short answer is yes.

You have to remember, convolutions and pooling are just operations. They don't care if they operate on a window of 100 or 1000000. In that sense, you can absolutely train some weights and then just make them run ""blindly"" on something bigger. Just be careful that you truly are matching similar scales.

Also, one more thing,  by the looks of it you might be over complicating this problem. Not everything has to ""trained."" Looking at your inference image, you seem to have a decent number of region props, but in reality the numbers are all taking up the same amount of space. You can just hand code a few good window sizes for your sliding window. You could even hand code a pretty basic but robust number detector using the number of nonzeros.

Are you familiar with RCNN and it variants?",NA
"2424",2423,88,"I would like to do something similar to this.  But my concentration is shit. Congratulations",NA
"2425",2424,88,"Github Repo:
https://github.com/hithesh111/Hith100

PS: I'm continuing after 100 days and decided to document the progress in the repo below. 
https://github.com/hithesh111/HithBeyond100",NA
"2426",2425,88,"That's pretty impressive. How many hrs did you put each day approximately?",NA
"2427",2426,88,"Well done man, keep it rolling",NA
"2428",2427,88,"After taking the coursera one why didn’t you list octave or matlab as a coding language. Out of preference?",NA
"2429",2428,88,"How did you do the Udacity course? Is it free?",NA
"2430",2429,88,"Wow. The course content mentioned in this list is excellent to learn. I think adding Feature Engineering to this list would be nice.",NA
"2431",2430,88,"I would like to know where can I find the Stanford NLP course ?",NA
"2432",2431,88,"Is the NLP by Jurafsky on yt?",NA
"2433",2432,88,"Congratulations! Great motivation!  
I would like to know how much does is cost to you taking all the Deep Learning courses provided by [deaplearning.ai](https://deaplearning.ai). Have you asked for any financial aid?",NA
"2434",2433,88,"Damn, GoodLuck. I am doing Andrew Ng Coursera now, about 30,35 days have passed yet. Since I am also doing University so my speed is comparatively slow. Hopefully I will able to do a lot more than this in summer break...",NA
"2435",2434,88,"Cool. I'm taking some of these as well.",NA
"2436",2435,88,"Congratulations man!!

I tried to do it too but didn't plan it ahead and just went to a spiral and then dropped it. Will try to do it again.",NA
"2437",2436,88,"May I know what was your background prior to starting this? Any tips on where exactly I can start or Can I really just start base off your list",NA
"2438",2437,88,"How important is it to learn from highly reputed courses like coursera n.g. I am learning ML from youtube videos like Simplearn.",NA
"2439",2438,88,"Good stuff man. Congrats. That is some nice progress.",NA
"2440",2439,88,"I'm starting my journey this month.",NA
"2441",2440,88,"Was the data visualization course useful? That sounds like the most interesting one to me at the moment!",NA
"2442",2441,88,"Josh starmer is a hero",NA
"2443",2442,88,"Congrats on the progress so far.

Have you done any projects along the way? And also, is doing the course videos (without assignments, etc.) sufficient for an in-depth understanding?",NA
"2444",2443,88,"Andrew NG is a crack!",NA
"2445",2444,88,"How did you find the time to do all of this!?",NA
"2446",2445,88,"Congrats!! Just got a job that involves AI, NLP & ML (I'm assuming) on the go. Did they give you a solid understand on what goes on in those fields?",NA
"2447",2446,88,"How much time per day do you spend on this? How do you feel about the progress you've made?",NA
"2448",2447,88,"Have you messed around with OpenAI's gym? It's pretty fun to watch, not sure if there's specifically something to learn but interesting nonetheless",NA
"2449",2448,88,"Hey, how do I get started in 100 days of ML as someone who has only the most rudimentary knowledge in machine learning?",NA
"2450",2449,88,"congratulations !! That's inspiring .",NA
"2451",2450,88,"My yankee self was doing the math like, “I’m pretty sure March 12, 2019 - November 3, 2020 is over 2.5 years. And it’s only March!” 😂",NA
"2452",2451,88,"Thank you. 

You can try starting slow. I tried the same challenge once with poor planning back in September last year and it fell apart on Day 6.

Once you can do it consistently till Day 15-20, the habit will most likely stick to your daily routine. (at least in my case)",NA
"2453",2452,88,"Then do 100 days of concentration/discpline. There are many ways to work on that. Streak, timeboxing, timetracking(toggl), time planning and estimating your work. Working on being intentional in your taks in relation to small goals.

It's a skill in itself and it takes much longer to train, might aswell focus on that early.",NA
"2454",2453,88,"You could always use the pomodoro technique. Set yourself a timer for 25 minutes in which you work or study on that one thing without distractions. After that, take about a 3-5 minute break. Then another round, and so on until you finish four 'pomodoros', after which you take a longer break. [https://en.wikipedia.org/wiki/Pomodoro\_Technique](https://en.wikipedia.org/wiki/Pomodoro_Technique)",NA
"2455",2454,88,"That is some serious journaling! You are tempting me to it heh",NA
"2456",2455,88,"Thank you. I started during my winter holidays. Then I used to spend upto 5 hours a day. When college resumed, I used to put 30 mins - 1 hour a day on normal days, 2-3 hours if I'm too curious or if I'm coding and 15-30 mins on busy days (days when I'm exhausted or exam days).",NA
"2457",2456,88,"Thanks and happy cake day.",NA
"2458",2457,88,"I did that course a year ago and started the challenge with a revision of the course(excluding assignments). I have forgotten most of Octave and never used it since then. Also, I prefer to work in Python.",NA
"2459",2458,88,"The Tensorflow course was free.
'Intro to Tensorflow for Deep Learning.'",NA
"2460",2459,88,"How to Win a Data Science Competition course by National Research University Higher School of Economics has some very good lectures on Feature Engineering methods. The videos are available for free on YouTube.

Referred this while doing competitions. Thanks, I'll add that as well.",NA
"2461",2460,88,"This is one of them that might start you at a good point: https://youtu.be/tZ_Jrc_nRJY",NA
"2462",2461,88,"YouTube. 
https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm",NA
"2463",2462,88,"Don't know if it's official but here's what I've referred. 

https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm",NA
"2464",2463,88,"I have done only the lectures from Youtube, not the assignments and quizzes. However, as suggested by someone on another post of mine on the same topic, I do plan to take the 7-day trial someday and try to finish all of the assignments and quizzes in that week when I'm confident enough.",NA
"2465",2464,88,"All the best!",NA
"2466",2465,88,"All the best!",NA
"2467",2466,88,"Even I didn't plan everything ahead but I always had it in my mind what I could do for the next 5 days at least.",NA
"2468",2467,88,"I had done the ML Andrew NG course before (had forgotten most of it, so I started with revising the course for first 10 days) and had surface level knowledge of a lot of things related to machine learning and deep learning.

One thing that will surely help is googling what you want to learn, skim through many articles and watch YouTube videos at 2x, 3x not looking to learn deep theory, but instead trying to figure out what are the keywords, trending topics and what you can build with the ML methods.",NA
"2469",2468,88,"To some extent it matters. But if you believe objectively that the material from any online learning content is good, then it shouldn't matter. Another thing that matters is the instructor teaching it. Andrew Ng really is a renowned authority on ML, so it checks out to take his courses. Also, they are free, unless you wish to access particular learning materials or earn a certificate or be graded on your assessment, you'll have to pay a token sum.",NA
"2470",2469,88,"All the best!",NA
"2471",2470,88,"Yes.",NA
"2472",2471,88,"I have done a reasonable amount of projects and implementations too. Even when I'm doing only lectures, I try to code certain things down to see how things work. Yes, projects are needed to get in-depth knowledge.",NA
"2473",2472,88,"I got an internship offer from LinkedIn. However, due to some personal reasons, I won't be able to work full time during summer break this year. Also, I'll be trying for a higher degree in Statistics before I join a permanent job.",NA
"2474",2473,88,"No. I'll check it out. Thanks.",NA
"2475",2474,88,"Andrew NG course covers almost every relevant topic upto reasonably good depth for a starter.",NA
"2476",2475,88,"Great, one challenge i face while watching videos is that i get curious about other way of doing the things and pause the video and start googling stuffs, aand hence my speed is very slow and time just ends. 

However, i do learn deeper concepts by this approach of side quests.",NA
"2477",2476,88,"Every little bit helps, congrats on completing it!",NA
"2478",2477,88,"Thanks :)",NA
"2479",2478,88,"Thank you for the tip! Super Helpful :)",NA
"2480",2479,88,"Thanks!",NA
"2481",2480,88,"Thank you !",NA
"2482",2481,88,"Thanks!",NA
"2483",2482,88,"Thanks! Just curious what are you taking in university?",NA
"2484",2483,88,"I really like the material that I can use offline  (I go to places with no wifi to study like a library). Thats why I shy away from Coursera even though some courses are free. Although I have heard about Andrew Ng, i guess its worth checking it out. Thanks",NA
"2485",2484,88,"Ah! The famous rabbit hole. Personally have a solid love/hate relationship with it. I think it's important and valuable to recognize that there is a time for it and a time for completing tasks.

Something that works for me is to track my time with Toggl and try to keep it as close to the reality as possible. This mean recognizing when i get outside of my task and bringing myself back to it. The fact that a timer is running keeps my mind on point after a while.

Timeboxing can help to. If you assign 1h to a task and are intentional in your action to complete it in the assigned time, then you'll keep out of rabbit hole.

Taking a note of your sidetracks and going back to them in lazier time is good thought.

Progress not perfection.",NA
"2486",2485,88,"I am in my final year of BMath at Indian Statistical Institute.",NA
"2487",2486,88,"I agree with you as well, as it's relevant for learners with little access to wifi. Also, yeah good luck to us all!",NA
"2488",2487,92,"I am also confused about the adjacency matrix in that article.",NA
"2489",2488,92,"The adjacency matrix is being computed by the SpectralClustering() function.

You may ask, ""that dataset was for credit card spend, how can spectral clustering be used when it's not even graph data?""

The answer is that under the hood, SpectralClustering() is performing a k-nearest neighbor algorithm (default value would be *n\_neighbors=10).* This means there are now ""edges"" between each point and it's k-nearest neighbors, so the algorithm uses this graph-like representation to construct the adjacency matrix, then the Laplacian, then finds the eigenvectors and clusters them to create the spectral cluster assignments.

Alternatively, you can create your own affinity matrix and just call SpectralClustering(affinity='precomputed') and it will treat the matrix being fitted as a user-provided affinity matrix.",NA
"2490",2489,92,"Wow! I love this answer! Thanks!
Have you used spectral clustering before? Perhaps for outlier detection?",NA
"2491",2490,92,"I thought affinity = 'precomputed'
Corresponds to the default setting?",NA
"2492",2491,92,"Nope, the default in sklearn is affinity='rbf'",NA
"2493",2492,96,"I think co efficient of correlation can be used for continuous target variables i.e. regression and not for classification which has discrete labels",NA
"2494",2493,96,"Thanks for the reply. I'm currently doing data preprocessing for that classification problem and I'm  excited to find out about the relation between correlation coefficient and the classification result.",NA
"2495",2494,99,"> paft

What's that?",NA
"2496",2495,99,"A lot of finance organizations have hand written documents. How big of a problem was this? Did you have those manually entered still?",NA
"2497",2496,3,"I don't know anything about that bootcamp besides a cursory look at the links you posted, but it seems like it's trying to touch on too many topics at too light a level to be really useful. You're going to learn machine learning in 3 weeks of night classes? And it's in the same category as tableau? Huh? It is hard for me to imagine a job where you'll need to use SQL, Excel, ML, also make front-end web viz.... it kind of seems like they took the wide spectrum of ""data science"" and tried to shove all together. In reality, some DS jobs will have lots of SQL and Excel but no ML. Or others will have ML and python but not so much on the data viz side. I dunno. Which kind are you shooting for? Which kind of DS job is in line with your interests *and your strengths*? Everyone is interested in deep learning, but at the end of the day, what is the special, unique skill that you will be bringing? I think it's a good idea to-- several months if not more before starting the job search-- browse job descriptions at companies you might be interested in working for. What are the specific skills that they require?

What do alumni from this program do? the two professionals ""possibly"" helping with employment afterwards doesn't sound that great. This sounds like something that could be worth it if you really wanted to learn these skills but were unable to do it on your own. I do not think that this class will really buy you the credibility you are hoping for. I don't really know because I'm not on the interviewing side of things, but I don't expect that a certificate of completion will really help your case all that much. It sounds like you don't have a degree beyond high school? I think DS positions are going to be very challenging for you to find. Sorry to be a downer, and maybe someone can correct me if they know of some examples where someone was able to make that leap and how they did it. But it sounds like if your skills are excel and pandas, focus on data analyst positions first. They might not pay as much or be as exciting, but they could help you get your foot in the door. Are there situations in your current job where you somehow could do a mini-project that could go on your github? Build a viz or model to predict patterns in demand at your bar to help staffing/supply efficiency? Do you know people who could put in your resume at a company and vouch for you even if you don't have the typical background they're looking for?",NA
"2498",2497,3,"1.	GATech online masters in analytics cost 10k all in. 
2.	even with a masters the competition is fierce. If you go to LinkedIn and look for Entry level data science/analyst job, there’s usually hundreds of applicants within a week or so. If you compare it with entry level civil engineering or mechanical engineering, it’s probably in the teens within over a month. That should tell you what your chances of landing a DA/DS job from a bootcamp.",NA
"2499",2498,3,"Thank you for all this detail! Let me see if I can answer some of the questions. My interests are in data visualization and financial algorithms for stock trading. I am good at spotting potential correlations/patterns in data and reformulating data to tell stories from different vantage points if that makes any sense. My end goal is to be a machine learning engineer. A lot of the places that I want to work seem like they put the entire kitchen sink as ""preferred experience"". SQL, HTML, CSS, C++. Python, ML are on almost all of them as well as a BS in computer science. My highest level of education was an AA degree in English Lit, don't know what the hell I thought I was going to do with that. 

Your alumni question is a great question I should ask the recruiter. I saw reviews of the boot camp online but it wasn't that many. They did have good things to say though. Seems like the DS field in inundated with entry level talent, so you might have a point that this will not give me the credibility that I am looking for. One place that I really wanted to work mentioned that one of their employees is taking this boot camp, so I thought if I enrolled and then went over for an interview this might lend me some credibility, at least with them. 

Yes there are some mini-projects I could work on just like that, I will start there and do several of them. Great idea. I do have a friend who is a sales manager at an SEO company that would vouch for me as a good worker, but we haven't actually worked together. Only reason I can't work as his company are the resume scanners they use scanning for BS, and MS. Sad but true. I wouldn't mind being a data analyst at all, like you said it is a foot in the door and I will absolutely take that.",NA
"2500",2499,3,"Holy hell, I had no idea.....thank you for posting this. I will check out the GATech Masters, but maybe I should switch to mechanical engineering instead? My highest level of education is an AA in  English literature from a community college.....what did I do with my life?!",NA
"2501",2500,3,"I think you should think to yourself: if a company is hiring a machine learning engineer and they are picking between someone with a phd in physics vs you, why should they hire you? I'm not saying that to be mean or discouaging-- there might be reasons now, and you might be able to take steps to create more reasons. You need to find an answer to that question, because that is the choice that the companies will be making. I think answering this question could help guide you to the right first position (for example, something where you're facing clients to leverage all of your experience in hospitality, where someone straight from academia might fall flat) and also help you define what gaps/training you need to fill to make a stronger case. And of course, this message needs to be hammered home in your cover letter, resume, etc. You're an unusual background and thus will seem like a risk to hire-- what is the benefit you'll be providing to make it worth that risk? Are there jobs that the physics phds aren't applying for that you would be willing to take now to help you reach a MLE position 10 years from now?

What is your math background? My guess is that unless you took some extra math as electives during your AA, this could be one of the most actionable areas to develop. You could start at an accredited community college to save money.

I'm confused why if your friend works at a company he can't refer you and bypass the resume scanners? This doesn't make sense. The last job my husband was offered, he never even gave a resume-- he just talked to people at a networking event and got the interview from there. Based on the way that my husband and I have gotten our jobs, I think the world of sending resumes out into the void is crumbling. It's mostly be networking. My husband's resume is stellar and he submitted his resume to company X but never heard back. Got offers at all (except maybe one? Not sure) of the other companies he applied to by talking to someone first. After meeting some people at company X through some conferences, they keep trying to recruit him. I think resumes just aren't really how things work much anymore",NA
"2502",2501,3,"Not sure if that was totally rhetorical or not. Whatever you’re doing, whether it’s this or something else, if you’re looking to evolve and learn then what you’re doing with your life is the right thing. Good luck on finding the right path!

FYI - know that a lot of employers understand how to quantify a bs or ms more than a certificate of study. There are still a ton of jobs despite the competition for data scientists. If you’re doing the UCSD thing rock on. Just know you’ll need some good portfolio examples to show what you can do for employers and hopefully yourself. Rock on my friend.",NA
"2503",2502,3,"I think you are being very fair, not discouraging at all. I think about this often, why hire me instead of someone just graduating college with a BS in compsci or an MS in mechanical engineering? Well because I am here, I am actively choosing this company as a long term career option and I am willing go above and beyond my current level of education to make myself as valuable as I can to this company. That may not be enough for some companies, but I am hoping it is for the others. So many people looking for jobs are so bland, good at regurgitation not imagination, and believe that their education period is over since they have a degree. 

My math background in college stopped at stats, but I have had to learn a little bit of calc and a good amount of linear algebra for ML. I am no expert, but I understand how the a multiplication matrix works. 

As for my friend, I think it is because the CEO of this company still runs the show and doesn't want to deal with any risks, i.e. someone who needs extra training to do data analysis. Sad, but I guess I understand because again, he is looking at me as someone he has to pay slightly more than a brand new graduate looking for his first job. I would gladly take less pay and start out as probationary just so I could get in the door....maybe I will talk to him again when I see him next.

I am definitely on the fence with this UCSD course vs buying 10-15 boot camp classes from Udemy and building my portfolio on my own. They give you this cute 20 question test to filter out people who can't extrapolate or reason through things in an ""if this, then this"" manner, but I highly doubt what score you get on that test, if you tell them you want to attend and throw $11k at them, I highly doubt they would say no....",NA
"2504",2503,3,"When I reread what I typed I can totally see how you might think it was rhetoric. Sorry about that, I was being genuine, but this is reddit and you never know ha ha! I checked out the GATech masters and my only issue is that is will take 6 months longer at the minimum and doesn't start until fall 2020. Might still pursue it because why not if I can actually put MS on my resume after that, but for now I am still debating if I should take this boot camp or just buy 10 Udemy classes for a fraction of the cost and build my portfolio myself over the next 6 months. Like you said, I will need some really amazing examples in my GitHub for anyone to take me seriously.",NA
"2505",2504,5,"I will be following this video series. Thank you and all the best!",NA
"2506",2505,5,"Thank you! All the best to you too :)",NA
"2507",2506,9,"Try github.",NA
"2508",2507,10,"Nice, thanks for sharing.",NA
"2509",2508,13,"I’ve found both of these useful.

Khan academy : [Statistics and probability ](https://www.khanacademy.org/math/statistics-probability) 

Coursera Specialization :[Mathematics for Machine Learning Specialization](https://www.coursera.org/specializations/mathematics-machine-learning)",NA
"2510",2509,13,"I would check your local university (or any large university where you can easily find information) and look for syllabuses. From there, look for which textbooks they use and see if you can get them for cheap online, or sometimes free. As far as personal recommendations, [StatQuest](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9) is nice... but a little odd.",NA
"2511",2510,16,"It's probably a good idea. You should look at apps that already do this and be very clear how your tool is different. For instance, I think Rapid Miner is the industry leader. JMP is similar.",NA
"2512",2511,16,"Thank you for responding",NA
"2513",2512,21,"An example of what you’re trying to do would help. Read through [the Google Maps API docs](https://developers.google.com/maps/documentation). If it provides the service you need, then yes you need an API key for their service. You’ll probably also want to check out their pricing page. While you’re at it, you should look into similar APIs. I’ve used Mapquest in the past to do some reverse geocoding.",NA
"2514",2513,24,"nice content",NA
"2515",2514,25,"So according to this, you should probably skip weka  [https://www.reddit.com/r/MachineLearning/comments/1rwj8p/why\_are\_python\_r\_so\_much\_more\_popular\_here\_than/](https://www.reddit.com/r/MachineLearning/comments/1rwj8p/why_are_python_r_so_much_more_popular_here_than/) 

&#x200B;

Tensorflow is a deep learning framework and is very handy to learn. But knowing the code is gonna be useless if you don't understand the theory behind it, much like other ML libraries (specifically scikit learn).

&#x200B;

Tableau is allegedly a pretty nice visualization tool but that's about it. I haven't tried to play around with it but my friend says it's very nice looking, and decently good enough (but as with all BI tools, have their own limitations such that it might be better to just use matplotlib or bokeh/plotly for interactive visualizations). 

&#x200B;

I think most folks will tell you to get your basics down. At least I'm of the opinion that you don't necessarily have to master them, but know enough such that when you read a paper, you'll understand in general what's happening (some may disagree, those are still very valid opinions). You can then master these frameworks or packages (tensorflow or pytorch for DL among other things) on your own. Because the field of data science is so vast (and isn't as structured), there's gonna be a lot of self-study here.",NA
"2516",2515,25,"Ok great, this is exactly the type of feedback I was looking for, thank you.",NA
"2517",2516,30,"My understanding is that k-means is already “spherical” by default? That’s assuming you’re using L2 distance, which implies the Euclidean ball. Where are you seeing differing definitions?",NA
"2518",2517,31,"https://stackoverflow.com/questions/56942670/matplotlib-seaborn-first-and-last-row-cut-in-half-of-heatmap-plot",NA
"2519",2518,32,"I recommend starting on Python with NumPy and pandas ASAP. Get courses on those if you need, or learn the basics yourself and then perfect while taking some intro ML courses. But these are all just to get the foundation ready — to get your first job you need to get experience, and the only way you can get it now is through personal projects or an internship. The point is when you are out seeking your first job, you will need to have at least a basic portfolio. One other thing — take soft skills courses while you can. Being charismatic at an interview gives one a very significant advantage.",NA
"2520",2519,32,"Not sure, maybe this would help you ([https://towardsdatascience.com/what-coding-languages-do-i-need-to-know-for-a-career-in-analytics-595887deadbd](https://towardsdatascience.com/what-coding-languages-do-i-need-to-know-for-a-career-in-analytics-595887deadbd)) also containing the best niche platforms to learn ds",NA
"2521",2520,32,"Agree. Data science requires programming skills. Almost everything you learn will be easier if you are a decent programmer. I've seen all ends of the spectrum in Coursera courses and when getting a master's in data science. Those who can write code move faster.",NA
"2522",2521,32,"You recommend that I finish my tableau learning or to let that for later and begin with Python (pd, np)?",NA
"2523",2522,32,"I have started my Python Journey and i am currently over the basics, I am aiming for a careen in DS/ML/AI but as i have no CS background i thought about doing a Python Internship(for the case when i wont be able to land a job) as a Python Developer.

What would you recommend, first do a Python Developer Job and then after some time go into DS/AI/ML? Or First study DS/AI/ML and then land an internship/Job in that niche?",NA
"2524",2523,32,"I think tableau brings you closer to BI/DA jobs, but for DS, Python, IMHO, makes you more employable. I would define a good generic skillset in DS as follows:

SQL, Python (numpy, pandas, scikit-learn, keras), Apache Spark, familiarity with versioning (git) and basics of nix (e.g. bash). That doesn't mean every DS job requires these, and it doesn't mean you only need these skills, but it's a good ""vanilla"" toolkit.",NA
"2525",2524,32,"It's hard to say. If you want to be an ML engineer, then Python dev experience might help a bit. For a more general data science role, being a software dev won't really count. Either way, it seems to me going the dev -> DS route means doing twice the work (unless you have an easy way in into software development). But then I'm looking at it from my perspective whereas in your circumstances there may other reasons. I think you should talk to a number of soft devs and data scientists in your area to have a better idea. If you have HackerNest or similar social events, that's a perfect opportunity to do that.",NA
"2526",2525,32,"Thanks! Is it okay if I pm you for a quick qstn?",NA
"2527",2526,32,"Sorry, I missed you message. Sure thing.",NA
"2528",2527,35,"Thought this was going to be about the philosophical underpinnings of data science... it was not.",NA
"2529",2528,39,"#1 SAS. Lol what?",NA
"2530",2529,40,"Hey, I’m a newbie as well. I’m studying business analytics in university right now and have recently started learning python. Apart from that I have a couple classes worth of SQL experience and working with databases. I want to learn data science but don’t really know where to begin. What helped you get started?",NA
"2531",2530,41,"If you like this and you'd like to learn more about movie recommendation, check out https://movielens.org/ with data available at https://grouplens.org/datasets/movielens/. GroupLens is a computer science research lab at the University of Minnesota focusing on recommender systems.",NA
"2532",2531,41,"Thanks for this, very nice resource for a beginner like me.",NA
"2533",2532,44,"This is a good question to ask, but too bad there’s not enough traction. Could you please by any chance repost this on r/datascience and see if you get some feedback?",NA
"2534",2533,44,"Meh I got a job, power bi and python",NA
"2535",2534,45,"I am currently doing this with the rtweet package",NA
"2536",2535,45,"Thanks! Instead of using R, I did scraping with the Python twitterscraper module instead. I really recommend that way. No need for API keys, and can return old tweets.",NA
"2537",2536,46,"Thanks for sharing. The TLDR could be that virtual environments and APIs are the shortcomings of online data science courses, which is fine. But I can't help thinking about when Coursera was just getting going and the pass rates from those initial courses was like .5%. They were so hard and you could learn so much from them. I thought to myself, ""this is the future."" But some 8 years later the courses seem to be watered down, alternative credentialing engines.",NA
"2538",2537,46,"So would you view Coursera/other MOOCs as legitimate experience if they had a lower pass rate?  
And what is your opinion on why there is no hacker rank for data scientists?",NA
"2539",2538,46,"I don't think the pass rate has a direct causal effect on legitimacy, but wouldn't you agree that some of your most meaningful courses have been difficult? A Standford CS course on Natural Language Processing should probably be tough, right? (I took one and it about killed me, but I learned a ton.) I get why the MOOC providers wanted to make their products more accessible, and maybe they're doing more good overall in the world, but their products are not for me anymore.

Speaking of falling out of touch, I had to look up ""Hacker Rank.""  I think Kaggle has done a good job with rankings based on competitions but I can see the limitations of their approach. I believe there are still business opportunities out there for accessing data science talent.",NA
"2540",2539,46,"I think the problem with Kaggle is that the same people usually at the top of each competition. There are only about 5,000 competitions Experts, but there are more jobs open in data science than that. I think we need something other than Kaggle to prove competency.",NA
"2541",2540,47,"This is a pretty straight forward GIS task. You'll want to use something like QGIS. The flood data looks like vector data. The bigger question is what format the construction data is in. If it's just point data, you just need to run an ""intersect"" to select construction points that fall within the flood area.",NA
"2542",2541,47,"Thanks a lot! Sounds logical. The construction data set is in CSV-format, with the\_geom and long/lat data. Like this: 

`the_geom, cost_estimate, the_geom_webmercator, latitude, longitude, borough`",NA
"2543",2542,47,"Then you'll have to first import the csv into QGIS and convert it to xy data. There are lots of tutorials on this.",NA
"2544",2543,49,"Is it case sensitive? A quick search of the twitter app shows most #tsla tweets are lower case.",NA
"2545",2544,49,"Not familiar so just a thought. Are you filtering to retweets?",NA
"2546",2545,49,"Good thought! Unfortunately not the issue though. the search is non-case sensitive...",NA
"2547",2546,49,"I am filtering out retweets `-filter: retweets` to avoid getting a bunch of identical tweets.",NA
"2548",2547,50,"Needs more python",NA
"2549",2548,53,"Hey there - I reached out on PM! 

I'm hacking on a product to solve this: [Datapane](https://datapane.com). It's an API which you can use from Python to deploy and host plots and datasets so you can embed them into other platforms (like Medium posts or [Reddit itself](https://www.reddit.com/r/PostPreview/comments/dz25dk/test1/))

You can check out the [quick start guide for the API](https://docs.datapane.com/quickstart) if you want to use it.

As I mentioned, we don't support folium right now, but we do support bokeh, altair, etc., and can definitely look at adding it to our supported visualisations (I did some interactive heatmap work on it a while back and found it way easier to work with than the alternatives).",NA
"2550",2549,53,"thanks, replied to your PM",NA
"2551",2550,54,"Minute Physics did a cool explanatory video on this: https://youtu.be/FE9ko2wtyeQ

Search for Statistical Data Disclosure Techniques.",NA
"2552",2551,54,"Step 1. Grow a brain. Step 2. Realize it's far too late for that. Step 3. Give up and get back to work you fucking wage slave loser.",NA
"2553",2552,55,"There's an oxymoron if I've ever heardone: Learn machine learning.",NA
"2554",2553,57,"The docs say you can only make 180 calls every 15 minutes. 
https://developer.twitter.com/en/docs/basics/rate-limiting",NA
"2555",2554,59,"Thanks for posting!",NA
"2556",2555,61,"Beautiful soup can extract data from html pages, but the issue here is some pages might format their pages differently so the approach might differ a bit. For pdfs, Im not sure.",NA
"2557",2556,61,"short answer: no

best case you'll have to string together a bunch of different technologies to get what you want. lxml or beautiful soup can parse the html. but you'll need to program it yourself. for pdf files you're best bet is something like Textract by AWS, which will turn it into a csv for you, but you'll still have to clean/parse it for your own purposes. for grabbing the html/pdf files to begin with you'll need a web scraper. for this you can use scrapy, selenium, or just the requests module, depending on your needs.",NA
"2558",2557,62,"Nice content!",NA
"2559",2558,65,"Yes it is as simple as you have said. You will need to run a regex function on the streaming data that comes through the Twitter API. Then just store the ""hit"" tweets in a dataframe or whatever your data structure of choice.

You may want to look into he API and see if it already allows for keywords. I used the API a long time ago and don't remember the parameters you have.


You also could just collect all the tweets for a time period then come back and santize the data. Both methods will work",NA
"2560",2559,65,">You may want to look into he API and see if it already allows for keywords. I used the API a long time ago and don't remember the parameters you have.

Ok, great. Thank you!",NA
"2561",2560,66,"Nice content thanks for sharing!!",NA
"2562",2561,71,"As far as I know, to make a heatmap using folium you would need a geojson file that would give the geometry of the city polygon. There's a great tutorial [in the documentation](https://python-visualization.github.io/folium/quickstart.html#Choropleth-maps) to do this. Several cities have this readily available through their city website.

Alternatively if you just have one set of latitude and longitude for each city in a pandas data fram, you could add them to a map as markers:

````
latitude = 37.7749
longitude = -122.4194

# creating map of SF using latitude and longitude values
map_sf = folium.Map(location=[latitude, longitude], zoom_start=12.5)`

# add markers to map
for lat, lng, neighborhood, rent in zip(sf_data['Latitude'], sf_data['Longitude'], 
    sf_data['Neighborhood'], sf_data['Rent']):
    label = 'Neighborhood: {}, Median Rent: ${}'.format(neighborhood, rent)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker( 
        [lat, lng], radius=5, popup=label, color='blue', fill=True, 
        fill_color='#3186cc', fill_opacity=0.7, parse_html=False).add_to(map_sf) 

#show map
map_sf
```",NA
"2563",2562,71,"Thank you! I will try this",NA
"2564",2563,71,"I managed to get all the markers on the map with your code, thanks a lot!

However, I'd like the markers to vary in size depending on the value the sf\_data\[""Rent""\], alternatively, color code it so that high rent = red, low rent = green. Would you know how to approach this?",NA
"2565",2564,72,"I think both customer and product category can be nodes. If cust buy prod, there will be a connection from cust to prod",NA
"2566",2565,72,"That’s true. But considering the amount of different customers, it might be hard to create a decent network? 

The reason for doing this with product group could be to analyze co-purchase patterns?",NA
"2567",2566,73,"Checkout folium. Its a python wrapper written over leaflet.js",NA
"2568",2567,73,"Swconding folium. Very easy to use and looks very nice.",NA
"2569",2568,75,"Well, there are a couple of ways you can achieve this:

#Apply method

Like /u/penatbater explained, you can use the [pandas.Series.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html#pandas.Series.apply) method to apply a function to each value of a Series, returning a new Series with the results. 

    def get_city_price(city):
        if city == 'London':
            return 10
        elif city == 'New York':
            return 12
        elif ... #other cities
        ...    
        else return 1 #or whatever you want

    df['total_price'] = df['city'].apply(get_city_price) * df['units']

You can also similarly use the [pandas.DataFrame.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html) method with parameter axis=1 to apply a function to each row of your DataFrame and do the entire multiplication inside your function, it would be less efficient if you use the entire DataFrame but it's good to know it's a possibility:

    def get_city_price(row):
        if row['city'] == 'London':
            return 10 * row['units']
        elif row['city'] == 'New York':
            return 12 * row['units']
        elif ... #other cities
        ...       
        else return 1  * row['units'] #or whatever you want

    df['total_price'] = df[['city','units']].apply(get_city_price,axis=1)

#Loc method
You can use several calls to the [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) method to selectively attribute values to a DataFrame, like so:

    df.loc[df['city'] == 'London', 'total_price'] = df['units'] * london_price
    df.loc[df['city'] == 'New York', 'total_price'] = df['units'] * newyork_price
    ... and so on...

#Merge method

You could have a new DataFrame with the pairs of city names and prices, like so:


    city_prices = pd.DataFrame([['London' , 10],['New York',12],['Boston',8]],columns=['city','city_price'])

    print city_prices

           city  multiplier
    0    London          10
    1  New York          12
    2    Boston           8

And then merge it into your df:

    df = df.merge(city_prices,how='left',on='city')
    df['total_price'] = df['city_price'] * df['units']

Even better if you put that list of city prices in a CSV instead of inside a DataFrame constructor call. Then it will be easier to edit in the future.

#Conclusion
There's probably other methods I'm unaware of, but those are my go-tos. Personally I'd use the merge method reading the city prices from a CSV, because it has less code repetition and will be easier to manage a long list of cities.",NA
"2570",2569,75,"Is price_boston a single value? Because if so, you can use the apply method. 

def loc(x):
If x == 'london' :
Return 8
Elif x=='boston' :
Return 10
And so on.

Then just use apply method.

Df['city_value'] = df['city'].apply(loc)

Then just multiply df[city_value] * df[column]. 
This will create a new column based on the value of your cities, essentially making a lookup table of sorts. Then just multiply that column with your column 1.

Let me know if I misunderstood your question. I'm on mobile so it's not super correct syntax wise, but the logic is there.",NA
"2571",2570,75,"Thank you so much! I will try your methods. Is it possible to return a variable which hold the value? I.e. like this**:**

**price\_variable\_london** = 8.456  
**price\_variable\_newyork** = 10.579

 def get\_city\_price(city):       
if city == 'London':           
return **price\_variable**      
elif city == 'New York':           
return **price\_variable**  
elif ... #other cities",NA
"2572",2571,75,"Thanks a lot!! This makes sense. I will have a go and get back if I have any issues.",NA
"2573",2572,75,"Sorry, I don't think I understand what you're trying to accomplish. But regardless, it seems you want to use the apply method and want to know the best way to organize your function and the values it will use.

In general it's bad practice to have stuff outside of your function declaration that may affect it's result. So here's what I would code:

    def get_city_price(city):
        prices_dict = {
            'London' : 8.456,
            'New York' : 10.579,
            'Boston' : 12.123
        }
        return prices_dict[city]

If you need to modifily the values of the prices_dict during your program's execution, you can put it outside the function, but that's not a good practice.",NA
"2574",2573,75,"I got it. Perfect. Thanks!",NA
"2575",2574,77,"I'm not sure, but pandas can directly read from a URL assuming that URL is the csv file. So just make a list of all the urls, then make a function that opens a pandas data frame then use the to_csv to save it. The tricky part would be naming the csv but you can just easily use numbers.",NA
"2576",2575,77,"You mean that this:

csv\_url = [http://xxxxxx.csv](http://xxxxxx.csv)  
data = pd.read\_csv(csv\_url)  


Would work as well as this, where the file is already downloaded to desktop:  
data = pd.read\_csv(""C:/Users/Desktop/file.csv"")  


Is that correct?",NA
"2577",2576,77,"If the name changes ie ver1, ver2, stateA, StateB, you can also incorporate Regex to capture the URL(s).

For [reference to read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)",NA
"2578",2577,77,"Yes that will work. You can even read from S3.",NA
"2579",2578,77,"Yep exactly! You can also use regex like that other commenter said to parse the filename from the URL.",NA
"2580",2579,77,"Right, thanks!!",NA
"2581",2580,78,"df.groupby('borough')['price'].mean()",NA
"2582",2581,78,"Wow that was simple. Thanks a lot!",NA
"2583",2582,82,"8. Whatever you budget for it, double the estimate.",NA
"2584",2583,83,"I took the data analyst one and saw the data science one come to life as I finished mine. My comments can credibly talk about my time when I was finishing Udacity's program. 

My background is in cell biology, and I wanted to use the certificate as a way to switch fields. Overall, it did not go well. So now I'm in a data science masters program. 

The support system is a mess. You are in big trouble if you get stuck. They had a mentor program, but that was unreliable. The slack channel is a great resource but only if enough people participate. 

I think the projects aren't rigorous enough. You can easily tinker around with little understanding and pass with little to no feedback. 

While you are encouraged to do more, the environment was not conducive to facilitated learning to me. There were lots of times when I got stuck on Python code and didn't know how to apply a stackoverflow answer to my code. 

I do like the video learning format. I get a lot more out of a guided video than I do through reading a book with code. 

You can and will come up with project ideas on your own. There are tons of different cleaned datasets out there ready for a ML project or visualization practice. 

I wouldn't recommend the Udacity program to someone like me, who started a weak coder hoping this was enough to get me into the field. It just wasn't enough.",NA
"2585",2584,83,"Wanna know as well, following",NA
"2586",2585,84,"check [https://github.com/virgili0/Virgilio](https://github.com/virgili0/Virgilio)

&#x200B;

It's an in-progress project, but you already find a lot of stuff.",NA
"2587",2586,84,"Have you considered getting a graduate degree in data science or re-entering school to complete the prerequisites necessary? 

http://datasciencemasters.org/

This about covers everything and provides you resources. The thing you'd be missing from a proper school environment is community, access to teachers who help, and some career help. 

This will also depend on what kind of data scientist you want to be and how you can get the job in the first place. I know data scientists who only do reporting aspects, and I also know data scientists who are more machine learning engineers than anything. The field is wide open and all over the place.",NA
"2588",2587,84,"I'm using DataCamp (https://www.datacamp.com/). There are tons of courses on different aspects and topics related with Data Science. They have also some projects and a community with lot of tutorials.

For now I'm happy with it.",NA
"2589",2588,86,"Check out r/datasets",NA
"2590",2589,90,"The primary difference here that is not directly identified, (and far too often isn't understood in the first place), is the difference between Supervised and Unsupervised Learning, and how they relate to perspectives and insights from data. 

That is something I often see referred to in the context of Machine Learning, however - it is also the primary difference between Data Analytics, and Data Science. One is ""analysis inside the box"" and the other is creating/recreating a box for doing the data analytics.

So the real answer to what a company wants isn't an either/or in the modern Business case. 

Only true Legacy Companies, with well-established metrics, where no change has happened due to e-commerce, and the supply and demand inputs have remained in place without interruption for the last hundred and fifty years would not need both, and I honestly can't think of a single example of one like that.

Even the small and medium size companies need both, and the Enterprise company most certainly needs both, and need them with interdepartmental scope, and increasing organizational accountability/oversight at each layer of depth, to ensure that the data architecture is sound (integrated and compatible) and to enforce appropriate governance over the source data and deliverables. 

When an Enterprise is too big for that to work, they need it even more than they think they do. 

If any CIO/CFO/CTO*, or other actual Decision Maker would like to consult with an the insane Datanaut who wrote this post about their situation and what can help them - feel free to message me. 

I'm not selling a product. I only represent myself, my views, and my experience. The value of which is debatable, but rather extensive.",NA
"2591",2590,91,"This is just a Panoply commercial.",NA
"2592",2591,92,"This is just another commercial from Panoply",NA
"2593",2592,94,"Even the term 'data science' is so broad what you're doing now can effectively be a part of data science. At its general core tho (at least from what I see on r/datascience), it's mostly a mix or a spectrum between machine learning engineers and business analysts, both who provide value to any company that can utilize those fields well. 

&#x200B;

I reckon most of the 'science' part comes in the form of machine learning and/or deep learning.",NA
"2594",2593,94,"I figured it was like statistical analysis or something. How do I actually glean useful information from the data after I have it? I guess that's what I want to learn.",NA
"2595",2594,94,"Oh definitely! I can't believe I forgot that haha",NA
"2596",2595,95,"I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit:

- [/r/datawarehouse] [How Your Data Warehouse Can Make Data Mining Easier And More Efficient](https://www.reddit.com/r/datawarehouse/comments/deuoeq/how_your_data_warehouse_can_make_data_mining/)

&nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*",NA
"2597",2596,97,"Don’t do any online course for the certification. Do it for the skills you can learn.",NA
"2598",2597,97,"Plus, whether it's taught in R or python shouldn't be an issue. Both have their uses in data science. Python is popular, sure, but nothing beats R's ggplot2 and tidyverse libraries for data manipulation. Plus, if you start with R, then learn python, that's 2 languages you now know!",NA
"2599",2598,97,">  Plus, if you start with R, then learn python, that's 2 languages you now know!

Not to disagree with you, but here are my two cents.  I went that route a few years ago. But now I'm thinking of dropping R from my list of active skills. I really like ggplot and dplyr, but since it's python+pandas+matplotlib that I use actively, I'm starting to forget the R libs. If it was only python and R, I could probably manage. But I also had/have to use matlab, js, and c occasionally. To be honest, I'd rather work more on my js skill than R. For beginners, js is especially important because it gives you the opportunity to show off your skills in ways not accessible to most other candidates. I mean would you hire someone who has the same standard crap on kaggle as a million other ds candidates or someone who has a very unusual interactive project appealing not only to d-ists but to the general audience as well?",NA
"2600",2599,97,"I wouldn't suggest one doesn't learn R... But it's simply not as robust a language as Python. 
For example, some data scientists need to make web apps; having familiarity with Python can help with flask and Django; you don't have that advantage with R.

I have friends who swear by R for visualization/data exploration and stats, but similar tools are available in Python for anything that exists in R (even if the tool is sometimes easier to use/more intuitive in R).

As far as learning multiple languages, one could make an argument to learn both Python and R, but Python covers so many of the capabilities of R that it would make more sense to learn Java/C++ (for creating super fast code) or Scala (for working with large amounts of data) than to learn another language that has similar capabilities to but is not as robust as python. 

Also, as far as courses, I would suggest a bootcamp if you can afford it. 

I did Metis and would recommend it to anyone and I hear insight is quite good (but you need a PhD to do the data science program).",NA
"2601",2600,97,"I'm the same ish. I started learning R but have since not touched it in a long time. On the times that I do use R (like when getting the p values of my coefficients), I find it like riding a bike, takes a bit but you quickly get the hang of it.

But I'm also not disagreeing with you hehe I think it really depends on what interests the person. I'm thinking of tinkering with Tableau after I've had some experience with bokeh to get more experience making dashboards.",NA
